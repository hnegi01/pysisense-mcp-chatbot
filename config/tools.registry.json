[
  {
    "tool_id": "access.change_folder_and_dashboard_ownership",
    "module": "access",
    "class": "AccessManagement",
    "method": "change_folder_and_dashboard_ownership",
    "description": "Method to change the ownership of folders and optionally dashboards.",
    "full_doc": "Method to change the ownership of folders and optionally dashboards.\nThis method changes the ownership of a target folder and the entire\ntree structure surrounding it, including subfolders, sibling folders,\nand parent folders.\nOptionally, it will also change the ownership of dashboards associated\nwith these folders.\n\nParameters:\n    user_name (str): The user running the tool. This is necessary for\n    API access checks.\n    folder_name (str): The target folder whose ownership needs to be\n    changed.\n    new_owner_name (str): The new owner to whom the folder (and\n    optionally dashboards) ownership will be transferred.\n    original_owner_rule (str, optional): Specifies the ownership rule\n    to set original owner after changing ownership('edit' or 'view').\n    Default is 'edit'.\n    change_dashboard_ownership (bool, optional): Specifies whether to\n    also change the ownership of dashboards in the folder tree. Default\n    is True.",
    "parameters": {
      "type": "object",
      "properties": {
        "executing_user": {
          "type": "string",
          "description": "executing_user parameter"
        },
        "folder_name": {
          "type": "string",
          "description": "The target folder whose ownership needs to be changed."
        },
        "new_owner_name": {
          "type": "string",
          "description": "The new owner to whom the folder (and optionally dashboards) ownership will be transferred."
        },
        "original_owner_rule": {
          "type": "string",
          "description": "Specifies the ownership rule to set original owner after changing ownership('edit' or 'view'). Default is 'edit'."
        },
        "change_dashboard_ownership": {
          "type": "boolean",
          "description": "Specifies whether to also change the ownership of dashboards in the folder tree. Default is True."
        }
      },
      "required": [
        "executing_user",
        "folder_name",
        "new_owner_name"
      ]
    },
    "mutates": true,
    "tags": [
      "access",
      "dashboards",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "access.create_schedule_build",
    "module": "access",
    "class": "AccessManagement",
    "method": "create_schedule_build",
    "description": "Method to create a schedule build for a DataModel.",
    "full_doc": "Method to create a schedule build for a DataModel.\n\nSupports both cron-based schedules (e.g., every Monday at 9:00 UTC)\nand interval-based schedules (e.g., every 2 days, 1 hour, 30 minutes).\n\nParameters:\n    datamodel_name (str): The name of the DataModel.\n    build_type (str): Optional. Type of the build (e.g., \"ACCUMULATE\", \"FULL\",\n    \"SCHEMA_CHANGES\"). Defaults to \"ACCUMULATE\".\n    days (list, optional): List of days for cron schedule. Eg.: [\"SUN\", \"MON\", \"TUE\", \"WED\", \"THU\", \"FRI\",\n    \"SAT\"] or [\"*\"] for all days.\n    hour (int, optional): Hour in 24-hour format (UTC).\n    minute (int, optional): Minute of the hour (UTC).\n    interval_days (int, optional): Interval in days.\n    interval_hours (int, optional): Interval in hours.\n    interval_minutes (int, optional): Interval in minutes.\n\nReturns:\n    dict: API response or error.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "The name of the DataModel."
        },
        "build_type": {
          "type": "string",
          "description": "Optional. Type of the build (e.g., \"ACCUMULATE\", \"FULL\", \"SCHEMA_CHANGES\"). Defaults to \"ACCUMULATE\"."
        },
        "days": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "List of days for cron schedule. Eg.: [\"SUN\", \"MON\", \"TUE\", \"WED\", \"THU\", \"FRI\", \"SAT\"] or [\"*\"] for all days."
        },
        "hour": {
          "type": "integer",
          "description": "Hour in 24-hour format (UTC)."
        },
        "minute": {
          "type": "integer",
          "description": "Minute of the hour (UTC)."
        },
        "interval_days": {
          "type": "integer",
          "description": "Interval in days."
        },
        "interval_hours": {
          "type": "integer",
          "description": "Interval in hours."
        },
        "interval_minutes": {
          "type": "integer",
          "description": "Interval in minutes."
        }
      },
      "required": [
        "datamodel_name"
      ]
    },
    "mutates": true,
    "tags": [
      "access",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "access.create_user",
    "module": "access",
    "class": "AccessManagement",
    "method": "create_user",
    "description": "Creates a new user by processing the provided user data to replace role",
    "full_doc": "Creates a new user by processing the provided user data to replace role\nnames and group names with their corresponding IDs, then sends a POST\nrequest to create the user.\n\nParameters:\n    user_data (dict): A dictionary containing user details such as\n    email, firstName, lastName, role (role name), groups (list of group\n    names), and preferences.\n\nReturns:\n    dict: The response from the API if successful,\n        or a dictionary with an 'error' key if the operation fails.",
    "parameters": {
      "type": "object",
      "properties": {
        "user_data": {
          "type": "object",
          "description": "A dictionary containing user details such as email, firstName, lastName, role (role name), groups (list of group names), and preferences."
        }
      },
      "required": [
        "user_data"
      ]
    },
    "mutates": true,
    "tags": [
      "access",
      "users",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "access.delete_user",
    "module": "access",
    "class": "AccessManagement",
    "method": "delete_user",
    "description": "Deletes a user by their email (username).",
    "full_doc": "Deletes a user by their email (username).\n\nParameters:\n    user_name (str): The email or username of the user to be deleted.\n\nReturns:\n    dict: Response from the API if successful,\n        or an error message dict.",
    "parameters": {
      "type": "object",
      "properties": {
        "user_name": {
          "type": "string",
          "description": "The email or username of the user to be deleted."
        }
      },
      "required": [
        "user_name"
      ]
    },
    "mutates": true,
    "tags": [
      "access",
      "users",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "access.get_all_dashboard_shares",
    "module": "access",
    "class": "AccessManagement",
    "method": "get_all_dashboard_shares",
    "description": "Method to retrieve all dashboard shares, including user and group details for each shared dashboard.",
    "full_doc": "Method to retrieve all dashboard shares, including user and group details for each shared dashboard.\n\nThis method uses pagination to retrieve all dashboards and their share information, and it collects\ncorresponding user and group details for each share.\n\nReturns:\n    list: A list of dictionaries containing the dashboard title, share type (user or group),\n    and share name (email or group name).",
    "parameters": {
      "type": "object",
      "properties": {},
      "required": []
    },
    "mutates": false,
    "tags": [
      "access",
      "dashboards",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "access.get_datamodel_columns",
    "module": "access",
    "class": "AccessManagement",
    "method": "get_datamodel_columns",
    "description": "Retrieves columns from a DataModel by collecting them from its datasets and tables.",
    "full_doc": "Retrieves columns from a DataModel by collecting them from its datasets and tables.\n\nParameters:\n    datamodel_name (str): The name of the DataModel from which to extract columns.\n\nReturns:\n    list: A list of dictionaries where each dictionary contains DataModel ID, DataModel name,\n    table name, and column name.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "The name of the DataModel from which to extract columns."
        }
      },
      "required": [
        "datamodel_name"
      ]
    },
    "mutates": false,
    "tags": [
      "access",
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "access.get_group",
    "module": "access",
    "class": "AccessManagement",
    "method": "get_group",
    "description": "Retrieves group details by their name.",
    "full_doc": "Retrieves group details by their name.\n\nParameters:\n    name (str): The name of the group to be retrieved.\n\nReturns:\n    dict: Group details, or {'error': ...} if retrieval fails or not\n    found.",
    "parameters": {
      "type": "object",
      "properties": {
        "name": {
          "type": "string",
          "description": "The name of the group to be retrieved."
        }
      },
      "required": [
        "name"
      ]
    },
    "mutates": false,
    "tags": [
      "access",
      "groups",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "access.get_unused_columns",
    "module": "access",
    "class": "AccessManagement",
    "method": "get_unused_columns",
    "description": "Identify unused columns in a given DataModel by comparing all available columns against the columns",
    "full_doc": "Identify unused columns in a given DataModel by comparing all available columns against the columns\nreferenced in associated dashboards.\n\nCovers:\n- Dashboard Filters: Dashboard-level filters, Widget filters, Dependent Filters.\n- Widget Panels: Includes Row, Values, Column panels, and Measured Filters.\n\nParameters:\n    datamodel_name (str): The name of the DataModel to analyze.\n\nReturns:\n    list: A list of dictionaries containing unused column details with a \"used\" field set to True or False.\n\nRaises:\n    ValueError: If no columns are found for the given DataModel (for example, if it does not exist\n                or is not accessible).",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "The name of the DataModel to analyze."
        }
      },
      "required": [
        "datamodel_name"
      ]
    },
    "mutates": false,
    "tags": [
      "access",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "access.get_unused_columns_bulk",
    "module": "access",
    "class": "AccessManagement",
    "method": "get_unused_columns_bulk",
    "description": "Run unused-column analysis for one or more data models and return a",
    "full_doc": "Run unused-column analysis for one or more data models and return a\ncombined result set.\n\nParameters\n----------\ndatamodels : str or list of str, optional\n    One or more data model references to analyze. Each reference can be:\n      - a data model ID, or\n      - a data model title (name).\n    At least one data model reference is required. At runtime this\n    parameter is tolerant of a single string and will normalize it to a\n    one-element list.\n\nReturns\n-------\nlist of dict\n    A flat list of rows across all processed data models. Each row has\n    the same structure as returned by get_unused_columns().\n    If no data models are successfully processed, an empty list is\n    returned and details are available in the logs.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodels": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "One or more data model references to analyze. Each reference can be: - a data model ID, or - a data model title (name). At least one data model reference is required. At runtime this parameter is tolerant of a single string and will normalize it to a one-element list."
        }
      },
      "required": []
    },
    "mutates": false,
    "tags": [
      "access",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "access.get_user",
    "module": "access",
    "class": "AccessManagement",
    "method": "get_user",
    "description": "Retrieves user details by their email (username) and expands the",
    "full_doc": "Retrieves user details by their email (username) and expands the\nresponse to include group and role information.\n\nParameters:\n    user_name (str): The email or username of the user to be retrieved.\n\nReturns:\n    dict: User details on success, or {'error': 'message'} on failure.",
    "parameters": {
      "type": "object",
      "properties": {
        "user_name": {
          "type": "string",
          "description": "The email or username of the user to be retrieved."
        }
      },
      "required": [
        "user_name"
      ]
    },
    "mutates": false,
    "tags": [
      "access",
      "users",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "access.get_users_all",
    "module": "access",
    "class": "AccessManagement",
    "method": "get_users_all",
    "description": "Retrieves user details along with tenant, group, and role information.",
    "full_doc": "Retrieves user details along with tenant, group, and role information.\nRemoves \"Everyone\" group from users if they belong to other groups, but\nkeeps the \"Everyone\" group if it's the only group the user belongs to.\n\nReturns:\n    list: List of user details dicts, or [{'error': ...}] if retrieval\n    fails.",
    "parameters": {
      "type": "object",
      "properties": {},
      "required": []
    },
    "mutates": false,
    "tags": [
      "access",
      "users",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "access.update_user",
    "module": "access",
    "class": "AccessManagement",
    "method": "update_user",
    "description": "Updates a user by their User Name.",
    "full_doc": "Updates a user by their User Name.\n\nParameters:\n    user_name (str): The email or username of the user to be updated.\n    user_data (dict): A dictionary containing user details to update,\n    such as role, groups, etc.\n\nReturns:\n    dict: The response from the API if successful,\n        or a dictionary with an 'error' key if the operation fails.",
    "parameters": {
      "type": "object",
      "properties": {
        "user_name": {
          "type": "string",
          "description": "The email or username of the user to be updated."
        },
        "user_data": {
          "type": "object",
          "description": "A dictionary containing user details to update, such as role, groups, etc."
        }
      },
      "required": [
        "user_name",
        "user_data"
      ]
    },
    "mutates": true,
    "tags": [
      "access",
      "users",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "access.users_per_group",
    "module": "access",
    "class": "AccessManagement",
    "method": "users_per_group",
    "description": "Retrieves all users within a specific group by name.",
    "full_doc": "Retrieves all users within a specific group by name.\n\nParameters:\n    group_name (str): The name of the group.\n\nReturns:\n    list or dict: A list of users in the group if successful, or a\n    dictionary containing an 'error' key if the operation fails.",
    "parameters": {
      "type": "object",
      "properties": {
        "group_name": {
          "type": "string",
          "description": "The name of the group."
        }
      },
      "required": [
        "group_name"
      ]
    },
    "mutates": false,
    "tags": [
      "access",
      "users",
      "groups",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "access.users_per_group_all",
    "module": "access",
    "class": "AccessManagement",
    "method": "users_per_group_all",
    "description": "Retrieves all groups and maps them with the users belonging to those",
    "full_doc": "Retrieves all groups and maps them with the users belonging to those\ngroups.\nGroups like 'Everyone' and 'All users in system' are excluded.\nUsers with roles like 'admin', 'dataAdmin', and 'sysAdmin' are mapped\nto the existing 'Admins' group.\nGroups with no users are also included in the final result.\n\nReturns:\n    list: A list of dictionaries, where each dictionary contains a\n    group name and the list of usernames in that group.",
    "parameters": {
      "type": "object",
      "properties": {},
      "required": []
    },
    "mutates": false,
    "tags": [
      "access",
      "users",
      "groups",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "datamodel.add_datamodel_shares",
    "module": "datamodel",
    "class": "DataModel",
    "method": "add_datamodel_shares",
    "description": "Adds share entries (users and groups) to a DataModel.",
    "full_doc": "Adds share entries (users and groups) to a DataModel.\n\nParameters:\n    datamodel_name (str): Name of the DataModel to add shares to.\n    shares (list): List of dictionaries containing share details. Each dictionary should have:\n        - name: Name of the user or group\n        - type: Type of the party (user or group)\n        - permission: Permission level (EDIT, READ, USE)\n\nReturns:\n    dict: Result of the share addition operation.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel to add shares to."
        },
        "shares": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "List of dictionaries containing share details. Each dictionary should have: - name: Name of the user or group - type: Type of the party (user or group) - permission: Permission level (EDIT, READ, USE)"
        }
      },
      "required": [
        "datamodel_name",
        "shares"
      ]
    },
    "mutates": true,
    "tags": [
      "datamodel",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "datamodel.create_connections",
    "module": "datamodel",
    "class": "DataModel",
    "method": "create_connections",
    "description": "Creates a new connection using the provided payload.",
    "full_doc": "Creates a new connection using the provided payload.\n\nParameters:\n    connection_payload (dict): The configuration payload for the connection.\n\nReturns:\n    dict or None: JSON response with connection details if successful, otherwise None.",
    "parameters": {
      "type": "object",
      "properties": {
        "connection_payload": {
          "type": "object",
          "description": "The configuration payload for the connection."
        }
      },
      "required": [
        "connection_payload"
      ]
    },
    "mutates": true,
    "tags": [
      "datamodel",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "datamodel.create_datamodel",
    "module": "datamodel",
    "class": "DataModel",
    "method": "create_datamodel",
    "description": "Creates a new DataModel in Sisense.",
    "full_doc": "Creates a new DataModel in Sisense.\n\nParameters:\n    datamodel_name (str): Name of the DataModel.\n    datamodel_type (str): Type of the DataModel. Should be either \"extract\" (for Elasticube)\n        or \"live\" (for Live).\n\nReturns:\n    dict: Dictionary with the DataModel ID if created successfully, or an error message.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel."
        },
        "datamodel_type": {
          "type": "string",
          "description": "Either 'extract' (Elasticube/EC) or 'live'. If user says 'elasticube' or 'ec', normalize to 'extract'.",
          "enum": [
            "extract",
            "live"
          ],
          "x-aliases": {
            "extract": [
              "ec",
              "elasticube",
              "elastic cube",
              "cube",
              "elastic-cube"
            ],
            "live": [
              "realtime",
              "real-time",
              "live model"
            ]
          }
        }
      },
      "required": [
        "datamodel_name",
        "datamodel_type"
      ]
    },
    "mutates": true,
    "tags": [
      "datamodel",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "datamodel.create_dataset",
    "module": "datamodel",
    "class": "DataModel",
    "method": "create_dataset",
    "description": "Creates a new dataset in the specified DataModel.",
    "full_doc": "Creates a new dataset in the specified DataModel.\n\nParameters:\n    datamodel_name (str): Name of the DataModel where the dataset will be created.\n    connection_name (str): Name of the connection to use.\n    database_name (str): Name of the data source database.\n    schema_name (str): Name of the data source schema.\n    dataset_name (str, optional): Name of the dataset. Defaults to schema name if not provided.\n\nReturns:\n    dict: A dictionary containing the full dataset object on success, or an error message on failure.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel where the dataset will be created."
        },
        "connection_name": {
          "type": "string",
          "description": "Name of the connection to use."
        },
        "database_name": {
          "type": "string",
          "description": "Name of the data source database."
        },
        "schema_name": {
          "type": "string",
          "description": "Name of the data source schema."
        },
        "dataset_name": {
          "type": "string",
          "description": "Name of the dataset. Defaults to schema name if not provided."
        }
      },
      "required": [
        "datamodel_name",
        "connection_name",
        "database_name",
        "schema_name"
      ]
    },
    "mutates": true,
    "tags": [
      "datamodel",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "datamodel.create_table",
    "module": "datamodel",
    "class": "DataModel",
    "method": "create_table",
    "description": "Create a new table in the specified DataModel.",
    "full_doc": "Create a new table in the specified DataModel.\n\nParameters:\n    datamodel_name (str): Name of the DataModel where the table will be created.\n    table_name (str): Name of the table to create.\n    database_name (str, optional): Name of the data source database.\n        If not provided, will try to infer from the DataModel.\n    schema_name (str, optional): Name of the data source schema.\n        If not provided, will try to infer from the DataModel.\n    dataset_id (str, optional): ID of the dataset where the table will be created.\n        If not provided, will try to infer from the DataModel.\n    import_query (str, optional): SQL statement used as custom import query. Defaults to None.\n    description (str, optional): Description for the table. Defaults to an empty string.\n    tags (list, optional): List of tags to apply to the table. Defaults to None.\n    build_behavior_config (dict, optional): Configuration for table build behavior.\n\nReturns:\n    dict: Table object if created successfully or an error message.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel where the table will be created."
        },
        "table_name": {
          "type": "string",
          "description": "Name of the table to create."
        },
        "database_name": {
          "type": "string",
          "description": "Name of the data source database. If not provided, will try to infer from the DataModel."
        },
        "schema_name": {
          "type": "string",
          "description": "Name of the data source schema. If not provided, will try to infer from the DataModel."
        },
        "dataset_id": {
          "type": "string",
          "description": "ID of the dataset where the table will be created. If not provided, will try to infer from the DataModel."
        },
        "import_query": {
          "type": "string",
          "description": "SQL statement used as custom import query. Defaults to None."
        },
        "description": {
          "type": "string",
          "description": "Description for the table. Defaults to an empty string."
        },
        "tags": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "List of tags to apply to the table. Defaults to None."
        },
        "build_behavior_config": {
          "type": "object",
          "description": "Configuration for table build behavior."
        }
      },
      "required": [
        "datamodel_name",
        "table_name"
      ]
    },
    "mutates": true,
    "tags": [
      "datamodel",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "datamodel.deploy_datamodel",
    "module": "datamodel",
    "class": "DataModel",
    "method": "deploy_datamodel",
    "description": "Deploy (build or publish) the specified DataModel based on its type.",
    "full_doc": "Deploy (build or publish) the specified DataModel based on its type.\n\nThis method supports both Elasticube (EXTRACT) and Live models.\nThe behavior and required parameters differ based on model type:\n\n- For Elasticube models:\n    - build_type (str): Type of deployment. Options:\n        * \"schema_changes\" \u2014 Build only schema changes\n        * \"by_table\" \u2014 Build based on each table's config (e.g. incremental, accumulative)\n        * \"full\" \u2014 Rebuild the entire model from scratch (default)\n    - row_limit (int): Maximum number of rows to process. Defaults to 0 (no limit).\n    - schema_origin (str): Schema source. Options:\n        * \"latest\" \u2014 Build the schema as seen in the Data page (default)\n        * \"running\" \u2014 Build from the last successfully built version\n\n- For Live models:\n    - Only the `build_type` parameter is used internally and will be set to \"publish\"\n    - `row_limit` and `schema_origin` are ignored\n\nParameters:\n    datamodel_name (str): Name of the DataModel to deploy.\n    build_type (str): Type of deployment. Required for EXTRACT only.\n    row_limit (int): Row limit for build. Applicable only for EXTRACT.\n    schema_origin (str): Schema origin for build. Applicable only for EXTRACT.\n\nReturns:\n    dict: Deployment result including status, or error details.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel to deploy."
        },
        "build_type": {
          "type": "string",
          "description": "Build strategy for extract models. Omit for live/publish.",
          "enum": [
            "full",
            "by_table"
          ],
          "x-aliases": {
            "full": [
              "build",
              "rebuild",
              "start",
              "run",
              "execute",
              "refresh"
            ],
            "by_table": [
              "by-table",
              "table-wise",
              "incremental-tables"
            ]
          }
        },
        "row_limit": {
          "type": "integer",
          "description": "Row limit for build. Applicable only for EXTRACT.",
          "minimum": 1
        },
        "schema_origin": {
          "type": "string",
          "description": "Schema origin for build. Applicable only for EXTRACT.",
          "enum": [
            "latest",
            "schema_changes"
          ]
        }
      },
      "required": [
        "datamodel_name"
      ]
    },
    "mutates": true,
    "tags": [
      "datamodel",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "datamodel.describe_datamodel",
    "module": "datamodel",
    "class": "DataModel",
    "method": "describe_datamodel",
    "description": "Retrieve detailed datamodel structure in a flat, row-based format suitable for DataFrame or CSV export.",
    "full_doc": "Retrieve detailed datamodel structure in a flat, row-based format suitable for DataFrame or CSV export.\n\nParameters:\n    datamodel_name (str): Name of the DataModel to describe.\n\nReturns:\n    list: List of dictionaries, each representing a single table row with context (datamodel, dataset, table).",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel to describe."
        }
      },
      "required": [
        "datamodel_name"
      ]
    },
    "mutates": false,
    "tags": [
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "datamodel.describe_datamodel_raw",
    "module": "datamodel",
    "class": "DataModel",
    "method": "describe_datamodel_raw",
    "description": "Retrieve detailed information about a specific DataModel, including share details.",
    "full_doc": "Retrieve detailed information about a specific DataModel, including share details.\n\nParameters:\n    datamodel_name (str): Name of the DataModel to describe.\n\nReturns:\n    dict: Detailed information about the DataModel, or an error message if not found.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel to describe."
        }
      },
      "required": [
        "datamodel_name"
      ]
    },
    "mutates": false,
    "tags": [
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "datamodel.generate_connections_payload",
    "module": "datamodel",
    "class": "DataModel",
    "method": "generate_connections_payload",
    "description": "Generates the appropriate connections payload based on the datasource type.",
    "full_doc": "Generates the appropriate connections payload based on the datasource type.\n\nParameters:\n    datasource_type (str): Type of datasource (e.g., \"ATHENA\", \"SNOWFLAKE\", \"ORACLE\").\n    connection_params (dict): Connection details for the datasource.\n\nReturns:\n    dict: Connections payload.",
    "parameters": {
      "type": "object",
      "properties": {
        "datasource_type": {
          "type": "string",
          "description": "Type of datasource (e.g., \"ATHENA\", \"SNOWFLAKE\", \"ORACLE\")."
        },
        "connection_params": {
          "type": "object",
          "description": "Connection details for the datasource."
        }
      },
      "required": [
        "datasource_type",
        "connection_params"
      ]
    },
    "mutates": false,
    "tags": [
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "datamodel.get_all_datamodel",
    "module": "datamodel",
    "class": "DataModel",
    "method": "get_all_datamodel",
    "description": "Retrieves metadata details of all DataModels using an undocumented internal API.",
    "full_doc": "Retrieves metadata details of all DataModels using an undocumented internal API.\nThis includes additional fields like build status, size, and timestamps that may\nnot be available through the standard public endpoints.\n\nReturns:\n    dict: Parsed metadata details of all DataModels, or a dictionary with an error message.",
    "parameters": {
      "type": "object",
      "properties": {},
      "required": []
    },
    "mutates": false,
    "tags": [
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "datamodel.get_connection",
    "module": "datamodel",
    "class": "DataModel",
    "method": "get_connection",
    "description": "Retrieves a Connection by its name.",
    "full_doc": "Retrieves a Connection by its name.\n\nParameters:\n    connection_name (str): Name of the connection to filter by.\n\nReturns:\n    List: Connection details if found, or a dictionary with an error message.",
    "parameters": {
      "type": "object",
      "properties": {
        "connection_name": {
          "type": "string",
          "description": "Name of the connection to filter by."
        }
      },
      "required": [
        "connection_name"
      ]
    },
    "mutates": false,
    "tags": [
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "datamodel.get_data",
    "module": "datamodel",
    "class": "DataModel",
    "method": "get_data",
    "description": "Retrieves data from a specific table in a DataModel and returns it as a list of dicts",
    "full_doc": "Retrieves data from a specific table in a DataModel and returns it as a list of dicts\n(row-based format) compatible with to_dataframe.\n\nParameters:\n    datamodel_name (str): Name of the DataModel.\n    table_name (str): Name of the table to retrieve data from.\n    query (str): Optional SQL query to filter the data.\n\nReturns:\n    list: List of dictionaries where each dict represents a row.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel."
        },
        "table_name": {
          "type": "string",
          "description": "Name of the table to retrieve data from."
        },
        "query": {
          "type": "string",
          "description": "Optional SQL query to filter the data."
        }
      },
      "required": [
        "datamodel_name",
        "table_name"
      ]
    },
    "mutates": false,
    "tags": [
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "datamodel.get_datamodel",
    "module": "datamodel",
    "class": "DataModel",
    "method": "get_datamodel",
    "description": "Retrieves a DataModel by its name.",
    "full_doc": "Retrieves a DataModel by its name.\n\nParameters:\n    datamodel_name (str): Name of the DataModel to retrieve.\n\nReturns:\n    dict: DataModel details if found, or a dictionary with an error message.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel to retrieve."
        }
      },
      "required": [
        "datamodel_name"
      ]
    },
    "mutates": false,
    "tags": [
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "datamodel.get_datamodel_shares",
    "module": "datamodel",
    "class": "DataModel",
    "method": "get_datamodel_shares",
    "description": "Retrieves all share entries (users and groups) for a given DataModel in flat row format.",
    "full_doc": "Retrieves all share entries (users and groups) for a given DataModel in flat row format.\n\nParameters:\n    datamodel_name (str): Name of the DataModel to retrieve shares for.\n\nReturns:\n    list: List of dicts with datamodel name, party name, type, and permission.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel to retrieve shares for."
        }
      },
      "required": [
        "datamodel_name"
      ]
    },
    "mutates": false,
    "tags": [
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "datamodel.get_datasecurity",
    "module": "datamodel",
    "class": "DataModel",
    "method": "get_datasecurity",
    "description": "Retrieves datasecurity table and column entries for a given DataModel in flat row format.",
    "full_doc": "Retrieves datasecurity table and column entries for a given DataModel in flat row format.\n\nParameters:\n    datamodel_name (str): Name of the DataModel to retrieve datasecurity for.\n\nReturns:\n    list: List of dicts with datamodel name, table name, column name, and security type.\n        If no rules exist, a single row is returned with empty values and the datamodel name.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel to retrieve datasecurity for."
        }
      },
      "required": [
        "datamodel_name"
      ]
    },
    "mutates": false,
    "tags": [
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "datamodel.get_datasecurity_detail",
    "module": "datamodel",
    "class": "DataModel",
    "method": "get_datasecurity_detail",
    "description": "Retrieves detailed datasecurity rules for a specific DataModel, including share-level visibility.",
    "full_doc": "Retrieves detailed datasecurity rules for a specific DataModel, including share-level visibility.\nEach row represents a unique column-level rule and is repeated per share for clarity.\n\nSpecial handling is applied to interpret member values:\n- If \"members\" is an empty list and \"exclusionary\" is missing/null => interpreted as \"Nothing\"\n- If \"members\" is empty and \"exclusionary\" is False => interpreted as \"Everything\"\n- If values exist and \"exclusionary\" is True => treated as restricted subset\n\nParameters:\n    datamodel_name (str): Name of the DataModel to retrieve datasecurity rules for.\n\nReturns:\n    list: A list of dictionaries representing datasecurity rules in flat, share-resolved format.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel to retrieve datasecurity rules for."
        }
      },
      "required": [
        "datamodel_name"
      ]
    },
    "mutates": false,
    "tags": [
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "datamodel.get_model_schema",
    "module": "datamodel",
    "class": "DataModel",
    "method": "get_model_schema",
    "description": "Retrieves the schema of a DataModel, including tables and columns.",
    "full_doc": "Retrieves the schema of a DataModel, including tables and columns.\n\nParameters:\n    datamodel_name (str): Name of the DataModel to retrieve the schema for.\n\nReturns:\n    list: A list of dictionaries containing schema information (one per column),\n        or an error message if not found.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel to retrieve the schema for."
        }
      },
      "required": [
        "datamodel_name"
      ]
    },
    "mutates": false,
    "tags": [
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "datamodel.get_row_count",
    "module": "datamodel",
    "class": "DataModel",
    "method": "get_row_count",
    "description": "Retrieves the row count for each table in a specific DataModel",
    "full_doc": "Retrieves the row count for each table in a specific DataModel\nand returns it in a flat row-based structure suitable for tabular representation.\n\nParameters:\n    datamodel_name (str): Name of the DataModel.\n\nReturns:\n    list: List of dictionaries, each with 'table_name' and 'row_count'.\n        Includes an additional row for total row count.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel."
        }
      },
      "required": [
        "datamodel_name"
      ]
    },
    "mutates": false,
    "tags": [
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "datamodel.get_table_schema",
    "module": "datamodel",
    "class": "DataModel",
    "method": "get_table_schema",
    "description": "Retrieves the schema of a table in a specified connection from Data Source.",
    "full_doc": "Retrieves the schema of a table in a specified connection from Data Source.\nThis method uses an undocumented Sisense API endpoint to fetch the table schema details.\nNOTE: This endpoint is undocumented and may change in future versions of Sisense.\nIt is recommended to use this method with caution.\n\nParameters:\n    connection_name (str): Name of the connection.\n    database_name (str): Name of the database.\n    schema_name (str): Name of the schema.\n    table_name (str): Name of the table.\n\nReturns:\n    dict: Table schema details if found, or a dictionary with an error message.",
    "parameters": {
      "type": "object",
      "properties": {
        "connection_name": {
          "type": "string",
          "description": "Name of the connection."
        },
        "database_name": {
          "type": "string",
          "description": "Name of the database."
        },
        "schema_name": {
          "type": "string",
          "description": "Name of the schema."
        },
        "table_name": {
          "type": "string",
          "description": "Name of the table."
        }
      },
      "required": [
        "connection_name",
        "database_name",
        "schema_name",
        "table_name"
      ]
    },
    "mutates": false,
    "tags": [
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "datamodel.resolve_datamodel_reference",
    "module": "datamodel",
    "class": "DataModel",
    "method": "resolve_datamodel_reference",
    "description": "Resolve a data model reference (ID or title) to a concrete data model ID and title.",
    "full_doc": "Resolve a data model reference (ID or title) to a concrete data model ID and title.\n\nThis helper accepts a single string that may be either:\n- a Sisense data model ID, or\n- a data model title (schema name).\n\nIt first attempts to treat the reference as an ID using\n`/api/v2/datamodels/{id}/schema`. If that fails, it falls back to\ncalling `/api/v2/datamodels/schema` with a `title` query parameter.\n\nParameters\n----------\ndatamodel_ref : str\n    Data model reference to resolve. This can be either an ID or a name.\n\nReturns\n-------\ndict\n    A dictionary with the following keys:\n    - success (bool): True if the reference was resolved to a data model.\n    - status_code (int): 200 if resolved successfully, 404 if not found,\n      or 500 if an unexpected error occurred.\n    - datamodel_id (str or None): Resolved data model ID (oid) if found,\n      otherwise None.\n    - datamodel_title (str or None): Resolved data model title if found,\n      otherwise None.\n    - error (str or None): Error message if success is False, otherwise None.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_ref": {
          "type": "string",
          "description": "Data model reference to resolve. This can be either an ID or a name."
        }
      },
      "required": [
        "datamodel_ref"
      ]
    },
    "mutates": false,
    "tags": [
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "datamodel.setup_datamodel",
    "module": "datamodel",
    "class": "DataModel",
    "method": "setup_datamodel",
    "description": "Setup a DataModel using existing connection and by creating a datamodel, dataset, and table.",
    "full_doc": "Setup a DataModel using existing connection and by creating a datamodel, dataset, and table.\n\nParameters:\n    datamodel_name (str): Name of the DataModel.\n    datamodel_type (str): Type of DataModel. Should be either \"extract\" (for Elasticube) or \"live\" (for Live).\n    connection_name (str): Name of the connection to use.\n    database_name (str): Name of the data source database.\n    schema_name (str): Name of the data source schema.\n    dataset_name (str, optional): Name of the dataset. Defaults to schema name if not provided.\n    tables (list): List of tables to create in the DataModel.\n        Each table should be a dictionary with keys:\n        \"table_name\", \"import_query\", \"description\", \"tags\", and \"build_behavior_config\".\n        import_query (str, optional): SQL statement used as custom import query. Defaults to None.\n        description (str, optional): Description for the table. Defaults to an empty string.\n        tags (list, optional): List of tags to apply to the table. Defaults to None.\n        build_behavior_config (dict, optional): Configuration for table build behavior.\n\nReturns:\n    dict: A dictionary containing the full DataModel object on success or an error message on failure.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel."
        },
        "datamodel_type": {
          "type": "string",
          "description": "Type of DataModel. Should be either \"extract\" (for Elasticube) or \"live\" (for Live).",
          "enum": [
            "extract",
            "live"
          ],
          "x-aliases": {
            "extract": [
              "ec",
              "elasticube",
              "elastic cube",
              "cube",
              "elastic-cube"
            ],
            "live": [
              "realtime",
              "real-time",
              "live model"
            ]
          }
        },
        "connection_name": {
          "type": "string",
          "description": "Name of the connection to use."
        },
        "database_name": {
          "type": "string",
          "description": "Name of the data source database."
        },
        "schema_name": {
          "type": "string",
          "description": "Name of the data source schema."
        },
        "tables": {
          "type": "array",
          "description": "List of tables to add. For 'live' models, build_behavior_config is ignored. For 'extract' models, set build_behavior_config as needed.",
          "items": {
            "type": "object",
            "properties": {
              "database_name": {
                "type": "string",
                "description": "Optional override of the table's database. Defaults to top-level database_name if omitted."
              },
              "schema_name": {
                "type": "string",
                "description": "Optional override of the table's schema. Defaults to top-level schema_name if omitted."
              },
              "table_name": {
                "type": "string",
                "description": "Physical table name to add, or a logical name when using import_query."
              },
              "import_query": {
                "type": "string",
                "description": "Optional custom SQL to import rows (e.g., SELECT ... LIMIT 10). If provided, supersedes physical table fetch."
              },
              "description": {
                "type": "string",
                "description": "Optional table description."
              },
              "tags": {
                "type": "array",
                "description": "Optional list of tags for the table.",
                "items": {
                  "type": "string"
                }
              },
              "build_behavior_config": {
                "type": "object",
                "description": "Only used for 'extract' models. Ignored for 'live'. For 'increment' mode, column_name is required.",
                "properties": {
                  "mode": {
                    "type": "string",
                    "enum": [
                      "replace",
                      "replace_changes",
                      "append",
                      "increment"
                    ],
                    "description": "Table build behavior for extract models."
                  },
                  "column_name": {
                    "type": "string",
                    "description": "Required when mode='increment'; ignored otherwise."
                  }
                }
              }
            },
            "required": [
              "table_name"
            ]
          },
          "minItems": 1
        },
        "dataset_name": {
          "type": "string",
          "description": "Name of the dataset. Defaults to schema name if not provided."
        },
        "row_limit": {
          "type": "integer",
          "minimum": 1
        }
      },
      "required": [
        "datamodel_name",
        "datamodel_type",
        "connection_name",
        "database_name",
        "schema_name",
        "tables"
      ]
    },
    "mutates": true,
    "tags": [
      "datamodel",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "dashboard.add_dashboard_script",
    "module": "dashboard",
    "class": "Dashboard",
    "method": "add_dashboard_script",
    "description": "Adds or overwrites a script to a dashboard, temporarily changing ownership if required.",
    "full_doc": "Adds or overwrites a script to a dashboard, temporarily changing ownership if required.\n\nParameters:\n    dashboard_id (str): The ID of the dashboard where the script will be added.\n    script (str): The JavaScript script as either:\n                - A properly formatted JSON string.\n                - A raw Python docstring (multi-line string).\n    executing_user (str, optional): The username of the API user. This is used to temporarily change\n                                the owner of the dashboard, as only the owner can add scripts.\n                                If not provided, assumes the dashboard owner is the same as the API user.\n\nReturns:\n    str: Success message or error details.",
    "parameters": {
      "type": "object",
      "properties": {
        "dashboard_id": {
          "type": "string",
          "description": "The ID of the dashboard where the script will be added."
        },
        "script": {
          "type": "string",
          "description": "The JavaScript script as either: - A properly formatted JSON string. - A raw Python docstring (multi-line string)."
        },
        "executing_user": {
          "type": "string",
          "description": "The username of the API user. This is used to temporarily change the owner of the dashboard, as only the owner can add scripts. If not provided, assumes the dashboard owner is the same as the API user."
        }
      },
      "required": [
        "dashboard_id",
        "script"
      ]
    },
    "mutates": true,
    "tags": [
      "dashboard",
      "dashboards",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "dashboard.add_dashboard_shares",
    "module": "dashboard",
    "class": "Dashboard",
    "method": "add_dashboard_shares",
    "description": "Adds or updates shares for a dashboard, specifying users and groups along with their access rules.",
    "full_doc": "Adds or updates shares for a dashboard, specifying users and groups along with their access rules.\n\nParameters:\n    dashboard_id (str): The ID of the dashboard to which the shares will be added.\n    shares (list of dicts): A list of dictionaries, each containing:\n        - \"name\" (str): The username or group name.\n        - \"type\" (str): Either \"user\" or \"group\" to indicate the share type.\n        - \"rule\" (str): The access level (e.g., \"view\", \"edit\").\n\nReturns:\n    str: Success message or error details.",
    "parameters": {
      "type": "object",
      "properties": {
        "dashboard_id": {
          "type": "string",
          "description": "The ID of the dashboard to which the shares will be added."
        },
        "shares": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "A list of dictionaries, each containing: - \"name\" (str): The username or group name. - \"type\" (str): Either \"user\" or \"group\" to indicate the share type. - \"rule\" (str): The access level (e.g., \"view\", \"edit\")."
        }
      },
      "required": [
        "dashboard_id",
        "shares"
      ]
    },
    "mutates": true,
    "tags": [
      "dashboard",
      "dashboards",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "dashboard.add_widget_script",
    "module": "dashboard",
    "class": "Dashboard",
    "method": "add_widget_script",
    "description": "Adds or overwrites a script for a specific widget within a dashboard.",
    "full_doc": "Adds or overwrites a script for a specific widget within a dashboard.\n\nIf required, temporarily changes the dashboard ownership, as only the owner can modify widget scripts.\n\nParameters:\n    dashboard_id (str): The ID of the dashboard containing the widget.\n    widget_id (str): The ID of the widget where the script will be added.\n    script (str): The JavaScript script as either:\n                - A properly formatted JSON string.\n                - A raw Python docstring (multi-line string).\n    executing_user (str, optional): The username of the API user. This is used to temporarily change\n                                the owner of the dashboard, as only the owner can add scripts.\n                                If not provided, assumes the dashboard owner is the same as the API user.\n\nReturns:\n    str: Success message or error details.",
    "parameters": {
      "type": "object",
      "properties": {
        "dashboard_id": {
          "type": "string",
          "description": "The ID of the dashboard containing the widget."
        },
        "widget_id": {
          "type": "string",
          "description": "The ID of the widget where the script will be added."
        },
        "script": {
          "type": "string",
          "description": "The JavaScript script as either: - A properly formatted JSON string. - A raw Python docstring (multi-line string)."
        },
        "executing_user": {
          "type": "string",
          "description": "The username of the API user. This is used to temporarily change the owner of the dashboard, as only the owner can add scripts. If not provided, assumes the dashboard owner is the same as the API user."
        }
      },
      "required": [
        "dashboard_id",
        "widget_id",
        "script"
      ]
    },
    "mutates": true,
    "tags": [
      "dashboard",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "dashboard.get_all_dashboards",
    "module": "dashboard",
    "class": "Dashboard",
    "method": "get_all_dashboards",
    "description": "Retrieves all dashboards from the Sisense server.",
    "full_doc": "Retrieves all dashboards from the Sisense server.\n\nReturns:\n    list or dict: A list of dashboards if successful,\n                or a dict containing an error message.",
    "parameters": {
      "type": "object",
      "properties": {},
      "required": []
    },
    "mutates": false,
    "tags": [
      "dashboard",
      "dashboards",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "dashboard.get_dashboard_by_id",
    "module": "dashboard",
    "class": "Dashboard",
    "method": "get_dashboard_by_id",
    "description": "Retrieves a specific dashboard by its ID.",
    "full_doc": "Retrieves a specific dashboard by its ID.\n\nParameters:\n    dashboard_id (str): The ID of the dashboard to retrieve.\n\nReturns:\n    dict: A dictionary containing dashboard details if found,\n        or a dict with an error message if the request fails.",
    "parameters": {
      "type": "object",
      "properties": {
        "dashboard_id": {
          "type": "string",
          "description": "The ID of the dashboard to retrieve."
        }
      },
      "required": [
        "dashboard_id"
      ]
    },
    "mutates": false,
    "tags": [
      "dashboard",
      "dashboards",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "dashboard.get_dashboard_by_name",
    "module": "dashboard",
    "class": "Dashboard",
    "method": "get_dashboard_by_name",
    "description": "Retrieves a specific dashboard by its name.",
    "full_doc": "Retrieves a specific dashboard by its name.\n\nParameters:\n    dashboard_name (str): The name of the dashboard to retrieve.\n\nReturns:\n    dict or list: A dictionary containing dashboard details if found,\n                or {'error': 'message'} if not found or failed.",
    "parameters": {
      "type": "object",
      "properties": {
        "dashboard_name": {
          "type": "string",
          "description": "The name of the dashboard to retrieve."
        }
      },
      "required": [
        "dashboard_name"
      ]
    },
    "mutates": false,
    "tags": [
      "dashboard",
      "dashboards",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "dashboard.get_dashboard_columns",
    "module": "dashboard",
    "class": "Dashboard",
    "method": "get_dashboard_columns",
    "description": "Retrieves columns from a specific dashboard, including both widget and filter-level columns.",
    "full_doc": "Retrieves columns from a specific dashboard, including both widget and filter-level columns.\n\nThis method:\n- Uses the `get_dashboard_by_name` method to fetch the dashboard.\n- Extracts columns from widgets and filters.\n- Deduplicates the final column list.\n\nParameters:\n    dashboard_name (str): The name of the dashboard to retrieve columns from.\n\nReturns:\n    list: A list of dictionaries containing distinct table and column information from the dashboard.",
    "parameters": {
      "type": "object",
      "properties": {
        "dashboard_name": {
          "type": "string",
          "description": "The name of the dashboard to retrieve columns from."
        }
      },
      "required": [
        "dashboard_name"
      ]
    },
    "mutates": false,
    "tags": [
      "dashboard",
      "dashboards",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "dashboard.get_dashboard_share",
    "module": "dashboard",
    "class": "Dashboard",
    "method": "get_dashboard_share",
    "description": "Retrieves share details (users and groups) for a specific dashboard by title.",
    "full_doc": "Retrieves share details (users and groups) for a specific dashboard by title.\n\nParameters:\n    dashboard_name (str): The title of the dashboard to retrieve share information for.\n\nReturns:\n    list: A list of dictionaries containing share type (user or group), and share name (email or group name),\n        or an empty list if the dashboard is not found or has no shares.",
    "parameters": {
      "type": "object",
      "properties": {
        "dashboard_name": {
          "type": "string",
          "description": "The title of the dashboard to retrieve share information for."
        }
      },
      "required": [
        "dashboard_name"
      ]
    },
    "mutates": false,
    "tags": [
      "dashboard",
      "dashboards",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "dashboard.resolve_dashboard_reference",
    "module": "dashboard",
    "class": "Dashboard",
    "method": "resolve_dashboard_reference",
    "description": "Resolve a dashboard reference (ID or name) to a concrete dashboard ID and title.",
    "full_doc": "Resolve a dashboard reference (ID or name) to a concrete dashboard ID and title.\n\nThis helper accepts a single string that may be either:\n- a Sisense dashboard ID (24-character ID), or\n- a dashboard title (name).\n\nIt first attempts to treat the reference as an ID using\n`get_dashboard_by_id`. If that fails or the reference does not look\nlike an ID, it falls back to `get_dashboard_by_name`. The underlying\nmethods are reused as-is.\n\nParameters\n----------\ndashboard_ref : str\n    Dashboard reference to resolve. This can be either an ID or a name.\n\nReturns\n-------\ndict\n    A dictionary with the following keys:\n    - success (bool): True if the reference was resolved to a dashboard.\n    - status_code (int): 200 if resolved successfully, 404 if not found,\n      or 500 if an unexpected error occurred.\n    - dashboard_id (str or None): Resolved dashboard ID (oid) if found,\n      otherwise None.\n    - dashboard_title (str or None): Resolved dashboard title if found,\n      otherwise None.\n    - error (str or None): Error message if success is False, otherwise None.",
    "parameters": {
      "type": "object",
      "properties": {
        "dashboard_ref": {
          "type": "string",
          "description": "Dashboard reference to resolve. This can be either an ID or a name."
        }
      },
      "required": [
        "dashboard_ref"
      ]
    },
    "mutates": false,
    "tags": [
      "dashboard",
      "dashboards",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "migration.migrate_all_dashboards",
    "module": "migration",
    "class": "Migration",
    "method": "migrate_all_dashboards",
    "description": "Migrates all dashboards from the source to the target environment in batches.",
    "full_doc": "Migrates all dashboards from the source to the target environment in batches.\n\nParameters\n----------\naction : str, optional\n    Determines how to handle existing dashboards in the target environment.\n    Options:\n    - 'skip': Skip existing dashboards in the target; new dashboards are processed\n    normally, including shares and ownership.\n    - 'overwrite': Overwrite existing dashboards; shares and ownership will not be\n    migrated. If the dashboard already exists, shares will be retained, but the API\n    user will be set as the new owner.\n    - 'duplicate': Create a duplicate of existing dashboards without migrating shares\n    or ownership.\n    Default: None. Existing dashboards are skipped, and only new ones are migrated.\n    Unless existing dashboards are different owners, shares will be migrated.\n    **Note:** If an existing dashboard in the target environment has a different owner\n    than the user's token running the SDK, the dashboard will be migrated with a new\n    ID, and its shares and ownership will be migrated from the original source\n    dashboard.\nrepublish : bool, optional\n    Whether to republish dashboards after migration. Default: False.\nmigrate_share : bool, optional\n    Whether to migrate shares for the dashboards. If `True`, shares will be\n    migrated, and ownership migration will be controlled by the `change_ownership` parameter.\n    If `False`, both shares and ownership migration will be skipped. Default: False.\nchange_ownership : bool, optional\n    Whether to change ownership of the target dashboards.\n    Effective only if `migrate_share` is True. Default: False.\nbatch_size : int, optional\n    Number of dashboards to process in each batch. Default: 10.\nsleep_time : int, optional\n    Time (in seconds) to sleep between batches. Default: 10 seconds.\nemit : Callable[[Dict[str, Any]], None], optional\n    Optional callback invoked with structured progress events. If not provided, the method\n    emits no events and only returns a final result.\n\n    Event payloads follow a consistent shape:\n    - ``type``: str (\"started\" | \"progress\" | \"warning\" | \"error\" | \"completed\")\n    - ``step``: str logical step identifier\n    - ``message``: str human-readable message\n    - Additional fields depending on the step (counts, status_code, etc.)\n\nReturns\n-------\ndict\n    A summary of the migration results for all batches, containing lists of succeeded, skipped,\n    and failed dashboards.\n\nNotes\n-----\n- **Batch Processing**: Dashboards are processed in batches to avoid overloading the system.\n- **Best Use Case**: This method is suitable when migrating all dashboards from a source to a\ntarget environment.\n- **Overwrite Action**: When using `overwrite`, shares and ownership will not be migrated.\nIf a dashboard already exists, the target dashboard will be overwritten, retaining its existing shares\nbut setting the API user as the new owner. Subsequent adjustments to shares and ownership will not be\nsupported in this mode.\n- **Duplicate Action**: Creates duplicate dashboards without shares and ownership migration.\n- **Skip Action**: Skips migration for existing dashboards, but new ones are processed normally.",
    "parameters": {
      "type": "object",
      "properties": {
        "action": {
          "type": "string",
          "description": "Determines how to handle existing dashboards in the target environment. Options: - 'skip': Skip existing dashboards in the target; new dashboards are processed normally, including shares and ownership. - 'overwrite': Overwrite existing dashboards; shares and ownership will not be migrated. If the dashboard already exists, shares will be retained, but the API user will be set as the new owner. - 'duplicate': Create a duplicate of existing dashboards without migrating shares or ownership. Default: None. Existing dashboards are skipped, and only new ones are migrated. Unless existing dashboards are different owners, shares will be migrated. **Note:** If an existing dashboard in the target environment has a different owner than the user's token running the SDK, the dashboard will be migrated with a new ID, and its shares and ownership will be migrated from the original source dashboard.",
          "enum": [
            "skip",
            "overwrite",
            "duplicate"
          ]
        },
        "republish": {
          "type": "boolean",
          "description": "Whether to republish dashboards after migration. Default: False."
        },
        "migrate_share": {
          "type": "boolean",
          "description": "Whether to migrate shares for the dashboards. If `True`, shares will be migrated, and ownership migration will be controlled by the `change_ownership` parameter. If `False`, both shares and ownership migration will be skipped. Default: False."
        },
        "change_ownership": {
          "type": "boolean",
          "description": "Whether to change ownership of the target dashboards. Effective only if `migrate_share` is True. Default: False."
        },
        "batch_size": {
          "type": "integer",
          "description": "Number of dashboards to process in each batch. Default: 10."
        },
        "sleep_time": {
          "type": "integer",
          "description": "Time (in seconds) to sleep between batches. Default: 10 seconds."
        },
        "emit": {
          "type": "object",
          "description": "Optional callback invoked with structured progress events. If not provided, the method emits no events and only returns a final result. Event payloads follow a consistent shape: - ``type``: str (\"started\" | \"progress\" | \"warning\" | \"error\" | \"completed\") - ``step``: str logical step identifier - ``message``: str human-readable message - Additional fields depending on the step (counts, status_code, etc.)"
        }
      },
      "required": []
    },
    "mutates": true,
    "tags": [
      "migration",
      "dashboards",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "migration.migrate_all_datamodels",
    "module": "migration",
    "class": "Migration",
    "method": "migrate_all_datamodels",
    "description": "Migrates all data models from the source environment to the target environment in batches.",
    "full_doc": "Migrates all data models from the source environment to the target environment in batches.\n\nParameters\n----------\ndependencies : list[str] or str or None, default None\n    Dependencies to include in the migration. If None or \"all\", all supported dependencies\n    are included by default.\n\n    Supported values:\n    - \"dataSecurity\" (includes both Data Security and Scope Configuration)\n    - \"formulas\" (for Formulas)\n    - \"hierarchies\" (for Drill Hierarchies)\n    - \"perspectives\" (for Perspectives)\n\nshares : bool, default False\n    Whether to also migrate data model shares after the schema import.\n\nbatch_size : int, default 10\n    Number of data models to migrate per batch.\n\nsleep_time : int, default 5\n    Time (in seconds) to sleep between batches.\n\naction : str or None, default None\n    Strategy to handle existing data models in the target environment.\n\n    - \"overwrite\": Attempts to overwrite an existing model using its original ID via the\n    ``datamodelId`` parameter. If the model is not found in the target environment, it\n    falls back to creating the model.\n    - \"duplicate\": Creates a new model by appending \" (Duplicate)\" to the original name.\n\nemit : Callable[[dict], None] or None, default None\n    Optional callback invoked with structured progress events. If not provided, the method\n    emits no events and only returns a final result.\n\n    Event payloads follow a consistent shape:\n    - ``type``: str (\"started\" | \"progress\" | \"warning\" | \"error\" | \"completed\")\n    - ``step``: str logical step identifier\n    - ``message``: str human-readable message\n    - Additional fields depending on the step (counts, status_code, etc.)\n\nReturns\n-------\ndict\n    A migration summary containing:\n    - ``succeeded``: list\n    - ``skipped``: list\n    - ``failed``: list\n    - Counts/metadata fields (for example: ``total_count``, ``batches_total``, etc.)",
    "parameters": {
      "type": "object",
      "properties": {
        "dependencies": {
          "type": "array",
          "items": {
            "type": "string",
            "enum": [
              "dataSecurity",
              "formulas",
              "hierarchies",
              "perspectives"
            ]
          },
          "description": "Dependencies to include in the migration. If None or \"all\", all supported dependencies are included by default. Supported values: - \"dataSecurity\" (includes both Data Security and Scope Configuration) - \"formulas\" (for Formulas) - \"hierarchies\" (for Drill Hierarchies) - \"perspectives\" (for Perspectives)"
        },
        "shares": {
          "type": "boolean",
          "description": "Whether to also migrate data model shares after the schema import."
        },
        "batch_size": {
          "type": "integer",
          "description": "Number of data models to migrate per batch."
        },
        "sleep_time": {
          "type": "integer",
          "description": "Time (in seconds) to sleep between batches."
        },
        "action": {
          "type": "string",
          "description": "Strategy to handle existing data models in the target environment. - \"overwrite\": Attempts to overwrite an existing model using its original ID via the ``datamodelId`` parameter. If the model is not found in the target environment, it falls back to creating the model. - \"duplicate\": Creates a new model by appending \" (Duplicate)\" to the original name.",
          "enum": [
            "overwrite",
            "duplicate"
          ]
        },
        "emit": {
          "type": "object",
          "description": "Optional callback invoked with structured progress events. If not provided, the method emits no events and only returns a final result. Event payloads follow a consistent shape: - ``type``: str (\"started\" | \"progress\" | \"warning\" | \"error\" | \"completed\") - ``step``: str logical step identifier - ``message``: str human-readable message - Additional fields depending on the step (counts, status_code, etc.)"
        }
      },
      "required": []
    },
    "mutates": true,
    "tags": [
      "migration",
      "datamodel",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "migration.migrate_all_groups",
    "module": "migration",
    "class": "Migration",
    "method": "migrate_all_groups",
    "description": "Migrate groups from the source environment to the target environment using the bulk endpoint.",
    "full_doc": "Migrate groups from the source environment to the target environment using the bulk endpoint.\n\nThis method supports optional progress emission via the ``emit`` callback. If provided,\nthe method will publish structured progress events at key milestones so a caller (for\nexample an MCP server) can stream updates to a UI while the migration is running.\n\nParameters\n----------\nemit : Callable[[Dict[str, Any]], None], optional\n    Optional callback invoked with structured progress events. If not provided, the\n    method behaves like a standard synchronous call and only returns a final result.\n\n    Each emitted event is a dictionary that typically includes:\n    - ``type``: str, event type (e.g., \"started\", \"progress\", \"completed\", \"error\")\n    - ``step``: str, logical step name\n    - ``message``: str, human-readable message\n    - Additional fields depending on the event (counts, status_code, etc.)\n\nReturns\n-------\nDict[str, Any]\n    Structured result payload with:\n    - ``ok``: bool\n    - ``status``: str (\"success\" | \"failed\" | \"noop\")\n    - ``results``: List[Dict[str, str]] per-group statuses\n    - ``source_count``: int\n    - ``eligible_count``: int\n    - ``success_count``: int\n    - ``failed_count``: int\n    - ``raw_error``: Any\n    - ``warnings``: List[str]",
    "parameters": {
      "type": "object",
      "properties": {
        "emit": {
          "type": "object",
          "description": "Optional callback invoked with structured progress events. If not provided, the method behaves like a standard synchronous call and only returns a final result. Each emitted event is a dictionary that typically includes: - ``type``: str, event type (e.g., \"started\", \"progress\", \"completed\", \"error\") - ``step``: str, logical step name - ``message``: str, human-readable message - Additional fields depending on the event (counts, status_code, etc.)"
        }
      },
      "required": []
    },
    "mutates": true,
    "tags": [
      "migration",
      "groups",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "migration.migrate_all_users",
    "module": "migration",
    "class": "Migration",
    "method": "migrate_all_users",
    "description": "Migrate all eligible users from the source environment to the target environment using the bulk endpoint.",
    "full_doc": "Migrate all eligible users from the source environment to the target environment using the bulk endpoint.\n\nThis method is designed to support MCP-style streaming by optionally emitting structured\nprogress events via the ``emit`` callback. It remains synchronous, but callers can run it\nin a worker thread and forward events to an event stream.\n\nThe migration performs the following steps:\n1) Fetch users from the source environment (expanded with groups and role).\n2) Fetch roles and groups from the target environment for ID mapping.\n3) Build a bulk payload by mapping role names to role IDs and group names to group IDs.\n4) Submit the payload to the target bulk endpoint.\n5) Return a structured summary and per-user status list.\n\nParameters\n----------\nemit : Callable[[Dict[str, Any]], None], optional\n    Optional callback invoked with structured progress events. If not provided, the method\n    emits no events and only returns a final result.\n\n    Event payloads follow a consistent shape:\n    - ``type``: str (\"started\" | \"progress\" | \"warning\" | \"error\" | \"completed\")\n    - ``step``: str logical step identifier\n    - ``message``: str human-readable message\n    - Additional fields depending on the step (counts, status_code, etc.)\n\nReturns\n-------\nDict[str, Any]\n    Structured result payload:\n    - ``ok``: bool\n    - ``status``: str (\"success\" | \"failed\" | \"noop\")\n    - ``results``: List[Dict[str, str]] per-user statuses (name=email, status)\n    - ``source_count``: int number of users retrieved from source\n    - ``eligible_count``: int number of users included in the bulk payload\n    - ``skipped_super_count``: int number of sysadmin users skipped\n    - ``missing_role_mappings_count``: int number of users with unresolved role mapping\n    - ``missing_group_mappings_count``: int number of group memberships not found in target\n    - ``success_count``: int\n    - ``failed_count``: int\n    - ``raw_error``: Any error payload if the bulk request fails, else None\n    - ``warnings``: List[str]",
    "parameters": {
      "type": "object",
      "properties": {
        "emit": {
          "type": "object",
          "description": "Optional callback invoked with structured progress events. If not provided, the method emits no events and only returns a final result. Event payloads follow a consistent shape: - ``type``: str (\"started\" | \"progress\" | \"warning\" | \"error\" | \"completed\") - ``step``: str logical step identifier - ``message``: str human-readable message - Additional fields depending on the step (counts, status_code, etc.)"
        }
      },
      "required": []
    },
    "mutates": true,
    "tags": [
      "migration",
      "users",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "migration.migrate_dashboard_shares",
    "module": "migration",
    "class": "Migration",
    "method": "migrate_dashboard_shares",
    "description": "Migrates shares for specific dashboards from the source to the target environment.",
    "full_doc": "Migrates shares for specific dashboards from the source to the target environment.\n\nParameters:\n    source_dashboard_ids (list): A list of dashboard IDs from the source environment to fetch shares from.\n    target_dashboard_ids (list): A list of dashboard IDs from the target environment to apply shares to.\n    change_ownership (bool, optional): Whether to change ownership of the target dashboard. Defaults to False.\n\nReturns:\n    dict: A summary of the share migration process with counts of succeeded and failed shares,\n        and details of failed dashboards.\n\nRaises:\n    ValueError: If `source_dashboard_ids` or `target_dashboard_ids` are not provided,\n                or if their lengths do not match.",
    "parameters": {
      "type": "object",
      "properties": {
        "source_dashboard_ids": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "A list of dashboard IDs from the source environment to fetch shares from."
        },
        "target_dashboard_ids": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "A list of dashboard IDs from the target environment to apply shares to."
        },
        "change_ownership": {
          "type": "boolean",
          "description": "Whether to change ownership of the target dashboard. Defaults to False."
        }
      },
      "required": [
        "source_dashboard_ids",
        "target_dashboard_ids"
      ]
    },
    "mutates": true,
    "tags": [
      "migration",
      "dashboards",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "migration.migrate_dashboards",
    "module": "migration",
    "class": "Migration",
    "method": "migrate_dashboards",
    "description": "Migrate dashboards from the source to the target environment using Sisense bulk import.",
    "full_doc": "Migrate dashboards from the source to the target environment using Sisense bulk import.\n\nThis method exports dashboards from the source and imports them into the target in a single\nbulk request. It then parses the bulk import response to determine which dashboards were\nsucceeded, skipped, and failed. Finally, it optionally migrates shares/ownership for\ndashboards that were actually created in the target (depending on `migrate_share`, `action`).\n\nParameters\n----------\ndashboard_ids : list[str] or None, default None\n    List of dashboard OIDs to migrate. Provide either `dashboard_ids` or `dashboard_names`.\ndashboard_names : list[str] or None, default None\n    List of dashboard titles to migrate. Provide either `dashboard_ids` or `dashboard_names`.\naction : {\"skip\", \"overwrite\", \"duplicate\"} or None, default None\n    Determines how the target handles conflicts.\n    - None: default Sisense behavior (typically skip existing).\n    - \"skip\": skip if exists.\n    - \"overwrite\": overwrite if exists (shares/ownership migration is skipped).\n    - \"duplicate\": create duplicate (shares/ownership migration is skipped).\nrepublish : bool, default False\n    Whether to republish dashboards after import.\nmigrate_share : bool, default False\n    If True, attempts to migrate shares (and optionally ownership) after dashboards are created.\nchange_ownership : bool, default False\n    If True and `migrate_share=True`, attempts to change ownership on the target dashboards.\n\nReturns\n-------\ndict\n    Migration summary with the following keys:\n    - \"succeeded\": list[dict]\n        Each item includes: {\"title\": str, \"source_id\": str | None, \"target_id\": str | None}\n    - \"skipped\": list[dict]\n        Each item includes: {\"title\": str, \"source_id\": str | None, \"target_id\": str | None, \"reason\": str | None}\n    - \"failed\": list[dict]\n        Each item includes: {\"title\": str | None, \"source_id\": str | None, \"reason\": str}\n    - \"meta\": dict\n        Helpful metadata about request-level status.\n\nNotes\n-----\nResponse parsing strategy:\n1) Primary (source of truth): when bulk import returns a success status (typically 201),\nparse the structured response fields:\n- \"succeded\" / \"succeeded\" (Sisense sometimes uses the misspelling)\n- \"skipped\"\n- \"failed\" (often grouped by error category)\n2) Fallbacks (old failsafes): if the response is missing expected fields, not JSON,\nor indicates a request-level error via keys like \"message\" / \"error\", we treat the\nentire batch as failed and attach the best-available reason from:\n- response JSON \"message\" / \"error\" / \"error.message\"\n- response.text (truncated)",
    "parameters": {
      "type": "object",
      "properties": {
        "dashboard_ids": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "List of dashboard OIDs to migrate. Provide either `dashboard_ids` or `dashboard_names`."
        },
        "dashboard_names": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "List of dashboard titles to migrate. Provide either `dashboard_ids` or `dashboard_names`."
        },
        "action": {
          "type": "string",
          "description": "Determines how the target handles conflicts. - None: default Sisense behavior (typically skip existing). - \"skip\": skip if exists. - \"overwrite\": overwrite if exists (shares/ownership migration is skipped). - \"duplicate\": create duplicate (shares/ownership migration is skipped).",
          "enum": [
            "skip",
            "overwrite",
            "duplicate"
          ]
        },
        "republish": {
          "type": "boolean",
          "description": "Whether to republish dashboards after import."
        },
        "migrate_share": {
          "type": "boolean",
          "description": "If True, attempts to migrate shares (and optionally ownership) after dashboards are created."
        },
        "change_ownership": {
          "type": "boolean",
          "description": "If True and `migrate_share=True`, attempts to change ownership on the target dashboards."
        },
        "emit": {
          "type": "string",
          "description": "emit parameter"
        }
      },
      "required": []
    },
    "mutates": true,
    "tags": [
      "migration",
      "dashboards",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "migration.migrate_datamodels",
    "module": "migration",
    "class": "Migration",
    "method": "migrate_datamodels",
    "description": "Migrates specific data models from the source environment to the target environment.",
    "full_doc": "Migrates specific data models from the source environment to the target environment.\n\nParameters\n----------\ndatamodel_ids : list[str] or None, default None\n    A list of data model IDs to migrate. Either `datamodel_ids` or `datamodel_names` must be provided.\ndatamodel_names : list[str] or None, default None\n    A list of data model names to migrate. Either `datamodel_ids` or `datamodel_names` must be provided.\nprovider_connection_map : dict[str, str] or None, default None\n    A dictionary mapping provider names to connection IDs. This allows specifying different connections\n    per provider.\n    Example:\n    {\n        \"Databricks\": \"Connection ID\",\n        \"GoogleBigQuery\": \"Connection ID\"\n    }\ndependencies : list[str] or str or None, default None\n    A list of dependencies to include in the migration. If not provided or if \"all\" is passed, all\n    dependencies are selected by default.\n\n    Possible values:\n    - \"dataSecurity\" (includes both Data Security and Scope Configuration)\n    - \"formulas\" (for Formulas)\n    - \"hierarchies\" (for Drill Hierarchies)\n    - \"perspectives\" (for Perspectives)\n\n    If left blank or set to \"all\", all dependencies are included by default.\nshares : bool, default False\n    Whether to also migrate the data model's shares.\naction : str or None, default None\n    Strategy to handle existing data models in the target environment.\n\n        - \"overwrite\": Attempts to overwrite existing model using its original ID via the datamodelId parameter.\n        If the model is not found in target environment, it will automatically fall back and create the model.\n        - \"duplicate\": Creates a new model by passing a `new_title` to the `newTitle` parameter of the import API.\n        If `new_title` is not provided, the original title will be used with \" (Duplicate)\" appended.\nnew_title : str or None, default None\n    New name for the duplicated data model. Used only when `action='duplicate'`.\nemit : Callable[[dict], None] or None, default None\n    Optional callback invoked with structured progress events. If not provided, the method emits no events\n    and only returns a final result.\n\n    Event payloads follow a consistent shape:\n    - ``type``: str (\"started\" | \"progress\" | \"warning\" | \"error\" | \"completed\")\n    - ``step``: str logical step identifier\n    - ``message``: str human-readable message\n    - Additional fields depending on the step (counts, status_code, etc.)\n\nReturns\n-------\ndict\n    A summary of the migration results containing lists of succeeded, skipped, and failed data models,\n    plus counts/metadata in a dashboard-consistent format.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_ids": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "A list of data model IDs to migrate. Either `datamodel_ids` or `datamodel_names` must be provided."
        },
        "datamodel_names": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "A list of data model names to migrate. Either `datamodel_ids` or `datamodel_names` must be provided."
        },
        "provider_connection_map": {
          "type": "object",
          "description": "A dictionary mapping provider names to connection IDs. This allows specifying different connections per provider. Example: { \"Databricks\": \"Connection ID\", \"GoogleBigQuery\": \"Connection ID\" }"
        },
        "dependencies": {
          "type": "array",
          "items": {
            "type": "string",
            "enum": [
              "dataSecurity",
              "formulas",
              "hierarchies",
              "perspectives"
            ]
          },
          "description": "A list of dependencies to include in the migration. If not provided or if \"all\" is passed, all dependencies are selected by default. Possible values: - \"dataSecurity\" (includes both Data Security and Scope Configuration) - \"formulas\" (for Formulas) - \"hierarchies\" (for Drill Hierarchies) - \"perspectives\" (for Perspectives) If left blank or set to \"all\", all dependencies are included by default."
        },
        "shares": {
          "type": "boolean",
          "description": "Whether to also migrate the data model's shares."
        },
        "action": {
          "type": "string",
          "description": "Strategy to handle existing data models in the target environment. - \"overwrite\": Attempts to overwrite existing model using its original ID via the datamodelId parameter. If the model is not found in target environment, it will automatically fall back and create the model. - \"duplicate\": Creates a new model by passing a `new_title` to the `newTitle` parameter of the import API. If `new_title` is not provided, the original title will be used with \" (Duplicate)\" appended.",
          "enum": [
            "overwrite",
            "duplicate"
          ]
        },
        "new_title": {
          "type": "string",
          "description": "New name for the duplicated data model. Used only when `action='duplicate'`."
        },
        "emit": {
          "type": "object",
          "description": "Optional callback invoked with structured progress events. If not provided, the method emits no events and only returns a final result. Event payloads follow a consistent shape: - ``type``: str (\"started\" | \"progress\" | \"warning\" | \"error\" | \"completed\") - ``step``: str logical step identifier - ``message``: str human-readable message - Additional fields depending on the step (counts, status_code, etc.)"
        }
      },
      "required": []
    },
    "mutates": true,
    "tags": [
      "migration",
      "datamodel",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "migration.migrate_groups",
    "module": "migration",
    "class": "Migration",
    "method": "migrate_groups",
    "description": "Migrates specific groups from the source environment to the target environment using the bulk endpoint.",
    "full_doc": "Migrates specific groups from the source environment to the target environment using the bulk endpoint.\n\nParameters:\n    group_name_list (list): A list of group names to migrate.\n\nReturns:\n    list: A list of group migration results, including any errors encountered during the process.",
    "parameters": {
      "type": "object",
      "properties": {
        "group_name_list": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "A list of group names to migrate."
        }
      },
      "required": [
        "group_name_list"
      ]
    },
    "mutates": true,
    "tags": [
      "migration",
      "groups",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "migration.migrate_users",
    "module": "migration",
    "class": "Migration",
    "method": "migrate_users",
    "description": "Migrates specific users from the source environment to the target environment.",
    "full_doc": "Migrates specific users from the source environment to the target environment.\n\nParameters:\n    user_name_list (list): A list of user names to migrate.\n\nReturns:\n    list: A list of user migration results, including any errors encountered during the process.",
    "parameters": {
      "type": "object",
      "properties": {
        "user_name_list": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "A list of user names to migrate."
        }
      },
      "required": [
        "user_name_list"
      ]
    },
    "mutates": true,
    "tags": [
      "migration",
      "users",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "wellcheck.check_dashboard_structure",
    "module": "wellcheck",
    "class": "WellCheck",
    "method": "check_dashboard_structure",
    "description": "Analyze the structure of one or more dashboards.",
    "full_doc": "Analyze the structure of one or more dashboards.\n\nThis method:\n  - Counts pivot widgets (pivot / pivot2)\n  - Counts tabber widgets (WidgetsTabber)\n  - Counts accordion widgets via accordionConfig on widgets\n  - Counts jump-to-dashboard (JTD) instances, treated as child dashboards\n\nA \"child dashboard\" is treated as any dashboard referenced as a\njump-to-dashboard target from the parent dashboard, either via\nwidget options (drillTarget) or via dashboard script\n(prism.jumpToDashboard calls).\n\nThis method is intended to back wellcheck tasks and agentic tools that\nrespond to prompts such as:\n  - \"check child dashboards for this dashboard\"\n  - \"check jump-to dashboards on XYZ\"\n  - \"analyze dashboard structure / complexity\"\n\nParameters\n----------\ndashboards : list of str, optional\n    One or more dashboard references to analyze. Each reference can be:\n      - a Sisense dashboard ID, or\n      - a dashboard title (name).\n    At least one dashboard reference is required. At runtime this\n    method is tolerant of a single string being passed instead of a\n    list, and will normalize it to a one-element list.\n\nReturns\n-------\nlist of dict\n    A list with one entry per successfully processed dashboard. Each\n    entry has the keys:\n      - dashboard_id (str): Resolved dashboard ID.\n      - dashboard_title (str): Resolved dashboard title.\n      - pivot_count (int): Number of pivot/pivot2 widgets.\n      - tabber_count (int): Number of WidgetsTabber widgets.\n      - accordion_count (int): Number of accordion widgets detected\n        via accordionConfig on widgets.\n      - jtd_count (int): Number of jump-to-dashboard (JTD) instances\n        (child dashboards) detected in widget options and scripts.\n    If no dashboards are successfully processed, an empty list is\n    returned and details are available in the logs.",
    "parameters": {
      "type": "object",
      "properties": {
        "dashboards": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "One or more dashboard references to analyze. Each reference can be: - a Sisense dashboard ID, or - a dashboard title (name). At least one dashboard reference is required. At runtime this method is tolerant of a single string being passed instead of a list, and will normalize it to a one-element list."
        }
      },
      "required": []
    },
    "mutates": false,
    "tags": [
      "wellcheck",
      "dashboards",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "wellcheck.check_dashboard_widget_counts",
    "module": "wellcheck",
    "class": "WellCheck",
    "method": "check_dashboard_widget_counts",
    "description": "Compute widget counts for one or more dashboards.",
    "full_doc": "Compute widget counts for one or more dashboards.\n\nThis method retrieves each specified dashboard definition, counts the\nnumber of widgets on that dashboard, and returns a per-dashboard\nsummary.\n\nParameters\n----------\ndashboards : list of str, optional\n    One or more dashboard references to analyze. Each reference can be:\n      - a Sisense dashboard ID, or\n      - a dashboard title (name).\n    At least one dashboard reference is required. At runtime this\n    method is tolerant of a single string being passed instead of a\n    list, and will normalize it to a one-element list.\n\nReturns\n-------\nlist of dict\n    A list with one entry per successfully processed dashboard. Each\n    entry contains:\n      - dashboard_id (str): Resolved dashboard ID.\n      - dashboard_title (str): Resolved dashboard title.\n      - widget_count (int): Number of widgets on the dashboard.\n\n    If no dashboards are successfully processed, an empty list is\n    returned and details are available in the logs.",
    "parameters": {
      "type": "object",
      "properties": {
        "dashboards": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "One or more dashboard references to analyze. Each reference can be: - a Sisense dashboard ID, or - a dashboard title (name). At least one dashboard reference is required. At runtime this method is tolerant of a single string being passed instead of a list, and will normalize it to a one-element list."
        }
      },
      "required": []
    },
    "mutates": false,
    "tags": [
      "wellcheck",
      "dashboards",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "wellcheck.check_datamodel_custom_tables",
    "module": "wellcheck",
    "class": "WellCheck",
    "method": "check_datamodel_custom_tables",
    "description": "Inspect custom tables in one or more data models and flag the use of UNION.",
    "full_doc": "Inspect custom tables in one or more data models and flag the use of UNION.\n\nThis method resolves each data model reference (ID or title), retrieves\nits schema from the Sisense API, iterates through all datasets/tables,\nand returns one row per custom table with a flag indicating whether its\nSQL expression contains the word \"union\" (case-insensitive).\n\nParameters\n----------\ndatamodels : list of str, optional\n    One or more data model references to analyze. Each reference can be:\n      - a Sisense data model ID, or\n      - a data model title (name).\n    At least one data model reference is required. At runtime this\n    method is tolerant of a single string being passed instead of a\n    list, and will normalize it to a one-element list.\n\nReturns\n-------\nlist of dict\n    A list with one entry per custom table. Each entry contains:\n      - data_model (str): Data model title.\n      - table (str): Table name.\n      - has_union (str): \"yes\" if the custom table expression contains\n        \"union\" (case-insensitive), otherwise \"no\".\n\n    If no data models are successfully processed, an empty list is\n    returned and details are available in the logs.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodels": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "One or more data model references to analyze. Each reference can be: - a Sisense data model ID, or - a data model title (name). At least one data model reference is required. At runtime this method is tolerant of a single string being passed instead of a list, and will normalize it to a one-element list."
        }
      },
      "required": []
    },
    "mutates": false,
    "tags": [
      "wellcheck",
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "wellcheck.check_datamodel_import_queries",
    "module": "wellcheck",
    "class": "WellCheck",
    "method": "check_datamodel_import_queries",
    "description": "Inspect tables in one or more data models for import queries.",
    "full_doc": "Inspect tables in one or more data models for import queries.\n\nThis method resolves each data model reference (ID or title), loads its\nschema, and checks every table for a ``configOptions.importQuery``\nconfiguration. For each table, it returns a row indicating whether an\nimport query is configured.\n\nParameters\n----------\ndatamodels : list of str, optional\n    One or more data model references to analyze. Each reference can be:\n      - a Sisense data model ID, or\n      - a data model title (name).\n    At least one data model reference is required. At runtime this\n    method is tolerant of a single string being passed instead of a\n    list, and will normalize it to a one-element list.\n\nReturns\n-------\nlist of dict\n    A list with one entry per table across all successfully processed\n    data models. Each entry contains:\n      - data_model (str): Resolved data model title.\n      - table (str): Table name.\n      - has_import_query (str): \"yes\" if an importQuery is present in\n        the table config options, otherwise \"no\".\n\n    If no data models are successfully processed, an empty list is\n    returned and details are available in the logs.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodels": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "One or more data model references to analyze. Each reference can be: - a Sisense data model ID, or - a data model title (name). At least one data model reference is required. At runtime this method is tolerant of a single string being passed instead of a list, and will normalize it to a one-element list."
        }
      },
      "required": []
    },
    "mutates": false,
    "tags": [
      "wellcheck",
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "wellcheck.check_datamodel_island_tables",
    "module": "wellcheck",
    "class": "WellCheck",
    "method": "check_datamodel_island_tables",
    "description": "Identify island tables (tables with no relationships) in one or more data models.",
    "full_doc": "Identify island tables (tables with no relationships) in one or more data models.\n\nThis method retrieves the schema for each specified data model, inspects\nits relations and tables, and returns information about tables that do\nnot participate in any relationship (often called \"island tables\").\n\nParameters\n----------\ndatamodels : list of str, optional\n    One or more data model references to analyze. Each reference can be:\n      - a Sisense data model ID, or\n      - a data model title (name).\n    At least one data model reference is required. At runtime this\n    method is tolerant of a single string being passed instead of a\n    list, and will normalize it to a one-element list.\n\nReturns\n-------\nlist of dict\n    A list with one entry per island table. Each entry contains:\n      - datamodel (str): Data model title.\n      - datamodel_oid (str): Data model ID.\n      - table (str): Table name.\n      - table_oid (str): Table ID.\n      - type (str): Table type (e.g., 'live', 'custom').\n      - relation (str): Always \"no\" for island tables.\n\n    If no data models are successfully processed, an empty list is\n    returned and details are available in the logs.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodels": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "One or more data model references to analyze. Each reference can be: - a Sisense data model ID, or - a data model title (name). At least one data model reference is required. At runtime this method is tolerant of a single string being passed instead of a list, and will normalize it to a one-element list."
        }
      },
      "required": []
    },
    "mutates": false,
    "tags": [
      "wellcheck",
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "wellcheck.check_datamodel_m2m_relationships",
    "module": "wellcheck",
    "class": "WellCheck",
    "method": "check_datamodel_m2m_relationships",
    "description": "Check for potential many-to-many (M2M) relationships between tables",
    "full_doc": "Check for potential many-to-many (M2M) relationships between tables\nin one or more data models.\n\nFor each data model, this method inspects the relation graph, builds\ntable/column pairs from the relations, and runs aggregate SQL queries\nagainst the data source to detect whether both sides of the relation\ncontain duplicate keys. Pairs where each side has more than one\noccurrence of its key are flagged as many-to-many.\n\nParameters\n----------\ndatamodels : list of str, optional\n    One or more data model references to analyze. Each reference can be:\n      - a Sisense data model ID, or\n      - a data model title (name).\n    At least one data model reference is required. At runtime this\n    method is tolerant of a single string being passed instead of a\n    list, and will normalize it to a one-element list.\n\nReturns\n-------\nlist of dict\n    A list with one entry per relation field pair checked. Each entry\n    contains:\n      - data_model (str): Data model title.\n      - left_table (str): Name of the left table.\n      - left_column (str): Name of the left column.\n      - right_table (str): Name of the right table.\n      - right_column (str): Name of the right column.\n      - is_m2m (bool): True when both sides have more than one\n        occurrence of their key, False otherwise.\n\n    If no data models are successfully processed, an empty list is\n    returned and details are available in the logs.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodels": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "One or more data model references to analyze. Each reference can be: - a Sisense data model ID, or - a data model title (name). At least one data model reference is required. At runtime this method is tolerant of a single string being passed instead of a list, and will normalize it to a one-element list."
        }
      },
      "required": []
    },
    "mutates": false,
    "tags": [
      "wellcheck",
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "wellcheck.check_datamodel_rls_datatypes",
    "module": "wellcheck",
    "class": "WellCheck",
    "method": "check_datamodel_rls_datatypes",
    "description": "Inspect row-level security (RLS) rules for one or more data models and",
    "full_doc": "Inspect row-level security (RLS) rules for one or more data models and\nreport the datatype of the columns used in those rules.\n\nThis method resolves each data model reference, fetches its RLS (data\nsecurity) rules from the appropriate API endpoint based on the data\nmodel type (extract or live), and returns one row per unique\n(datamodel, table, column, datatype) combination.\n\nParameters\n----------\ndatamodels : list of str, optional\n    One or more data model references to analyze. Each reference can be:\n      - a Sisense data model ID, or\n      - a data model title (name).\n    At least one data model reference is required. At runtime this\n    method is tolerant of a single string being passed instead of a\n    list, and will normalize it to a one-element list.\n\nReturns\n-------\nlist of dict\n    A list with one entry per unique RLS column. Each entry contains:\n      - datamodel (str): Data model title.\n      - table (str): Table name where RLS is applied.\n      - column (str): Column name used in the RLS rule.\n      - datatype (str): Datatype reported by Sisense for that column.\n\n    If no data models are successfully processed, an empty list is\n    returned and details are available in the logs.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodels": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "One or more data model references to analyze. Each reference can be: - a Sisense data model ID, or - a data model title (name). At least one data model reference is required. At runtime this method is tolerant of a single string being passed instead of a list, and will normalize it to a one-element list."
        }
      },
      "required": []
    },
    "mutates": false,
    "tags": [
      "wellcheck",
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "wellcheck.check_pivot_widget_fields",
    "module": "wellcheck",
    "class": "WellCheck",
    "method": "check_pivot_widget_fields",
    "description": "Analyze pivot widgets on one or more dashboards and report those with many fields.",
    "full_doc": "Analyze pivot widgets on one or more dashboards and report those with many fields.\n\nThis method retrieves each specified dashboard, scans all pivot widgets\n(types containing ``\"pivot\"`` or ``\"pivot2\"``), counts how many fields\n(items) are attached to those widgets, and returns a per-widget summary\nfor any pivot with more than ``max_fields`` fields.\n\nParameters\n----------\ndashboards : list of str, optional\n    One or more dashboard references to analyze. Each reference can be:\n      - a Sisense dashboard ID, or\n      - a dashboard title (name).\n    At least one dashboard reference is required. At runtime this\n    method is tolerant of a single string being passed instead of a\n    list, and will normalize it to a one-element list.\nmax_fields : int, optional\n    Threshold for the number of fields on a pivot widget. Only pivot\n    widgets with more than this number of fields are included in the\n    returned data. Defaults to 20.\n\nReturns\n-------\nlist of dict\n    A list of dictionaries, each describing a pivot widget that exceeds\n    the configured field threshold. Each entry contains:\n      - dashboard_id (str): Resolved dashboard ID.\n      - dashboard_title (str): Resolved dashboard title.\n      - widget_id (str): Pivot widget ID.\n      - has_more_fields (bool): Always True for returned rows, since\n        only widgets above the threshold are included.\n      - field_count (int): Total number of fields (items) in the widget.\n\n    If no dashboards are successfully processed, or no pivot widgets\n    exceed the threshold, an empty list is returned and details are\n    available in the logs.",
    "parameters": {
      "type": "object",
      "properties": {
        "dashboards": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "One or more dashboard references to analyze. Each reference can be: - a Sisense dashboard ID, or - a dashboard title (name). At least one dashboard reference is required. At runtime this method is tolerant of a single string being passed instead of a list, and will normalize it to a one-element list."
        },
        "max_fields": {
          "type": "integer",
          "description": "Threshold for the number of fields on a pivot widget. Only pivot widgets with more than this number of fields are included in the returned data. Defaults to 20."
        }
      },
      "required": []
    },
    "mutates": false,
    "tags": [
      "wellcheck",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "wellcheck.run_full_wellcheck",
    "module": "wellcheck",
    "class": "WellCheck",
    "method": "run_full_wellcheck",
    "description": "Run a composite \"full\" wellcheck across dashboards and data models.",
    "full_doc": "Run a composite \"full\" wellcheck across dashboards and data models.\n\nThis method is a convenience wrapper that orchestrates multiple\ndashboard-level and data-model-level checks and returns a structured\nreport that groups their results.\n\nParameters\n----------\ndashboards : str or list of str, optional\n    One or more dashboard references to analyze. Each reference can be:\n      - a Sisense dashboard ID, or\n      - a dashboard title (name).\n    At runtime this parameter is tolerant of a single string and will\n    normalize it to a one-element list.\ndatamodels : str or list of str, optional\n    One or more data model references to analyze. Each reference can be:\n      - a data model ID, or\n      - a data model title (name).\n    At runtime this parameter is tolerant of a single string and will\n    normalize it to a one-element list.\nmax_pivot_fields : int, optional\n    Threshold used by the pivot-fields check. Any pivot widget with\n    more than this number of fields is flagged.\n\nReturns\n-------\ndict\n    A dictionary with two top-level sections:\n\n    - \"dashboards\": {\n          \"structure\": [...],\n          \"widget_counts\": [...],\n          \"pivot_widget_fields\": [...],\n      }\n\n    - \"datamodels\": {\n          \"custom_tables\": [...],\n          \"island_tables\": [...],\n          \"rls_datatypes\": [...],\n          \"import_queries\": [...],\n          \"m2m_relationships\": [...],\n          \"unused_columns\": [...],\n      }\n\n    Each subsection contains the list of rows returned by the\n    corresponding check method. If a given set of references is not\n    provided or no assets are successfully processed, that subsection\n    will be an empty list.",
    "parameters": {
      "type": "object",
      "properties": {
        "dashboards": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "One or more dashboard references to analyze. Each reference can be: - a Sisense dashboard ID, or - a dashboard title (name). At runtime this parameter is tolerant of a single string and will normalize it to a one-element list."
        },
        "datamodels": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "One or more data model references to analyze. Each reference can be: - a data model ID, or - a data model title (name). At runtime this parameter is tolerant of a single string and will normalize it to a one-element list."
        },
        "max_pivot_fields": {
          "type": "integer",
          "description": "Threshold used by the pivot-fields check. Any pivot widget with more than this number of fields is flagged."
        }
      },
      "required": []
    },
    "mutates": true,
    "tags": [
      "wellcheck",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  }
]