[
  {
    "tool_id": "access.change_folder_and_dashboard_ownership",
    "module": "access",
    "class": "AccessManagement",
    "method": "change_folder_and_dashboard_ownership",
    "description": "Method to change the ownership of folders and optionally dashboards.",
    "full_doc": "Method to change the ownership of folders and optionally dashboards.\nThis method changes the ownership of a target folder and the entire\ntree structure surrounding it, including subfolders, sibling folders,\nand parent folders.\nOptionally, it will also change the ownership of dashboards associated\nwith these folders.\n\nParameters:\n    user_name (str): The user running the tool. This is necessary for\n    API access checks.\n    folder_name (str): The target folder whose ownership needs to be\n    changed.\n    new_owner_name (str): The new owner to whom the folder (and\n    optionally dashboards) ownership will be transferred.\n    original_owner_rule (str, optional): Specifies the ownership rule\n    to set original owner after changing ownership('edit' or 'view').\n    Default is 'edit'.\n    change_dashboard_ownership (bool, optional): Specifies whether to\n    also change the ownership of dashboards in the folder tree. Default\n    is True.",
    "parameters": {
      "type": "object",
      "properties": {
        "executing_user": {
          "type": "string",
          "description": "executing_user parameter"
        },
        "folder_name": {
          "type": "string",
          "description": "The target folder whose ownership needs to be changed."
        },
        "new_owner_name": {
          "type": "string",
          "description": "The new owner to whom the folder (and optionally dashboards) ownership will be transferred."
        },
        "original_owner_rule": {
          "type": "string",
          "description": "Specifies the ownership rule to set original owner after changing ownership('edit' or 'view'). Default is 'edit'."
        },
        "change_dashboard_ownership": {
          "type": "boolean",
          "description": "Specifies whether to also change the ownership of dashboards in the folder tree. Default is True."
        }
      },
      "required": [
        "executing_user",
        "folder_name",
        "new_owner_name"
      ]
    },
    "mutates": true,
    "tags": [
      "access",
      "dashboards",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "Transfer ownership of the 'Sales Reports' folder to user 'jane.doe', including all dashboards.",
        "arguments": {
          "executing_user": "admin.user",
          "folder_name": "Sales Reports",
          "new_owner_name": "jane.doe",
          "original_owner_rule": "edit",
          "change_dashboard_ownership": true
        },
        "notes": "This call transfers ownership of the 'Sales Reports' folder and all associated dashboards to 'jane.doe'. The original owner will retain 'edit' permissions."
      },
      {
        "user_query": "Change the owner of the 'Marketing Data' folder to 'john.smith' but do not change dashboard ownership.",
        "arguments": {
          "executing_user": "admin.user",
          "folder_name": "Marketing Data",
          "new_owner_name": "john.smith",
          "original_owner_rule": "view",
          "change_dashboard_ownership": false
        },
        "notes": "This call changes the ownership of the 'Marketing Data' folder to 'john.smith' without affecting the ownership of dashboards. The original owner will retain 'view' permissions."
      },
      {
        "user_query": "Assign ownership of the 'Finance Reports' folder to 'alex.jones' and update all dashboards in the folder tree.",
        "arguments": {
          "executing_user": "system.admin",
          "folder_name": "Finance Reports",
          "new_owner_name": "alex.jones",
          "original_owner_rule": "edit",
          "change_dashboard_ownership": true
        },
        "notes": "This call assigns ownership of the 'Finance Reports' folder and all dashboards within its tree structure to 'alex.jones'. The original owner will retain 'edit' permissions."
      }
    ]
  },
  {
    "tool_id": "access.create_schedule_build",
    "module": "access",
    "class": "AccessManagement",
    "method": "create_schedule_build",
    "description": "Method to create a schedule build for a DataModel.",
    "full_doc": "Method to create a schedule build for a DataModel.\n\nSupports both cron-based schedules (e.g., every Monday at 9:00 UTC)\nand interval-based schedules (e.g., every 2 days, 1 hour, 30 minutes).\n\nParameters:\n    datamodel_name (str): The name of the DataModel.\n    build_type (str): Optional. Type of the build (e.g., \"ACCUMULATE\", \"FULL\",\n    \"SCHEMA_CHANGES\"). Defaults to \"ACCUMULATE\".\n    days (list, optional): List of days for cron schedule. Eg.: [\"SUN\", \"MON\", \"TUE\", \"WED\", \"THU\", \"FRI\",\n    \"SAT\"] or [\"*\"] for all days.\n    hour (int, optional): Hour in 24-hour format (UTC).\n    minute (int, optional): Minute of the hour (UTC).\n    interval_days (int, optional): Interval in days.\n    interval_hours (int, optional): Interval in hours.\n    interval_minutes (int, optional): Interval in minutes.\n\nReturns:\n    dict: API response or error.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "The name of the DataModel."
        },
        "build_type": {
          "type": "string",
          "description": "Optional. Type of the build (e.g., \"ACCUMULATE\", \"FULL\", \"SCHEMA_CHANGES\"). Defaults to \"ACCUMULATE\"."
        },
        "days": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "List of days for cron schedule. Eg.: [\"SUN\", \"MON\", \"TUE\", \"WED\", \"THU\", \"FRI\", \"SAT\"] or [\"*\"] for all days."
        },
        "hour": {
          "type": "integer",
          "description": "Hour in 24-hour format (UTC)."
        },
        "minute": {
          "type": "integer",
          "description": "Minute of the hour (UTC)."
        },
        "interval_days": {
          "type": "integer",
          "description": "Interval in days."
        },
        "interval_hours": {
          "type": "integer",
          "description": "Interval in hours."
        },
        "interval_minutes": {
          "type": "integer",
          "description": "Interval in minutes."
        }
      },
      "required": [
        "datamodel_name"
      ]
    },
    "mutates": true,
    "tags": [
      "access",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "Schedule a daily build for the 'SalesDataModel' at 2:00 UTC.",
        "arguments": {
          "datamodel_name": "SalesDataModel",
          "build_type": "ACCUMULATE",
          "days": [
            "*"
          ],
          "hour": 2,
          "minute": 0
        },
        "notes": "This call schedules a daily build for the specified DataModel at 2:00 UTC using the default 'ACCUMULATE' build type."
      },
      {
        "user_query": "Set up a weekly build for the 'MarketingAnalytics' DataModel every Monday at 9:30 UTC.",
        "arguments": {
          "datamodel_name": "MarketingAnalytics",
          "build_type": "FULL",
          "days": [
            "MON"
          ],
          "hour": 9,
          "minute": 30
        },
        "notes": "This call schedules a weekly build for the specified DataModel every Monday at 9:30 UTC using the 'FULL' build type."
      },
      {
        "user_query": "Create an interval-based build for the 'CustomerInsights' DataModel to run every 3 days, 4 hours, and 15 minutes.",
        "arguments": {
          "datamodel_name": "CustomerInsights",
          "build_type": "SCHEMA_CHANGES",
          "interval_days": 3,
          "interval_hours": 4,
          "interval_minutes": 15
        },
        "notes": "This call sets up an interval-based schedule for the specified DataModel to run every 3 days, 4 hours, and 15 minutes using the 'SCHEMA_CHANGES' build type."
      }
    ]
  },
  {
    "tool_id": "access.create_user",
    "module": "access",
    "class": "AccessManagement",
    "method": "create_user",
    "description": "Creates a new user by processing the provided user data to replace role",
    "full_doc": "Creates a new user by processing the provided user data to replace role\nnames and group names with their corresponding IDs, then sends a POST\nrequest to create the user.\n\nParameters:\n    user_data (dict): A dictionary containing user details such as\n    email, firstName, lastName, role (role name), groups (list of group\n    names), and preferences.\n\nReturns:\n    dict: The response from the API if successful,\n        or a dictionary with an 'error' key if the operation fails.",
    "parameters": {
      "type": "object",
      "properties": {
        "user_data": {
          "type": "object",
          "description": "A dictionary containing user details such as email, firstName, lastName, role (role name), groups (list of group names), and preferences."
        }
      },
      "required": [
        "user_data"
      ]
    },
    "mutates": true,
    "tags": [
      "access",
      "users",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "Add a new user with the role of 'Admin' and assign them to the 'Finance' and 'HR' groups.",
        "arguments": {
          "user_data": {
            "email": "jane.doe@example.com",
            "firstName": "Jane",
            "lastName": "Doe",
            "role": "Admin",
            "groups": [
              "Finance",
              "HR"
            ],
            "preferences": {
              "language": "en",
              "timezone": "UTC"
            }
          }
        },
        "notes": "This call creates a new user with administrative privileges and assigns them to the specified groups. Use this when onboarding a new admin user."
      },
      {
        "user_query": "Create a user with the role of 'Viewer' and add them to the 'Marketing' group.",
        "arguments": {
          "user_data": {
            "email": "john.smith@example.com",
            "firstName": "John",
            "lastName": "Smith",
            "role": "Viewer",
            "groups": [
              "Marketing"
            ],
            "preferences": {
              "language": "en",
              "timezone": "America/New_York"
            }
          }
        },
        "notes": "This call adds a new user with view-only access and assigns them to the Marketing group. Use this for users who only need to view dashboards."
      },
      {
        "user_query": "Add a new user with the role of 'Editor' and assign them to the 'Sales' group with custom preferences.",
        "arguments": {
          "user_data": {
            "email": "alex.taylor@example.com",
            "firstName": "Alex",
            "lastName": "Taylor",
            "role": "Editor",
            "groups": [
              "Sales"
            ],
            "preferences": {
              "language": "fr",
              "timezone": "Europe/Paris"
            }
          }
        },
        "notes": "This call creates a user with editing permissions and assigns them to the Sales group. Custom preferences are set for language and timezone."
      }
    ]
  },
  {
    "tool_id": "access.delete_user",
    "module": "access",
    "class": "AccessManagement",
    "method": "delete_user",
    "description": "Deletes a user by their email (username).",
    "full_doc": "Deletes a user by their email (username).\n\nParameters:\n    user_name (str): The email or username of the user to be deleted.\n\nReturns:\n    dict: Response from the API if successful,\n        or an error message dict.",
    "parameters": {
      "type": "object",
      "properties": {
        "user_name": {
          "type": "string",
          "description": "The email or username of the user to be deleted."
        }
      },
      "required": [
        "user_name"
      ]
    },
    "mutates": true,
    "tags": [
      "access",
      "users",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "Remove the user with the email john.doe@example.com from the system.",
        "arguments": {
          "user_name": "john.doe@example.com"
        },
        "notes": "This call deletes the user with the specified email address. Use it when you need to revoke access for a specific user."
      },
      {
        "user_query": "Delete the user account associated with the username admin.user@sisense.com.",
        "arguments": {
          "user_name": "admin.user@sisense.com"
        },
        "notes": "This call removes the user account for the given username. Useful for deactivating admin accounts no longer in use."
      },
      {
        "user_query": "Can you delete the user with the email jane.smith@company.com?",
        "arguments": {
          "user_name": "jane.smith@company.com"
        },
        "notes": "This operation deletes the user identified by the provided email. Use this to clean up inactive or outdated user accounts."
      }
    ]
  },
  {
    "tool_id": "access.get_all_dashboard_shares",
    "module": "access",
    "class": "AccessManagement",
    "method": "get_all_dashboard_shares",
    "description": "Method to retrieve all dashboard shares, including user and group details for each shared dashboard.",
    "full_doc": "Method to retrieve all dashboard shares, including user and group details for each shared dashboard.\n\nThis method uses pagination to retrieve all dashboards and their share information, and it collects\ncorresponding user and group details for each share.\n\nReturns:\n    list: A list of dictionaries containing the dashboard title, share type (user or group),\n    and share name (email or group name).",
    "parameters": {
      "type": "object",
      "properties": {},
      "required": []
    },
    "mutates": false,
    "tags": [
      "access",
      "dashboards",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "How can I see all the dashboards shared with users and groups in my organization?",
        "arguments": {},
        "notes": "This call retrieves a list of all dashboards that have been shared, along with details about the users and groups they are shared with. Useful for auditing shared resources."
      },
      {
        "user_query": "I need to review all shared dashboards to ensure proper access control. How can I get this information?",
        "arguments": {},
        "notes": "This call provides a comprehensive list of all shared dashboards, including the type of share (user or group) and the associated user emails or group names. Ideal for access management reviews."
      },
      {
        "user_query": "Can I get a list of all dashboards and their sharing details for compliance reporting?",
        "arguments": {},
        "notes": "This call generates a detailed list of all shared dashboards, which can be used for compliance and reporting purposes to ensure proper sharing practices."
      }
    ]
  },
  {
    "tool_id": "access.get_datamodel_columns",
    "module": "access",
    "class": "AccessManagement",
    "method": "get_datamodel_columns",
    "description": "Retrieves columns from a DataModel by collecting them from its datasets and tables.",
    "full_doc": "Retrieves columns from a DataModel by collecting them from its datasets and tables.\n\nParameters:\n    datamodel_name (str): The name of the DataModel from which to extract columns.\n\nReturns:\n    list: A list of dictionaries where each dictionary contains DataModel ID, DataModel name,\n    table name, and column name.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "The name of the DataModel from which to extract columns."
        }
      },
      "required": [
        "datamodel_name"
      ]
    },
    "mutates": false,
    "tags": [
      "access",
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "What columns are available in the Sales DataModel?",
        "arguments": {
          "datamodel_name": "Sales"
        },
        "notes": "This call retrieves all columns from the 'Sales' DataModel, including those from its datasets and tables. Use this to explore the structure of the Sales DataModel."
      },
      {
        "user_query": "Can you list all columns in the Marketing DataModel?",
        "arguments": {
          "datamodel_name": "Marketing"
        },
        "notes": "This call extracts column details from the 'Marketing' DataModel. Useful for understanding the data structure before building dashboards or reports."
      },
      {
        "user_query": "I need to see the columns in the Finance DataModel for a new report.",
        "arguments": {
          "datamodel_name": "Finance"
        },
        "notes": "This call gathers column information from the 'Finance' DataModel, helping you identify the data available for reporting or analysis."
      }
    ]
  },
  {
    "tool_id": "access.get_group",
    "module": "access",
    "class": "AccessManagement",
    "method": "get_group",
    "description": "Retrieves group details by their name.",
    "full_doc": "Retrieves group details by their name.\n\nParameters:\n    name (str): The name of the group to be retrieved.\n\nReturns:\n    dict: Group details, or {'error': ...} if retrieval fails or not\n    found.",
    "parameters": {
      "type": "object",
      "properties": {
        "name": {
          "type": "string",
          "description": "The name of the group to be retrieved."
        }
      },
      "required": [
        "name"
      ]
    },
    "mutates": false,
    "tags": [
      "access",
      "groups",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "What are the details of the 'Sales Team' group?",
        "arguments": {
          "name": "Sales Team"
        },
        "notes": "Retrieves details for the 'Sales Team' group, such as its members and permissions."
      },
      {
        "user_query": "Can you provide information about the 'Admin' group?",
        "arguments": {
          "name": "Admin"
        },
        "notes": "Fetches details for the 'Admin' group, which typically includes administrators with elevated permissions."
      },
      {
        "user_query": "I need to check the configuration of the 'Marketing' group.",
        "arguments": {
          "name": "Marketing"
        },
        "notes": "Returns the details of the 'Marketing' group, which may include its assigned users and access levels."
      }
    ]
  },
  {
    "tool_id": "access.get_unused_columns",
    "module": "access",
    "class": "AccessManagement",
    "method": "get_unused_columns",
    "description": "Identify unused columns in a given DataModel by comparing all available columns against the columns",
    "full_doc": "Identify unused columns in a given DataModel by comparing all available columns against the columns\nreferenced in associated dashboards.\n\nCovers:\n- Dashboard Filters: Dashboard-level filters, Widget filters, Dependent Filters.\n- Widget Panels: Includes Row, Values, Column panels, and Measured Filters.\n\nParameters:\n    datamodel_name (str): The name of the DataModel to analyze.\n\nReturns:\n    list: A list of dictionaries containing unused column details with a \"used\" field set to True or False.\n\nRaises:\n    ValueError: If no columns are found for the given DataModel (for example, if it does not exist\n                or is not accessible).",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "The name of the DataModel to analyze."
        }
      },
      "required": [
        "datamodel_name"
      ]
    },
    "mutates": false,
    "tags": [
      "access",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "Which columns in the 'SalesDataModel' are not being used in any dashboards?",
        "arguments": {
          "datamodel_name": "SalesDataModel"
        },
        "notes": "This call identifies unused columns in the 'SalesDataModel' by analyzing dashboard filters and widget panels. Use this to optimize the DataModel by removing unnecessary columns."
      },
      {
        "user_query": "Can you check for unused columns in the 'CustomerInsightsModel'?",
        "arguments": {
          "datamodel_name": "CustomerInsightsModel"
        },
        "notes": "This call helps determine which columns in the 'CustomerInsightsModel' are not referenced in dashboards, enabling better DataModel management."
      },
      {
        "user_query": "I want to find unused columns in the 'MarketingAnalyticsModel'.",
        "arguments": {
          "datamodel_name": "MarketingAnalyticsModel"
        },
        "notes": "Use this call to identify unused columns in the 'MarketingAnalyticsModel', ensuring the DataModel is streamlined and efficient."
      }
    ]
  },
  {
    "tool_id": "access.get_unused_columns_bulk",
    "module": "access",
    "class": "AccessManagement",
    "method": "get_unused_columns_bulk",
    "description": "Run unused-column analysis for one or more data models and return a",
    "full_doc": "Run unused-column analysis for one or more data models and return a\ncombined result set.\n\nParameters\n----------\ndatamodels : str or list of str, optional\n    One or more data model references to analyze. Each reference can be:\n      - a data model ID, or\n      - a data model title (name).\n    At least one data model reference is required. At runtime this\n    parameter is tolerant of a single string and will normalize it to a\n    one-element list.\n\nReturns\n-------\nlist of dict\n    A flat list of rows across all processed data models. Each row has\n    the same structure as returned by get_unused_columns().\n    If no data models are successfully processed, an empty list is\n    returned and details are available in the logs.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodels": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "One or more data model references to analyze. Each reference can be: - a data model ID, or - a data model title (name). At least one data model reference is required. At runtime this parameter is tolerant of a single string and will normalize it to a one-element list."
        }
      },
      "required": []
    },
    "mutates": false,
    "tags": [
      "access",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "Which columns are unused in the Sales and Marketing data models?",
        "arguments": {
          "datamodels": [
            "Sales_Model_ID_123",
            "Marketing_Model_Title"
          ]
        },
        "notes": "This call analyzes unused columns in the Sales and Marketing data models using their respective references (ID and title). Use this to optimize these models by identifying unnecessary columns."
      },
      {
        "user_query": "Find unused columns in the Customer Insights data model.",
        "arguments": {
          "datamodels": [
            "Customer_Insights_Model"
          ]
        },
        "notes": "This call checks for unused columns in the Customer Insights data model using its title. Ideal for maintaining and improving the efficiency of this specific model."
      },
      {
        "user_query": "Analyze unused columns across multiple data models: Finance, HR, and Operations.",
        "arguments": {
          "datamodels": [
            "Finance_Model_ID_456",
            "HR_Model_Title",
            "Operations_Model_ID_789"
          ]
        },
        "notes": "This call performs unused-column analysis across three data models: Finance, HR, and Operations. Useful for bulk analysis when optimizing multiple models simultaneously."
      }
    ]
  },
  {
    "tool_id": "access.get_user",
    "module": "access",
    "class": "AccessManagement",
    "method": "get_user",
    "description": "Retrieves user details by their email (username) and expands the",
    "full_doc": "Retrieves user details by their email (username) and expands the\nresponse to include group and role information.\n\nParameters:\n    user_name (str): The email or username of the user to be retrieved.\n\nReturns:\n    dict: User details on success, or {'error': 'message'} on failure.",
    "parameters": {
      "type": "object",
      "properties": {
        "user_name": {
          "type": "string",
          "description": "The email or username of the user to be retrieved."
        }
      },
      "required": [
        "user_name"
      ]
    },
    "mutates": false,
    "tags": [
      "access",
      "users",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "Can you get the details for the user with the email john.doe@example.com?",
        "arguments": {
          "user_name": "john.doe@example.com"
        },
        "notes": "This call retrieves detailed information about the user with the specified email, including their group and role assignments."
      },
      {
        "user_query": "I need to check the roles assigned to the user admin@sisense.com.",
        "arguments": {
          "user_name": "admin@sisense.com"
        },
        "notes": "Use this call to fetch the user details for the admin account, including their roles and group memberships."
      },
      {
        "user_query": "Who is the user with the username jane.smith@sisense.io?",
        "arguments": {
          "user_name": "jane.smith@sisense.io"
        },
        "notes": "This request retrieves the user details for Jane Smith, including her roles and group affiliations, based on her username."
      }
    ]
  },
  {
    "tool_id": "access.get_users_all",
    "module": "access",
    "class": "AccessManagement",
    "method": "get_users_all",
    "description": "Retrieves user details along with tenant, group, and role information.",
    "full_doc": "Retrieves user details along with tenant, group, and role information.\nRemoves \"Everyone\" group from users if they belong to other groups, but\nkeeps the \"Everyone\" group if it's the only group the user belongs to.\n\nReturns:\n    list: List of user details dicts, or [{'error': ...}] if retrieval\n    fails.",
    "parameters": {
      "type": "object",
      "properties": {},
      "required": []
    },
    "mutates": false,
    "tags": [
      "access",
      "users",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "Get a list of all users along with their tenant, group, and role information.",
        "arguments": {},
        "notes": "This call retrieves detailed user information, including their associated tenants, groups, and roles. Use this to audit user access or manage user permissions."
      },
      {
        "user_query": "Fetch all user details to verify group memberships and roles.",
        "arguments": {},
        "notes": "Use this call to ensure users are assigned to the correct groups and roles. It automatically removes the 'Everyone' group if users belong to other groups."
      },
      {
        "user_query": "Retrieve user data for all tenants and groups in the system.",
        "arguments": {},
        "notes": "This call provides a comprehensive overview of user details across the system, useful for tenant-wide access reviews or compliance checks."
      }
    ]
  },
  {
    "tool_id": "access.update_user",
    "module": "access",
    "class": "AccessManagement",
    "method": "update_user",
    "description": "Updates a user by their User Name.",
    "full_doc": "Updates a user by their User Name.\n\nParameters:\n    user_name (str): The email or username of the user to be updated.\n    user_data (dict): A dictionary containing user details to update,\n    such as role, groups, etc.\n\nReturns:\n    dict: The response from the API if successful,\n        or a dictionary with an 'error' key if the operation fails.",
    "parameters": {
      "type": "object",
      "properties": {
        "user_name": {
          "type": "string",
          "description": "The email or username of the user to be updated."
        },
        "user_data": {
          "type": "object",
          "description": "A dictionary containing user details to update, such as role, groups, etc."
        }
      },
      "required": [
        "user_name",
        "user_data"
      ]
    },
    "mutates": true,
    "tags": [
      "access",
      "users",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "Update the role of user john.doe@example.com to 'Admin'.",
        "arguments": {
          "user_name": "john.doe@example.com",
          "user_data": {
            "role": "Admin"
          }
        },
        "notes": "This call updates the role of the specified user to 'Admin'. Use this when you need to change a user's role in the system."
      },
      {
        "user_query": "Add user jane.smith@example.com to the 'Marketing' and 'Analytics' groups.",
        "arguments": {
          "user_name": "jane.smith@example.com",
          "user_data": {
            "groups": [
              "Marketing",
              "Analytics"
            ]
          }
        },
        "notes": "This call adds the specified user to the 'Marketing' and 'Analytics' groups. Use this to manage group memberships for a user."
      },
      {
        "user_query": "Update the display name of user admin@example.com to 'System Administrator'.",
        "arguments": {
          "user_name": "admin@example.com",
          "user_data": {
            "display_name": "System Administrator"
          }
        },
        "notes": "This call updates the display name of the specified user. Use this to modify user profile details like their display name."
      }
    ]
  },
  {
    "tool_id": "access.users_per_group",
    "module": "access",
    "class": "AccessManagement",
    "method": "users_per_group",
    "description": "Retrieves all users within a specific group by name.",
    "full_doc": "Retrieves all users within a specific group by name.\n\nParameters:\n    group_name (str): The name of the group.\n\nReturns:\n    list or dict: A list of users in the group if successful, or a\n    dictionary containing an 'error' key if the operation fails.",
    "parameters": {
      "type": "object",
      "properties": {
        "group_name": {
          "type": "string",
          "description": "The name of the group."
        }
      },
      "required": [
        "group_name"
      ]
    },
    "mutates": false,
    "tags": [
      "access",
      "users",
      "groups",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "Who are all the users in the 'Sales Team' group?",
        "arguments": {
          "group_name": "Sales Team"
        },
        "notes": "This retrieves a list of all users who are members of the 'Sales Team' group. Useful for checking group membership."
      },
      {
        "user_query": "Can you show me the users in the 'Admin' group?",
        "arguments": {
          "group_name": "Admin"
        },
        "notes": "This fetches all users in the 'Admin' group. Typically used to verify administrative access."
      },
      {
        "user_query": "List all users in the 'Marketing' group.",
        "arguments": {
          "group_name": "Marketing"
        },
        "notes": "This retrieves the members of the 'Marketing' group. Useful for auditing or managing group memberships."
      }
    ]
  },
  {
    "tool_id": "access.users_per_group_all",
    "module": "access",
    "class": "AccessManagement",
    "method": "users_per_group_all",
    "description": "Retrieves all groups and maps them with the users belonging to those",
    "full_doc": "Retrieves all groups and maps them with the users belonging to those\ngroups.\nGroups like 'Everyone' and 'All users in system' are excluded.\nUsers with roles like 'admin', 'dataAdmin', and 'sysAdmin' are mapped\nto the existing 'Admins' group.\nGroups with no users are also included in the final result.\n\nReturns:\n    list: A list of dictionaries, where each dictionary contains a\n    group name and the list of usernames in that group.",
    "parameters": {
      "type": "object",
      "properties": {},
      "required": []
    },
    "mutates": false,
    "tags": [
      "access",
      "users",
      "groups",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "Which users belong to each group in our Sisense system?",
        "arguments": {},
        "notes": "This call retrieves a mapping of all groups to their respective users, excluding system-wide groups like 'Everyone' and 'All users in system'."
      },
      {
        "user_query": "Can I get a list of all groups and their associated users, including empty groups?",
        "arguments": {},
        "notes": "This call provides a full list of groups and their users, including groups with no users. Admin roles are grouped under 'Admins'."
      },
      {
        "user_query": "I need to see which users are assigned to each group, but skip system-wide groups like 'Everyone'.",
        "arguments": {},
        "notes": "This call generates a detailed mapping of users to groups, excluding system-wide groups and consolidating admin roles under 'Admins'."
      }
    ]
  },
  {
    "tool_id": "datamodel.add_datamodel_shares",
    "module": "datamodel",
    "class": "DataModel",
    "method": "add_datamodel_shares",
    "description": "Adds share entries (users and groups) to a DataModel.",
    "full_doc": "Adds share entries (users and groups) to a DataModel.\n\nParameters:\n    datamodel_name (str): Name of the DataModel to add shares to.\n    shares (list): List of dictionaries containing share details. Each dictionary should have:\n        - name: Name of the user or group\n        - type: Type of the party (user or group)\n        - permission: Permission level (EDIT, READ, USE)\n\nReturns:\n    dict: Result of the share addition operation.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel to add shares to."
        },
        "shares": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "List of dictionaries containing share details. Each dictionary should have: - name: Name of the user or group - type: Type of the party (user or group) - permission: Permission level (EDIT, READ, USE)"
        }
      },
      "required": [
        "datamodel_name",
        "shares"
      ]
    },
    "mutates": true,
    "tags": [
      "datamodel",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "How can I share the 'SalesDataModel' with the sales team and give them edit permissions?",
        "arguments": {
          "datamodel_name": "SalesDataModel",
          "shares": [
            {
              "name": "sales_team",
              "type": "group",
              "permission": "EDIT"
            }
          ]
        },
        "notes": "This call shares the 'SalesDataModel' with the 'sales_team' group, granting them edit permissions."
      },
      {
        "user_query": "I need to give read-only access to the 'FinanceDataModel' for the finance team and a specific user, John Doe.",
        "arguments": {
          "datamodel_name": "FinanceDataModel",
          "shares": [
            {
              "name": "finance_team",
              "type": "group",
              "permission": "READ"
            },
            {
              "name": "john.doe",
              "type": "user",
              "permission": "READ"
            }
          ]
        },
        "notes": "This call shares the 'FinanceDataModel' with the 'finance_team' group and the user 'john.doe', both with read-only access."
      },
      {
        "user_query": "Can I allow the 'MarketingDataModel' to be used by the marketing team without giving them edit or read access?",
        "arguments": {
          "datamodel_name": "MarketingDataModel",
          "shares": [
            {
              "name": "marketing_team",
              "type": "group",
              "permission": "USE"
            }
          ]
        },
        "notes": "This call shares the 'MarketingDataModel' with the 'marketing_team' group, granting them 'USE' permissions without edit or read access."
      }
    ]
  },
  {
    "tool_id": "datamodel.create_connections",
    "module": "datamodel",
    "class": "DataModel",
    "method": "create_connections",
    "description": "Creates a new connection using the provided payload.",
    "full_doc": "Creates a new connection using the provided payload.\n\nParameters:\n    connection_payload (dict): The configuration payload for the connection.\n\nReturns:\n    dict or None: JSON response with connection details if successful, otherwise None.",
    "parameters": {
      "type": "object",
      "properties": {
        "connection_payload": {
          "type": "object",
          "description": "The configuration payload for the connection."
        }
      },
      "required": [
        "connection_payload"
      ]
    },
    "mutates": true,
    "tags": [
      "datamodel",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "How can I create a connection to my SQL database in Sisense?",
        "arguments": {
          "connection_payload": {
            "name": "SQL_Connection_01",
            "type": "SQL",
            "host": "sql.example.com",
            "port": 5432,
            "username": "admin",
            "password": "securepassword",
            "database": "analytics_db"
          }
        },
        "notes": "This call creates a new connection to a SQL database using the provided configuration details. Use this when setting up a new data source for your Sisense instance."
      },
      {
        "user_query": "I need to connect Sisense to a MongoDB instance. How do I do that?",
        "arguments": {
          "connection_payload": {
            "name": "MongoDB_Connection_01",
            "type": "MongoDB",
            "host": "mongo.example.com",
            "port": 27017,
            "username": "mongo_user",
            "password": "mongo_password",
            "database": "sisense_data"
          }
        },
        "notes": "This call establishes a connection to a MongoDB instance. Use this when integrating Sisense with a NoSQL database for analytics."
      },
      {
        "user_query": "Can I set up a connection to a REST API for data ingestion?",
        "arguments": {
          "connection_payload": {
            "name": "REST_API_Connection_01",
            "type": "REST",
            "endpoint": "https://api.example.com/data",
            "headers": {
              "Authorization": "Bearer api_token"
            },
            "method": "GET"
          }
        },
        "notes": "This call creates a connection to a REST API endpoint for data ingestion. Use this when pulling data from external APIs into Sisense."
      }
    ]
  },
  {
    "tool_id": "datamodel.create_datamodel",
    "module": "datamodel",
    "class": "DataModel",
    "method": "create_datamodel",
    "description": "Creates a new DataModel in Sisense.",
    "full_doc": "Creates a new DataModel in Sisense.\n\nParameters:\n    datamodel_name (str): Name of the DataModel.\n    datamodel_type (str): Type of the DataModel. Should be either \"extract\" (for Elasticube)\n        or \"live\" (for Live).\n\nReturns:\n    dict: Dictionary with the DataModel ID if created successfully, or an error message.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel."
        },
        "datamodel_type": {
          "type": "string",
          "description": "Either 'extract' (Elasticube/EC) or 'live'. If user says 'elasticube' or 'ec', normalize to 'extract'.",
          "enum": [
            "extract",
            "live"
          ],
          "x-aliases": {
            "extract": [
              "ec",
              "elasticube",
              "elastic cube",
              "cube",
              "elastic-cube"
            ],
            "live": [
              "realtime",
              "real-time",
              "live model"
            ]
          }
        }
      },
      "required": [
        "datamodel_name",
        "datamodel_type"
      ]
    },
    "mutates": true,
    "tags": [
      "datamodel",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "Create a new DataModel for our sales data as an Elasticube.",
        "arguments": {
          "datamodel_name": "Sales_Data_Model",
          "datamodel_type": "extract"
        },
        "notes": "This call creates a new DataModel named 'Sales_Data_Model' using the 'extract' type, which corresponds to an Elasticube. Use this when you need a DataModel for offline data processing."
      },
      {
        "user_query": "Set up a live DataModel for real-time analytics on customer behavior.",
        "arguments": {
          "datamodel_name": "Customer_Behavior_Live",
          "datamodel_type": "live"
        },
        "notes": "This call creates a new DataModel named 'Customer_Behavior_Live' using the 'live' type. Use this for real-time data analysis where the data is directly queried from the source."
      },
      {
        "user_query": "Create a DataModel for our inventory data as an Elasticube.",
        "arguments": {
          "datamodel_name": "Inventory_Elasticube",
          "datamodel_type": "extract"
        },
        "notes": "This call creates a new DataModel named 'Inventory_Elasticube' using the 'extract' type. Ideal for scenarios where inventory data needs to be processed offline for reporting and analytics."
      }
    ]
  },
  {
    "tool_id": "datamodel.create_dataset",
    "module": "datamodel",
    "class": "DataModel",
    "method": "create_dataset",
    "description": "Creates a new dataset in the specified DataModel.",
    "full_doc": "Creates a new dataset in the specified DataModel.\n\nParameters:\n    datamodel_name (str): Name of the DataModel where the dataset will be created.\n    connection_name (str): Name of the connection to use.\n    database_name (str): Name of the data source database.\n    schema_name (str): Name of the data source schema.\n    dataset_name (str, optional): Name of the dataset. Defaults to schema name if not provided.\n\nReturns:\n    dict: A dictionary containing the full dataset object on success, or an error message on failure.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel where the dataset will be created."
        },
        "connection_name": {
          "type": "string",
          "description": "Name of the connection to use."
        },
        "database_name": {
          "type": "string",
          "description": "Name of the data source database."
        },
        "schema_name": {
          "type": "string",
          "description": "Name of the data source schema."
        },
        "dataset_name": {
          "type": "string",
          "description": "Name of the dataset. Defaults to schema name if not provided."
        }
      },
      "required": [
        "datamodel_name",
        "connection_name",
        "database_name",
        "schema_name"
      ]
    },
    "mutates": true,
    "tags": [
      "datamodel",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "Create a new dataset in the Sales DataModel using the connection to the sales database.",
        "arguments": {
          "datamodel_name": "Sales",
          "connection_name": "SalesDBConnection",
          "database_name": "sales_database",
          "schema_name": "public",
          "dataset_name": "sales_dataset"
        },
        "notes": "This call creates a dataset named 'sales_dataset' in the 'Sales' DataModel using the 'SalesDBConnection' connection and the 'public' schema from the 'sales_database'."
      },
      {
        "user_query": "Add a dataset to the Marketing DataModel using the default schema name as the dataset name.",
        "arguments": {
          "datamodel_name": "Marketing",
          "connection_name": "MarketingConnection",
          "database_name": "marketing_db",
          "schema_name": "analytics"
        },
        "notes": "This call creates a dataset in the 'Marketing' DataModel using the 'MarketingConnection' connection and the 'analytics' schema from the 'marketing_db'. The dataset name defaults to 'analytics'."
      },
      {
        "user_query": "Create a dataset in the Finance DataModel using the finance connection and a custom dataset name.",
        "arguments": {
          "datamodel_name": "Finance",
          "connection_name": "FinanceConnection",
          "database_name": "finance_db",
          "schema_name": "reports",
          "dataset_name": "financial_reports"
        },
        "notes": "This call creates a dataset named 'financial_reports' in the 'Finance' DataModel using the 'FinanceConnection' connection and the 'reports' schema from the 'finance_db'."
      }
    ]
  },
  {
    "tool_id": "datamodel.create_table",
    "module": "datamodel",
    "class": "DataModel",
    "method": "create_table",
    "description": "Create a new table in the specified DataModel.",
    "full_doc": "Create a new table in the specified DataModel.\n\nParameters:\n    datamodel_name (str): Name of the DataModel where the table will be created.\n    table_name (str): Name of the table to create.\n    database_name (str, optional): Name of the data source database.\n        If not provided, will try to infer from the DataModel.\n    schema_name (str, optional): Name of the data source schema.\n        If not provided, will try to infer from the DataModel.\n    dataset_id (str, optional): ID of the dataset where the table will be created.\n        If not provided, will try to infer from the DataModel.\n    import_query (str, optional): SQL statement used as custom import query. Defaults to None.\n    description (str, optional): Description for the table. Defaults to an empty string.\n    tags (list, optional): List of tags to apply to the table. Defaults to None.\n    build_behavior_config (dict, optional): Configuration for table build behavior.\n\nReturns:\n    dict: Table object if created successfully or an error message.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel where the table will be created."
        },
        "table_name": {
          "type": "string",
          "description": "Name of the table to create."
        },
        "database_name": {
          "type": "string",
          "description": "Name of the data source database. If not provided, will try to infer from the DataModel."
        },
        "schema_name": {
          "type": "string",
          "description": "Name of the data source schema. If not provided, will try to infer from the DataModel."
        },
        "dataset_id": {
          "type": "string",
          "description": "ID of the dataset where the table will be created. If not provided, will try to infer from the DataModel."
        },
        "import_query": {
          "type": "string",
          "description": "Optional custom SQL (executed as-is). Use fully-qualified tables: schema.table (Databricks example: sales.orders)."
        },
        "description": {
          "type": "string",
          "description": "Description for the table. Defaults to an empty string."
        },
        "tags": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "List of tags to apply to the table. Defaults to None."
        },
        "build_behavior_config": {
          "type": "object",
          "description": "Configuration for table build behavior."
        }
      },
      "required": [
        "datamodel_name",
        "table_name"
      ]
    },
    "mutates": true,
    "tags": [
      "datamodel",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "Create a new table named 'sales_data' in the 'RetailAnalytics' DataModel.",
        "arguments": {
          "datamodel_name": "RetailAnalytics",
          "table_name": "sales_data",
          "database_name": "retail_db",
          "schema_name": "public",
          "description": "Table containing sales data for analysis.",
          "tags": [
            "sales",
            "analytics"
          ],
          "build_behavior_config": {
            "auto_build": true,
            "build_frequency": "daily"
          }
        },
        "notes": "This call creates a new table in the 'RetailAnalytics' DataModel using the specified database and schema. Tags and build behavior configuration are also applied."
      },
      {
        "user_query": "Add a table named 'customer_profiles' to the 'CustomerInsights' DataModel using a custom SQL query.",
        "arguments": {
          "datamodel_name": "CustomerInsights",
          "table_name": "customer_profiles",
          "import_query": "SELECT * FROM schema.customers WHERE active = true",
          "description": "Active customer profiles for segmentation.",
          "tags": [
            "customers",
            "segmentation"
          ]
        },
        "notes": "This call creates a table in the 'CustomerInsights' DataModel using a custom SQL query to import only active customer profiles. Tags are applied for categorization."
      },
      {
        "user_query": "Create a table named 'inventory_status' in the 'WarehouseManagement' DataModel without specifying database or schema.",
        "arguments": {
          "datamodel_name": "WarehouseManagement",
          "table_name": "inventory_status",
          "description": "Tracks current inventory levels and statuses."
        },
        "notes": "This call creates a table in the 'WarehouseManagement' DataModel and relies on the system to infer the database and schema. No tags or custom configurations are applied."
      }
    ]
  },
  {
    "tool_id": "datamodel.deploy_datamodel",
    "module": "datamodel",
    "class": "DataModel",
    "method": "deploy_datamodel",
    "description": "Deploy (build or publish) the specified DataModel based on its type.",
    "full_doc": "Deploy (build or publish) the specified DataModel based on its type.\n\nThis method supports both Elasticube (EXTRACT) and Live models.\nThe behavior and required parameters differ based on model type:\n\n- For Elasticube models:\n    - build_type (str): Type of deployment. Options:\n        * \"schema_changes\" \u2014 Build only schema changes\n        * \"by_table\" \u2014 Build based on each table's config (e.g. incremental, accumulative)\n        * \"full\" \u2014 Rebuild the entire model from scratch (default)\n    - row_limit (int): Maximum number of rows to process. Defaults to 0 (no limit).\n    - schema_origin (str): Schema source. Options:\n        * \"latest\" \u2014 Build the schema as seen in the Data page (default)\n        * \"running\" \u2014 Build from the last successfully built version\n\n- For Live models:\n    - Only the `build_type` parameter is used internally and will be set to \"publish\"\n    - `row_limit` and `schema_origin` are ignored\n\nParameters:\n    datamodel_name (str): Name of the DataModel to deploy.\n    build_type (str): Type of deployment. Required for EXTRACT only.\n    row_limit (int): Row limit for build. Applicable only for EXTRACT.\n    schema_origin (str): Schema origin for build. Applicable only for EXTRACT.\n\nReturns:\n    dict: Deployment result including status, or error details.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel to deploy."
        },
        "build_type": {
          "type": "string",
          "description": "Build strategy for extract models. Omit for live/publish.",
          "enum": [
            "full",
            "by_table"
          ],
          "x-aliases": {
            "full": [
              "build",
              "rebuild",
              "start",
              "run",
              "execute",
              "refresh"
            ],
            "by_table": [
              "by-table",
              "table-wise",
              "incremental-tables"
            ]
          }
        },
        "row_limit": {
          "type": "integer",
          "description": "Row limit for build. Applicable only for EXTRACT.",
          "minimum": 1
        },
        "schema_origin": {
          "type": "string",
          "description": "Schema origin for build. Applicable only for EXTRACT.",
          "enum": [
            "latest",
            "schema_changes"
          ]
        }
      },
      "required": [
        "datamodel_name"
      ]
    },
    "mutates": true,
    "tags": [
      "datamodel",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "How can I rebuild the entire 'Sales_Analytics' DataModel?",
        "arguments": {
          "datamodel_name": "Sales_Analytics",
          "build_type": "full"
        },
        "notes": "This call rebuilds the entire 'Sales_Analytics' DataModel from scratch. Use this for a complete refresh of the model."
      },
      {
        "user_query": "I need to deploy the 'Customer_Insights' DataModel but only process schema changes. How can I do that?",
        "arguments": {
          "datamodel_name": "Customer_Insights",
          "build_type": "by_table",
          "schema_origin": "schema_changes"
        },
        "notes": "This call deploys the 'Customer_Insights' DataModel by processing only schema changes and building incrementally by table."
      },
      {
        "user_query": "Can I deploy the 'Marketing_Performance' DataModel with a row limit of 1000?",
        "arguments": {
          "datamodel_name": "Marketing_Performance",
          "build_type": "full",
          "row_limit": 1000
        },
        "notes": "This call deploys the 'Marketing_Performance' DataModel with a full rebuild, but limits the number of rows processed to 1000."
      }
    ]
  },
  {
    "tool_id": "datamodel.describe_datamodel",
    "module": "datamodel",
    "class": "DataModel",
    "method": "describe_datamodel",
    "description": "Retrieve detailed datamodel structure in a flat, row-based format suitable for DataFrame or CSV export.",
    "full_doc": "Retrieve detailed datamodel structure in a flat, row-based format suitable for DataFrame or CSV export.\n\nParameters:\n    datamodel_name (str): Name of the DataModel to describe.\n\nReturns:\n    list: List of dictionaries, each representing a single table row with context (datamodel, dataset, table).",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel to describe."
        }
      },
      "required": [
        "datamodel_name"
      ]
    },
    "mutates": false,
    "tags": [
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "What does the Sales DataModel look like?",
        "arguments": {
          "datamodel_name": "Sales"
        },
        "notes": "This retrieves the structure of the 'Sales' DataModel, including details about its datasets and tables, in a format suitable for analysis or export."
      },
      {
        "user_query": "Can you provide the schema details for the Marketing DataModel?",
        "arguments": {
          "datamodel_name": "Marketing"
        },
        "notes": "This call fetches the detailed structure of the 'Marketing' DataModel, which can be used to understand its composition or export it for further processing."
      },
      {
        "user_query": "I need to analyze the structure of the Finance DataModel. Can you get that for me?",
        "arguments": {
          "datamodel_name": "Finance"
        },
        "notes": "This retrieves the detailed structure of the 'Finance' DataModel, allowing for inspection or export to a DataFrame or CSV."
      }
    ]
  },
  {
    "tool_id": "datamodel.describe_datamodel_raw",
    "module": "datamodel",
    "class": "DataModel",
    "method": "describe_datamodel_raw",
    "description": "Retrieve detailed information about a specific DataModel, including share details.",
    "full_doc": "Retrieve detailed information about a specific DataModel, including share details.\n\nParameters:\n    datamodel_name (str): Name of the DataModel to describe.\n\nReturns:\n    dict: Detailed information about the DataModel, or an error message if not found.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel to describe."
        }
      },
      "required": [
        "datamodel_name"
      ]
    },
    "mutates": false,
    "tags": [
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "Can you provide detailed information about the Sales DataModel?",
        "arguments": {
          "datamodel_name": "Sales"
        },
        "notes": "This call retrieves detailed information about the 'Sales' DataModel, including its share details. Use this to understand its structure and sharing configuration."
      },
      {
        "user_query": "I need to check the details of the MarketingAnalytics DataModel.",
        "arguments": {
          "datamodel_name": "MarketingAnalytics"
        },
        "notes": "Use this call to fetch detailed information about the 'MarketingAnalytics' DataModel, including its sharing settings and metadata."
      },
      {
        "user_query": "What are the details of the FinanceModel DataModel?",
        "arguments": {
          "datamodel_name": "FinanceModel"
        },
        "notes": "This call provides detailed insights into the 'FinanceModel' DataModel, including its structure and share details. Useful for auditing or understanding its configuration."
      }
    ]
  },
  {
    "tool_id": "datamodel.generate_connections_payload",
    "module": "datamodel",
    "class": "DataModel",
    "method": "generate_connections_payload",
    "description": "Generates the appropriate connections payload based on the datasource type.",
    "full_doc": "Generates the appropriate connections payload based on the datasource type.\n\nParameters:\n    datasource_type (str): Type of datasource (e.g., \"ATHENA\", \"SNOWFLAKE\", \"ORACLE\").\n    connection_params (dict): Connection details for the datasource.\n\nReturns:\n    dict: Connections payload.",
    "parameters": {
      "type": "object",
      "properties": {
        "datasource_type": {
          "type": "string",
          "description": "Type of datasource (e.g., \"ATHENA\", \"SNOWFLAKE\", \"ORACLE\")."
        },
        "connection_params": {
          "type": "object",
          "description": "Connection details for the datasource."
        }
      },
      "required": [
        "datasource_type",
        "connection_params"
      ]
    },
    "mutates": false,
    "tags": [
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "How do I generate a connections payload for an Athena datasource?",
        "arguments": {
          "datasource_type": "ATHENA",
          "connection_params": {
            "region": "us-west-2",
            "database": "analytics_db",
            "output_location": "s3://athena-query-results/"
          }
        },
        "notes": "This call generates a connections payload for an Athena datasource, specifying the AWS region, database name, and S3 output location."
      },
      {
        "user_query": "I need to create a connections payload for a Snowflake datasource. How can I do that?",
        "arguments": {
          "datasource_type": "SNOWFLAKE",
          "connection_params": {
            "account": "mycompany.snowflakecomputing.com",
            "warehouse": "COMPUTE_WH",
            "database": "SALES_DB",
            "schema": "PUBLIC"
          }
        },
        "notes": "This call generates a connections payload for a Snowflake datasource, including details such as account URL, warehouse, database, and schema."
      },
      {
        "user_query": "What parameters do I need to generate a connections payload for an Oracle datasource?",
        "arguments": {
          "datasource_type": "ORACLE",
          "connection_params": {
            "host": "oracle-db.mycompany.com",
            "port": 1521,
            "service_name": "ORCL",
            "username": "admin"
          }
        },
        "notes": "This call generates a connections payload for an Oracle datasource, providing the host, port, service name, and username for the connection."
      }
    ]
  },
  {
    "tool_id": "datamodel.get_all_datamodel",
    "module": "datamodel",
    "class": "DataModel",
    "method": "get_all_datamodel",
    "description": "Retrieves metadata details of all DataModels using an undocumented internal API.",
    "full_doc": "Retrieves metadata details of all DataModels using an undocumented internal API.\nThis includes additional fields like build status, size, and timestamps that may\nnot be available through the standard public endpoints.\n\nReturns:\n    dict: Parsed metadata details of all DataModels, or a dictionary with an error message.",
    "parameters": {
      "type": "object",
      "properties": {},
      "required": []
    },
    "mutates": false,
    "tags": [
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "Fetch metadata for all existing DataModels in the system.",
        "arguments": {},
        "notes": "This call retrieves detailed metadata for all DataModels, including build status, size, and timestamps. Use this to audit or analyze the state of all DataModels."
      },
      {
        "user_query": "Get a list of all DataModels with their metadata for reporting purposes.",
        "arguments": {},
        "notes": "Use this to extract comprehensive metadata about all DataModels for generating reports or debugging issues with specific models."
      },
      {
        "user_query": "Retrieve detailed information about all DataModels to check their build status and last update timestamps.",
        "arguments": {},
        "notes": "This call is useful for monitoring the health and update status of all DataModels in the system."
      }
    ]
  },
  {
    "tool_id": "datamodel.get_connection",
    "module": "datamodel",
    "class": "DataModel",
    "method": "get_connection",
    "description": "Retrieves a Connection by its name.",
    "full_doc": "Retrieves a Connection by its name.\n\nParameters:\n    connection_name (str): Name of the connection to filter by.\n\nReturns:\n    List: Connection details if found, or a dictionary with an error message.",
    "parameters": {
      "type": "object",
      "properties": {
        "connection_name": {
          "type": "string",
          "description": "Name of the connection to filter by."
        }
      },
      "required": [
        "connection_name"
      ]
    },
    "mutates": false,
    "tags": [
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "What are the details of the connection named 'SalesDB'?",
        "arguments": {
          "connection_name": "SalesDB"
        },
        "notes": "This retrieves the details of the connection named 'SalesDB', such as its configuration and status."
      },
      {
        "user_query": "Can you fetch the connection information for 'MarketingDataWarehouse'?",
        "arguments": {
          "connection_name": "MarketingDataWarehouse"
        },
        "notes": "This is used to get the connection details for the 'MarketingDataWarehouse' connection, which might be needed for troubleshooting or validation."
      },
      {
        "user_query": "I need to check the configuration of the 'FinanceAnalytics' connection.",
        "arguments": {
          "connection_name": "FinanceAnalytics"
        },
        "notes": "This call retrieves the configuration and metadata for the 'FinanceAnalytics' connection, useful for ensuring it is set up correctly."
      }
    ]
  },
  {
    "tool_id": "datamodel.get_data",
    "module": "datamodel",
    "class": "DataModel",
    "method": "get_data",
    "description": "Retrieves data from a specific table in a DataModel and returns it as a list of dicts",
    "full_doc": "Retrieves data from a specific table in a DataModel and returns it as a list of dicts\n(row-based format) compatible with to_dataframe.\n\nParameters:\n    datamodel_name (str): Name of the DataModel.\n    table_name (str): Name of the table to retrieve data from.\n    query (str): Optional SQL query to filter the data.\n\nReturns:\n    list: List of dictionaries where each dict represents a row.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel."
        },
        "table_name": {
          "type": "string",
          "description": "Name of the table to retrieve data from."
        },
        "query": {
          "type": "string",
          "description": "Optional SQL query to filter the data."
        }
      },
      "required": [
        "datamodel_name",
        "table_name"
      ]
    },
    "mutates": false,
    "tags": [
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "Fetch all data from the 'sales_data' table in the 'RetailAnalytics' DataModel.",
        "arguments": {
          "datamodel_name": "RetailAnalytics",
          "table_name": "sales_data"
        },
        "notes": "This retrieves all rows from the 'sales_data' table in the 'RetailAnalytics' DataModel without applying any filters."
      },
      {
        "user_query": "Get the data from the 'customer_info' table in the 'CustomerInsights' DataModel where the customer is located in California.",
        "arguments": {
          "datamodel_name": "CustomerInsights",
          "table_name": "customer_info",
          "query": "SELECT * FROM customer_info WHERE state = 'California'"
        },
        "notes": "This retrieves rows from the 'customer_info' table in the 'CustomerInsights' DataModel where the state is 'California'."
      },
      {
        "user_query": "Retrieve all records from the 'inventory' table in the 'WarehouseData' DataModel.",
        "arguments": {
          "datamodel_name": "WarehouseData",
          "table_name": "inventory"
        },
        "notes": "This fetches all rows from the 'inventory' table in the 'WarehouseData' DataModel without any filtering."
      }
    ]
  },
  {
    "tool_id": "datamodel.get_datamodel",
    "module": "datamodel",
    "class": "DataModel",
    "method": "get_datamodel",
    "description": "Retrieves a DataModel by its name.",
    "full_doc": "Retrieves a DataModel by its name.\n\nParameters:\n    datamodel_name (str): Name of the DataModel to retrieve.\n\nReturns:\n    dict: DataModel details if found, or a dictionary with an error message.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel to retrieve."
        }
      },
      "required": [
        "datamodel_name"
      ]
    },
    "mutates": false,
    "tags": [
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "Retrieve the details of the Sales DataModel.",
        "arguments": {
          "datamodel_name": "Sales"
        },
        "notes": "This call retrieves the DataModel named 'Sales', which might contain data related to sales metrics and analytics."
      },
      {
        "user_query": "Get the configuration for the Marketing DataModel.",
        "arguments": {
          "datamodel_name": "Marketing"
        },
        "notes": "Use this call to fetch the structure and metadata of the 'Marketing' DataModel for analysis or validation purposes."
      },
      {
        "user_query": "Find the DataModel for the Finance department.",
        "arguments": {
          "datamodel_name": "Finance"
        },
        "notes": "This retrieves the 'Finance' DataModel, which could include financial KPIs and reporting data."
      }
    ]
  },
  {
    "tool_id": "datamodel.get_datamodel_shares",
    "module": "datamodel",
    "class": "DataModel",
    "method": "get_datamodel_shares",
    "description": "Retrieves all share entries (users and groups) for a given DataModel in flat row format.",
    "full_doc": "Retrieves all share entries (users and groups) for a given DataModel in flat row format.\n\nParameters:\n    datamodel_name (str): Name of the DataModel to retrieve shares for.\n\nReturns:\n    list: List of dicts with datamodel name, party name, type, and permission.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel to retrieve shares for."
        }
      },
      "required": [
        "datamodel_name"
      ]
    },
    "mutates": false,
    "tags": [
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "Who has access to the 'SalesAnalytics' DataModel?",
        "arguments": {
          "datamodel_name": "SalesAnalytics"
        },
        "notes": "This call retrieves all share entries for the 'SalesAnalytics' DataModel, including users and groups with their permissions."
      },
      {
        "user_query": "Can you show me the sharing details for the 'CustomerInsights' DataModel?",
        "arguments": {
          "datamodel_name": "CustomerInsights"
        },
        "notes": "Use this call to get a flat list of all users and groups who have access to the 'CustomerInsights' DataModel, along with their permission levels."
      },
      {
        "user_query": "I need to audit the sharing settings for the 'MarketingPerformance' DataModel.",
        "arguments": {
          "datamodel_name": "MarketingPerformance"
        },
        "notes": "This call is useful for auditing purposes, as it provides a detailed list of all parties with access to the 'MarketingPerformance' DataModel."
      }
    ]
  },
  {
    "tool_id": "datamodel.get_datasecurity",
    "module": "datamodel",
    "class": "DataModel",
    "method": "get_datasecurity",
    "description": "Retrieves datasecurity table and column entries for a given DataModel in flat row format.",
    "full_doc": "Retrieves datasecurity table and column entries for a given DataModel in flat row format.\n\nParameters:\n    datamodel_name (str): Name of the DataModel to retrieve datasecurity for.\n\nReturns:\n    list: List of dicts with datamodel name, table name, column name, and security type.\n        If no rules exist, a single row is returned with empty values and the datamodel name.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel to retrieve datasecurity for."
        }
      },
      "required": [
        "datamodel_name"
      ]
    },
    "mutates": false,
    "tags": [
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "What are the data security rules for the Sales DataModel?",
        "arguments": {
          "datamodel_name": "Sales"
        },
        "notes": "This retrieves all data security rules for the 'Sales' DataModel, including table and column-level security."
      },
      {
        "user_query": "Can you show me the security settings for the Marketing DataModel?",
        "arguments": {
          "datamodel_name": "Marketing"
        },
        "notes": "Use this to fetch the data security configuration for the 'Marketing' DataModel, returning details in a flat row format."
      },
      {
        "user_query": "I need to check if there are any security rules defined for the Finance DataModel.",
        "arguments": {
          "datamodel_name": "Finance"
        },
        "notes": "This call retrieves the data security rules for the 'Finance' DataModel. If no rules are defined, it will return a single row with empty values and the DataModel name."
      }
    ]
  },
  {
    "tool_id": "datamodel.get_datasecurity_detail",
    "module": "datamodel",
    "class": "DataModel",
    "method": "get_datasecurity_detail",
    "description": "Retrieves detailed datasecurity rules for a specific DataModel, including share-level visibility.",
    "full_doc": "Retrieves detailed datasecurity rules for a specific DataModel, including share-level visibility.\nEach row represents a unique column-level rule and is repeated per share for clarity.\n\nSpecial handling is applied to interpret member values:\n- If \"members\" is an empty list and \"exclusionary\" is missing/null => interpreted as \"Nothing\"\n- If \"members\" is empty and \"exclusionary\" is False => interpreted as \"Everything\"\n- If values exist and \"exclusionary\" is True => treated as restricted subset\n\nParameters:\n    datamodel_name (str): Name of the DataModel to retrieve datasecurity rules for.\n\nReturns:\n    list: A list of dictionaries representing datasecurity rules in flat, share-resolved format.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel to retrieve datasecurity rules for."
        }
      },
      "required": [
        "datamodel_name"
      ]
    },
    "mutates": false,
    "tags": [
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "What are the data security rules for the Sales DataModel?",
        "arguments": {
          "datamodel_name": "Sales"
        },
        "notes": "This retrieves all column-level data security rules for the 'Sales' DataModel, including visibility rules for each share."
      },
      {
        "user_query": "Can you show me the data security details for the Finance DataModel?",
        "arguments": {
          "datamodel_name": "Finance"
        },
        "notes": "Use this to get a detailed breakdown of data security rules applied to the 'Finance' DataModel, including any share-specific restrictions."
      },
      {
        "user_query": "I need to check the security rules for the Marketing DataModel.",
        "arguments": {
          "datamodel_name": "Marketing"
        },
        "notes": "This call retrieves the data security rules for the 'Marketing' DataModel, showing how data is shared and restricted at the column level."
      }
    ]
  },
  {
    "tool_id": "datamodel.get_model_schema",
    "module": "datamodel",
    "class": "DataModel",
    "method": "get_model_schema",
    "description": "Retrieves the schema of a DataModel, including tables and columns.",
    "full_doc": "Retrieves the schema of a DataModel, including tables and columns.\n\nParameters:\n    datamodel_name (str): Name of the DataModel to retrieve the schema for.\n\nReturns:\n    list: A list of dictionaries containing schema information (one per column),\n        or an error message if not found.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel to retrieve the schema for."
        }
      },
      "required": [
        "datamodel_name"
      ]
    },
    "mutates": false,
    "tags": [
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "What is the schema of the Sales DataModel?",
        "arguments": {
          "datamodel_name": "Sales"
        },
        "notes": "This retrieves the schema for the 'Sales' DataModel, including its tables and columns. Useful for understanding the structure of the Sales DataModel."
      },
      {
        "user_query": "Can you show me the schema for the Marketing DataModel?",
        "arguments": {
          "datamodel_name": "Marketing"
        },
        "notes": "This call fetches the schema for the 'Marketing' DataModel. Use this to inspect the tables and columns available in the Marketing DataModel."
      },
      {
        "user_query": "I need to check the schema of the Customer Insights DataModel.",
        "arguments": {
          "datamodel_name": "Customer_Insights"
        },
        "notes": "This retrieves the schema for the 'Customer_Insights' DataModel. Ideal for verifying the structure of the DataModel before building dashboards or reports."
      }
    ]
  },
  {
    "tool_id": "datamodel.get_row_count",
    "module": "datamodel",
    "class": "DataModel",
    "method": "get_row_count",
    "description": "Retrieves the row count for each table in a specific DataModel",
    "full_doc": "Retrieves the row count for each table in a specific DataModel\nand returns it in a flat row-based structure suitable for tabular representation.\n\nParameters:\n    datamodel_name (str): Name of the DataModel.\n\nReturns:\n    list: List of dictionaries, each with 'table_name' and 'row_count'.\n        Includes an additional row for total row count.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel."
        }
      },
      "required": [
        "datamodel_name"
      ]
    },
    "mutates": false,
    "tags": [
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "How many rows are in the tables of the Sales DataModel?",
        "arguments": {
          "datamodel_name": "Sales_DataModel"
        },
        "notes": "This call retrieves the row count for each table in the 'Sales_DataModel', including a total row count. Use this to analyze table sizes in the Sales DataModel."
      },
      {
        "user_query": "Can you give me the row counts for the tables in the Marketing DataModel?",
        "arguments": {
          "datamodel_name": "Marketing_DataModel"
        },
        "notes": "This call provides the row counts for all tables in the 'Marketing_DataModel'. Useful for understanding data volume in the Marketing DataModel."
      },
      {
        "user_query": "I need to check the row counts for the tables in the Finance DataModel.",
        "arguments": {
          "datamodel_name": "Finance_DataModel"
        },
        "notes": "This call retrieves the row counts for each table in the 'Finance_DataModel'. Ideal for auditing or performance analysis of the Finance DataModel."
      }
    ]
  },
  {
    "tool_id": "datamodel.get_table_schema",
    "module": "datamodel",
    "class": "DataModel",
    "method": "get_table_schema",
    "description": "Retrieves the schema of a table in a specified connection from Data Source.",
    "full_doc": "Retrieves the schema of a table in a specified connection from Data Source.\nThis method uses an undocumented Sisense API endpoint to fetch the table schema details.\nNOTE: This endpoint is undocumented and may change in future versions of Sisense.\nIt is recommended to use this method with caution.\n\nParameters:\n    connection_name (str): Name of the connection.\n    database_name (str): Name of the database.\n    schema_name (str): Name of the schema.\n    table_name (str): Name of the table.\n\nReturns:\n    dict: Table schema details if found, or a dictionary with an error message.",
    "parameters": {
      "type": "object",
      "properties": {
        "connection_name": {
          "type": "string",
          "description": "Name of the connection."
        },
        "database_name": {
          "type": "string",
          "description": "Name of the database."
        },
        "schema_name": {
          "type": "string",
          "description": "Name of the schema."
        },
        "table_name": {
          "type": "string",
          "description": "Name of the table."
        }
      },
      "required": [
        "connection_name",
        "database_name",
        "schema_name",
        "table_name"
      ]
    },
    "mutates": false,
    "tags": [
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "What is the schema of the 'sales_data' table in the 'analytics' database on the 'main_connection'?",
        "arguments": {
          "connection_name": "main_connection",
          "database_name": "analytics",
          "schema_name": "public",
          "table_name": "sales_data"
        },
        "notes": "This call retrieves the schema details for the 'sales_data' table in the 'analytics' database using the 'main_connection' connection. Useful for understanding the structure of the table before querying or modeling."
      },
      {
        "user_query": "Can you fetch the schema for the 'customer_info' table in the 'crm_db' database under the 'enterprise_connection'?",
        "arguments": {
          "connection_name": "enterprise_connection",
          "database_name": "crm_db",
          "schema_name": "dbo",
          "table_name": "customer_info"
        },
        "notes": "This call retrieves the schema for the 'customer_info' table in the 'crm_db' database. This is helpful for verifying table structure in the 'enterprise_connection' data source."
      },
      {
        "user_query": "I need the schema for the 'inventory' table in the 'warehouse' database on the 'data_warehouse_connection'.",
        "arguments": {
          "connection_name": "data_warehouse_connection",
          "database_name": "warehouse",
          "schema_name": "inventory_schema",
          "table_name": "inventory"
        },
        "notes": "This call fetches the schema for the 'inventory' table in the 'warehouse' database under the 'data_warehouse_connection'. This is typically used for validating table structure before creating dashboards or widgets."
      }
    ]
  },
  {
    "tool_id": "datamodel.resolve_datamodel_reference",
    "module": "datamodel",
    "class": "DataModel",
    "method": "resolve_datamodel_reference",
    "description": "Resolve a data model reference (ID or title) to a concrete data model ID and title.",
    "full_doc": "Resolve a data model reference (ID or title) to a concrete data model ID and title.\n\nThis helper accepts a single string that may be either:\n- a Sisense data model ID, or\n- a data model title (schema name).\n\nIt first attempts to treat the reference as an ID using\n`/api/v2/datamodels/{id}/schema`. If that fails, it falls back to\ncalling `/api/v2/datamodels/schema` with a `title` query parameter.\n\nParameters\n----------\ndatamodel_ref : str\n    Data model reference to resolve. This can be either an ID or a name.\n\nReturns\n-------\ndict\n    A dictionary with the following keys:\n    - success (bool): True if the reference was resolved to a data model.\n    - status_code (int): 200 if resolved successfully, 404 if not found,\n      or 500 if an unexpected error occurred.\n    - datamodel_id (str or None): Resolved data model ID (oid) if found,\n      otherwise None.\n    - datamodel_title (str or None): Resolved data model title if found,\n      otherwise None.\n    - error (str or None): Error message if success is False, otherwise None.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_ref": {
          "type": "string",
          "description": "Data model reference to resolve. This can be either an ID or a name."
        }
      },
      "required": [
        "datamodel_ref"
      ]
    },
    "mutates": false,
    "tags": [
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "What is the ID of the data model titled 'Sales Analytics'?",
        "arguments": {
          "datamodel_ref": "Sales Analytics"
        },
        "notes": "This resolves the data model title 'Sales Analytics' to its corresponding ID and title. Use this when you know the title but need the ID."
      },
      {
        "user_query": "Can you confirm the details of the data model with ID '12345abcde'?",
        "arguments": {
          "datamodel_ref": "12345abcde"
        },
        "notes": "This resolves the data model ID '12345abcde' to its corresponding ID and title. Use this when you have the ID and want to verify its details."
      },
      {
        "user_query": "Find the data model information for 'Customer Insights'.",
        "arguments": {
          "datamodel_ref": "Customer Insights"
        },
        "notes": "This resolves the data model title 'Customer Insights' to its corresponding ID and title. Use this when you know the title but need to retrieve its details."
      }
    ]
  },
  {
    "tool_id": "datamodel.setup_datamodel",
    "module": "datamodel",
    "class": "DataModel",
    "method": "setup_datamodel",
    "description": "Setup a DataModel using existing connection and by creating a datamodel, dataset, and table.",
    "full_doc": "Setup a DataModel using existing connection and by creating a datamodel, dataset, and table.\n\nParameters:\n    datamodel_name (str): Name of the DataModel.\n    datamodel_type (str): Type of DataModel. Should be either \"extract\" (for Elasticube) or \"live\" (for Live).\n    connection_name (str): Name of the connection to use.\n    database_name (str): Name of the data source database.\n    schema_name (str): Name of the data source schema.\n    dataset_name (str, optional): Name of the dataset. Defaults to schema name if not provided.\n    tables (list): List of tables to create in the DataModel.\n        Each table should be a dictionary with keys:\n        \"table_name\", \"import_query\", \"description\", \"tags\", and \"build_behavior_config\".\n        import_query (str, optional): SQL statement used as custom import query. Defaults to None.\n        description (str, optional): Description for the table. Defaults to an empty string.\n        tags (list, optional): List of tags to apply to the table. Defaults to None.\n        build_behavior_config (dict, optional): Configuration for table build behavior.\n\nReturns:\n    dict: A dictionary containing the full DataModel object on success or an error message on failure.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel."
        },
        "datamodel_type": {
          "type": "string",
          "description": "Type of DataModel. Should be either \"extract\" (for Elasticube) or \"live\" (for Live).",
          "enum": [
            "extract",
            "live"
          ],
          "x-aliases": {
            "extract": [
              "ec",
              "elasticube",
              "elastic cube",
              "cube",
              "elastic-cube"
            ],
            "live": [
              "realtime",
              "real-time",
              "live model"
            ]
          }
        },
        "connection_name": {
          "type": "string",
          "description": "Name of the connection to use."
        },
        "database_name": {
          "type": "string",
          "description": "Name of the data source database."
        },
        "schema_name": {
          "type": "string",
          "description": "Name of the data source schema."
        },
        "tables": {
          "type": "array",
          "description": "List of tables to add. For 'live' models, omit build_behavior_config. For 'extract' models, use build_behavior_config as needed.",
          "items": {
            "type": "object",
            "properties": {
              "database_name": {
                "type": "string",
                "description": "Optional override of the table's database. Defaults to top-level database_name if omitted."
              },
              "schema_name": {
                "type": "string",
                "description": "Optional override of the table's schema. Defaults to top-level schema_name if omitted."
              },
              "table_name": {
                "type": "string",
                "description": "Physical table name to add, or a logical name when using import_query."
              },
              "import_query": {
                "type": "string",
                "description": "Optional custom SQL (executed as-is). Use fully-qualified tables: schema.table (Databricks: `schema`.`table`)."
              },
              "description": {
                "type": "string",
                "description": "Optional table description."
              },
              "tags": {
                "type": "array",
                "description": "Optional list of tags for the table.",
                "items": {
                  "type": "string"
                }
              },
              "build_behavior_config": {
                "type": "object",
                "description": "Extract models only; omit for 'live'. For 'increment' mode, column_name is required.",
                "properties": {
                  "mode": {
                    "type": "string",
                    "enum": [
                      "replace",
                      "replace_changes",
                      "append",
                      "increment"
                    ],
                    "description": "Table build behavior for extract models."
                  },
                  "column_name": {
                    "type": "string",
                    "description": "Required when mode='increment'; ignored otherwise."
                  }
                }
              }
            },
            "required": [
              "table_name"
            ]
          },
          "minItems": 1
        },
        "dataset_name": {
          "type": "string",
          "description": "Name of the dataset. Defaults to schema name if not provided."
        }
      },
      "required": [
        "datamodel_name",
        "datamodel_type",
        "connection_name",
        "database_name",
        "schema_name",
        "tables"
      ]
    },
    "mutates": true,
    "tags": [
      "datamodel",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "Create a new DataModel for an Elasticube using the 'SalesDB' connection and add two tables.",
        "arguments": {
          "datamodel_name": "Sales_Elasticube",
          "datamodel_type": "extract",
          "connection_name": "SalesDB",
          "database_name": "sales_database",
          "schema_name": "public",
          "tables": [
            {
              "table_name": "orders",
              "description": "Table containing order data.",
              "tags": [
                "sales",
                "orders"
              ],
              "build_behavior_config": {
                "mode": "replace"
              }
            },
            {
              "table_name": "customers",
              "description": "Table containing customer data.",
              "tags": [
                "sales",
                "customers"
              ],
              "build_behavior_config": {
                "mode": "increment",
                "column_name": "updated_at"
              }
            }
          ],
          "dataset_name": "SalesDataset"
        },
        "notes": "This call sets up an 'extract' DataModel for sales data, creating two tables with specific build behaviors."
      },
      {
        "user_query": "Set up a live DataModel for the 'MarketingDB' connection with one table using a custom SQL query.",
        "arguments": {
          "datamodel_name": "Marketing_LiveModel",
          "datamodel_type": "live",
          "connection_name": "MarketingDB",
          "database_name": "marketing_database",
          "schema_name": "analytics",
          "tables": [
            {
              "table_name": "campaign_performance",
              "import_query": "SELECT campaign_id, impressions, clicks FROM analytics.campaigns WHERE active = TRUE",
              "description": "Live data for active marketing campaigns.",
              "tags": [
                "marketing",
                "campaigns"
              ]
            }
          ]
        },
        "notes": "This call creates a 'live' DataModel for marketing data using a custom SQL query. For live models, omit build_behavior_config."
      },
      {
        "user_query": "Create a new DataModel for an Elasticube with three tables, using different build behaviors.",
        "arguments": {
          "datamodel_name": "Finance_Elasticube",
          "datamodel_type": "extract",
          "connection_name": "FinanceDB",
          "database_name": "finance_database",
          "schema_name": "main",
          "tables": [
            {
              "table_name": "transactions",
              "description": "Financial transactions data.",
              "tags": [
                "finance",
                "transactions"
              ],
              "build_behavior_config": {
                "mode": "append"
              }
            },
            {
              "table_name": "accounts",
              "description": "Account details.",
              "tags": [
                "finance",
                "accounts"
              ],
              "build_behavior_config": {
                "mode": "replace"
              }
            },
            {
              "table_name": "audit_logs",
              "description": "Audit logs for financial activities.",
              "tags": [
                "finance",
                "audit"
              ],
              "build_behavior_config": {
                "mode": "replace_changes"
              }
            }
          ],
          "dataset_name": "FinanceDataset"
        },
        "notes": "This call sets up an 'extract' DataModel for finance data with different build behaviors for each table."
      }
    ]
  },
  {
    "tool_id": "dashboard.add_dashboard_script",
    "module": "dashboard",
    "class": "Dashboard",
    "method": "add_dashboard_script",
    "description": "Adds or overwrites a script to a dashboard, temporarily changing ownership if required.",
    "full_doc": "Adds or overwrites a script to a dashboard, temporarily changing ownership if required.\n\nParameters:\n    dashboard_id (str): The ID of the dashboard where the script will be added.\n    script (str): The JavaScript script as either:\n                - A properly formatted JSON string.\n                - A raw Python docstring (multi-line string).\n    executing_user (str, optional): The username of the API user. This is used to temporarily change\n                                the owner of the dashboard, as only the owner can add scripts.\n                                If not provided, assumes the dashboard owner is the same as the API user.\n\nReturns:\n    str: Success message or error details.",
    "parameters": {
      "type": "object",
      "properties": {
        "dashboard_id": {
          "type": "string",
          "description": "The ID of the dashboard where the script will be added."
        },
        "script": {
          "type": "string",
          "description": "The JavaScript script as either: - A properly formatted JSON string. - A raw Python docstring (multi-line string)."
        },
        "executing_user": {
          "type": "string",
          "description": "The username of the API user. This is used to temporarily change the owner of the dashboard, as only the owner can add scripts. If not provided, assumes the dashboard owner is the same as the API user."
        }
      },
      "required": [
        "dashboard_id",
        "script"
      ]
    },
    "mutates": true,
    "tags": [
      "dashboard",
      "dashboards",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "dashboard.add_dashboard_shares",
    "module": "dashboard",
    "class": "Dashboard",
    "method": "add_dashboard_shares",
    "description": "Adds or updates shares for a dashboard, specifying users and groups along with their access rules.",
    "full_doc": "Adds or updates shares for a dashboard, specifying users and groups along with their access rules.\n\nParameters:\n    dashboard_id (str): The ID of the dashboard to which the shares will be added.\n    shares (list of dicts): A list of dictionaries, each containing:\n        - \"name\" (str): The username or group name.\n        - \"type\" (str): Either \"user\" or \"group\" to indicate the share type.\n        - \"rule\" (str): The access level (e.g., \"view\", \"edit\").\n\nReturns:\n    str: Success message or error details.",
    "parameters": {
      "type": "object",
      "properties": {
        "dashboard_id": {
          "type": "string",
          "description": "The ID of the dashboard to which the shares will be added."
        },
        "shares": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "A list of dictionaries, each containing: - \"name\" (str): The username or group name. - \"type\" (str): Either \"user\" or \"group\" to indicate the share type. - \"rule\" (str): The access level (e.g., \"view\", \"edit\")."
        }
      },
      "required": [
        "dashboard_id",
        "shares"
      ]
    },
    "mutates": true,
    "tags": [
      "dashboard",
      "dashboards",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "Share the sales dashboard with the marketing team and give them view access.",
        "arguments": {
          "dashboard_id": "dashboard_12345",
          "shares": [
            {
              "name": "marketing_team",
              "type": "group",
              "rule": "view"
            }
          ]
        },
        "notes": "This call shares the specified dashboard with the marketing team group, granting them view-only access."
      },
      {
        "user_query": "Grant edit access to John Doe for the executive summary dashboard.",
        "arguments": {
          "dashboard_id": "dashboard_67890",
          "shares": [
            {
              "name": "john.doe",
              "type": "user",
              "rule": "edit"
            }
          ]
        },
        "notes": "This call gives John Doe edit permissions for the executive summary dashboard."
      },
      {
        "user_query": "Update the shares for the regional performance dashboard to allow the sales team to edit and the finance team to view.",
        "arguments": {
          "dashboard_id": "dashboard_54321",
          "shares": [
            {
              "name": "sales_team",
              "type": "group",
              "rule": "edit"
            },
            {
              "name": "finance_team",
              "type": "group",
              "rule": "view"
            }
          ]
        },
        "notes": "This call updates the sharing rules for the regional performance dashboard, granting edit access to the sales team and view access to the finance team."
      }
    ]
  },
  {
    "tool_id": "dashboard.add_widget_script",
    "module": "dashboard",
    "class": "Dashboard",
    "method": "add_widget_script",
    "description": "Adds or overwrites a script for a specific widget within a dashboard.",
    "full_doc": "Adds or overwrites a script for a specific widget within a dashboard.\n\nIf required, temporarily changes the dashboard ownership, as only the owner can modify widget scripts.\n\nParameters:\n    dashboard_id (str): The ID of the dashboard containing the widget.\n    widget_id (str): The ID of the widget where the script will be added.\n    script (str): The JavaScript script as either:\n                - A properly formatted JSON string.\n                - A raw Python docstring (multi-line string).\n    executing_user (str, optional): The username of the API user. This is used to temporarily change\n                                the owner of the dashboard, as only the owner can add scripts.\n                                If not provided, assumes the dashboard owner is the same as the API user.\n\nReturns:\n    str: Success message or error details.",
    "parameters": {
      "type": "object",
      "properties": {
        "dashboard_id": {
          "type": "string",
          "description": "The ID of the dashboard containing the widget."
        },
        "widget_id": {
          "type": "string",
          "description": "The ID of the widget where the script will be added."
        },
        "script": {
          "type": "string",
          "description": "The JavaScript script as either: - A properly formatted JSON string. - A raw Python docstring (multi-line string)."
        },
        "executing_user": {
          "type": "string",
          "description": "The username of the API user. This is used to temporarily change the owner of the dashboard, as only the owner can add scripts. If not provided, assumes the dashboard owner is the same as the API user."
        }
      },
      "required": [
        "dashboard_id",
        "widget_id",
        "script"
      ]
    },
    "mutates": true,
    "tags": [
      "dashboard",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": []
  },
  {
    "tool_id": "dashboard.get_all_dashboards",
    "module": "dashboard",
    "class": "Dashboard",
    "method": "get_all_dashboards",
    "description": "Retrieves all dashboards from the Sisense server.",
    "full_doc": "Retrieves all dashboards from the Sisense server.\n\nReturns:\n    list or dict: A list of dashboards if successful,\n                or a dict containing an error message.",
    "parameters": {
      "type": "object",
      "properties": {},
      "required": []
    },
    "mutates": false,
    "tags": [
      "dashboard",
      "dashboards",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "How can I list all dashboards available on the Sisense server?",
        "arguments": {},
        "notes": "This call retrieves a complete list of all dashboards currently stored on the Sisense server. Useful for getting an overview of available dashboards."
      },
      {
        "user_query": "I need to fetch all dashboards to check which ones are active.",
        "arguments": {},
        "notes": "This call can be used to fetch all dashboards, which can then be filtered or analyzed to determine their status or usage."
      },
      {
        "user_query": "Can I get a list of all dashboards to display in my custom application?",
        "arguments": {},
        "notes": "This call retrieves all dashboards, which can be integrated into a custom application for display or further processing."
      }
    ]
  },
  {
    "tool_id": "dashboard.get_dashboard_by_id",
    "module": "dashboard",
    "class": "Dashboard",
    "method": "get_dashboard_by_id",
    "description": "Retrieves a specific dashboard by its ID.",
    "full_doc": "Retrieves a specific dashboard by its ID.\n\nParameters:\n    dashboard_id (str): The ID of the dashboard to retrieve.\n\nReturns:\n    dict: A dictionary containing dashboard details if found,\n        or a dict with an error message if the request fails.",
    "parameters": {
      "type": "object",
      "properties": {
        "dashboard_id": {
          "type": "string",
          "description": "The ID of the dashboard to retrieve."
        }
      },
      "required": [
        "dashboard_id"
      ]
    },
    "mutates": false,
    "tags": [
      "dashboard",
      "dashboards",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "Can you fetch the dashboard with ID 'sales_dashboard_2023'?",
        "arguments": {
          "dashboard_id": "sales_dashboard_2023"
        },
        "notes": "This retrieves the dashboard with the specified ID, which could represent a sales performance dashboard for 2023."
      },
      {
        "user_query": "I need to get the details of the dashboard with ID 'marketing_overview'.",
        "arguments": {
          "dashboard_id": "marketing_overview"
        },
        "notes": "This call fetches the dashboard with the given ID, likely used for viewing marketing performance metrics."
      },
      {
        "user_query": "What does the dashboard with ID 'finance_q4_report' contain?",
        "arguments": {
          "dashboard_id": "finance_q4_report"
        },
        "notes": "This retrieves the dashboard with the specified ID, which might be used for analyzing financial data for Q4."
      }
    ]
  },
  {
    "tool_id": "dashboard.get_dashboard_by_name",
    "module": "dashboard",
    "class": "Dashboard",
    "method": "get_dashboard_by_name",
    "description": "Retrieves a specific dashboard by its name.",
    "full_doc": "Retrieves a specific dashboard by its name.\n\nParameters:\n    dashboard_name (str): The name of the dashboard to retrieve.\n\nReturns:\n    dict or list: A dictionary containing dashboard details if found,\n                or {'error': 'message'} if not found or failed.",
    "parameters": {
      "type": "object",
      "properties": {
        "dashboard_name": {
          "type": "string",
          "description": "The name of the dashboard to retrieve."
        }
      },
      "required": [
        "dashboard_name"
      ]
    },
    "mutates": false,
    "tags": [
      "dashboard",
      "dashboards",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "Find the dashboard named 'Sales Performance Overview'.",
        "arguments": {
          "dashboard_name": "Sales Performance Overview"
        },
        "notes": "This call retrieves the dashboard details for 'Sales Performance Overview', which might be used to analyze sales metrics."
      },
      {
        "user_query": "Get the dashboard titled 'Marketing Campaign Analytics'.",
        "arguments": {
          "dashboard_name": "Marketing Campaign Analytics"
        },
        "notes": "Use this call to fetch insights and data visualizations related to marketing campaign performance."
      },
      {
        "user_query": "Locate the dashboard named 'Customer Retention Metrics'.",
        "arguments": {
          "dashboard_name": "Customer Retention Metrics"
        },
        "notes": "This call is useful for accessing dashboards focused on customer retention analysis and trends."
      }
    ]
  },
  {
    "tool_id": "dashboard.get_dashboard_columns",
    "module": "dashboard",
    "class": "Dashboard",
    "method": "get_dashboard_columns",
    "description": "Retrieves columns from a specific dashboard, including both widget and filter-level columns.",
    "full_doc": "Retrieves columns from a specific dashboard, including both widget and filter-level columns.\n\nThis method:\n- Uses the `get_dashboard_by_name` method to fetch the dashboard.\n- Extracts columns from widgets and filters.\n- Deduplicates the final column list.\n\nParameters:\n    dashboard_name (str): The name of the dashboard to retrieve columns from.\n\nReturns:\n    list: A list of dictionaries containing distinct table and column information from the dashboard.",
    "parameters": {
      "type": "object",
      "properties": {
        "dashboard_name": {
          "type": "string",
          "description": "The name of the dashboard to retrieve columns from."
        }
      },
      "required": [
        "dashboard_name"
      ]
    },
    "mutates": false,
    "tags": [
      "dashboard",
      "dashboards",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "What columns are available in the 'Sales Overview' dashboard?",
        "arguments": {
          "dashboard_name": "Sales Overview"
        },
        "notes": "This retrieves all distinct columns, including widget and filter-level columns, from the 'Sales Overview' dashboard."
      },
      {
        "user_query": "Can you list the columns used in the 'Marketing Performance' dashboard?",
        "arguments": {
          "dashboard_name": "Marketing Performance"
        },
        "notes": "Use this to get a list of all unique columns from the 'Marketing Performance' dashboard, which can include data from widgets and filters."
      },
      {
        "user_query": "I need to see the columns in the 'Customer Insights' dashboard for a report.",
        "arguments": {
          "dashboard_name": "Customer Insights"
        },
        "notes": "This call fetches and deduplicates all column data from the 'Customer Insights' dashboard, useful for reporting or analysis."
      }
    ]
  },
  {
    "tool_id": "dashboard.get_dashboard_share",
    "module": "dashboard",
    "class": "Dashboard",
    "method": "get_dashboard_share",
    "description": "Retrieves share details (users and groups) for a specific dashboard by title.",
    "full_doc": "Retrieves share details (users and groups) for a specific dashboard by title.\n\nParameters:\n    dashboard_name (str): The title of the dashboard to retrieve share information for.\n\nReturns:\n    list: A list of dictionaries containing share type (user or group), and share name (email or group name),\n        or an empty list if the dashboard is not found or has no shares.",
    "parameters": {
      "type": "object",
      "properties": {
        "dashboard_name": {
          "type": "string",
          "description": "The title of the dashboard to retrieve share information for."
        }
      },
      "required": [
        "dashboard_name"
      ]
    },
    "mutates": false,
    "tags": [
      "dashboard",
      "dashboards",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "Who has access to the Sales Performance dashboard?",
        "arguments": {
          "dashboard_name": "Sales Performance"
        },
        "notes": "Retrieves the list of users and groups who have access to the 'Sales Performance' dashboard. Use this to review sharing permissions for this dashboard."
      },
      {
        "user_query": "Can you show me the sharing details for the Executive Overview dashboard?",
        "arguments": {
          "dashboard_name": "Executive Overview"
        },
        "notes": "Fetches the sharing details for the 'Executive Overview' dashboard, including users and groups with access. Useful for auditing access permissions."
      },
      {
        "user_query": "I need to check who can view the Marketing Analytics dashboard.",
        "arguments": {
          "dashboard_name": "Marketing Analytics"
        },
        "notes": "Provides the share details for the 'Marketing Analytics' dashboard. This is helpful for ensuring the correct users and groups have access."
      }
    ]
  },
  {
    "tool_id": "dashboard.resolve_dashboard_reference",
    "module": "dashboard",
    "class": "Dashboard",
    "method": "resolve_dashboard_reference",
    "description": "Resolve a dashboard reference (ID or name) to a concrete dashboard ID and title.",
    "full_doc": "Resolve a dashboard reference (ID or name) to a concrete dashboard ID and title.\n\nThis helper accepts a single string that may be either:\n- a Sisense dashboard ID (24-character ID), or\n- a dashboard title (name).\n\nIt first attempts to treat the reference as an ID using\n`get_dashboard_by_id`. If that fails or the reference does not look\nlike an ID, it falls back to `get_dashboard_by_name`. The underlying\nmethods are reused as-is.\n\nParameters\n----------\ndashboard_ref : str\n    Dashboard reference to resolve. This can be either an ID or a name.\n\nReturns\n-------\ndict\n    A dictionary with the following keys:\n    - success (bool): True if the reference was resolved to a dashboard.\n    - status_code (int): 200 if resolved successfully, 404 if not found,\n      or 500 if an unexpected error occurred.\n    - dashboard_id (str or None): Resolved dashboard ID (oid) if found,\n      otherwise None.\n    - dashboard_title (str or None): Resolved dashboard title if found,\n      otherwise None.\n    - error (str or None): Error message if success is False, otherwise None.",
    "parameters": {
      "type": "object",
      "properties": {
        "dashboard_ref": {
          "type": "string",
          "description": "Dashboard reference to resolve. This can be either an ID or a name."
        }
      },
      "required": [
        "dashboard_ref"
      ]
    },
    "mutates": false,
    "tags": [
      "dashboard",
      "dashboards",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "What is the ID and title of the dashboard named 'Sales Overview'?",
        "arguments": {
          "dashboard_ref": "Sales Overview"
        },
        "notes": "This resolves the dashboard reference 'Sales Overview' to its concrete ID and title. Useful when you know the dashboard name but need its ID for further operations."
      },
      {
        "user_query": "Can you find the dashboard with ID '5f3a2b1c4d5e6f7g8h9i0jkl' and return its title?",
        "arguments": {
          "dashboard_ref": "5f3a2b1c4d5e6f7g8h9i0jkl"
        },
        "notes": "This resolves the dashboard reference using a known ID. Use this when you have the ID but want to confirm the associated title."
      },
      {
        "user_query": "I need the details for the dashboard called 'Marketing Performance'.",
        "arguments": {
          "dashboard_ref": "Marketing Performance"
        },
        "notes": "This resolves the dashboard reference 'Marketing Performance' to retrieve its ID and title. Ideal for cases where you know the dashboard name but not its ID."
      }
    ]
  },
  {
    "tool_id": "migration.migrate_all_dashboards",
    "module": "migration",
    "class": "Migration",
    "method": "migrate_all_dashboards",
    "description": "Migrates all dashboards from the source to the target environment in batches.",
    "full_doc": "Migrates all dashboards from the source to the target environment in batches.\n\nParameters\n----------\naction : str, optional\n    Determines how to handle existing dashboards in the target environment.\n    Options:\n    - 'skip': Skip existing dashboards in the target; new dashboards are processed\n    normally, including shares and ownership.\n    - 'overwrite': Overwrite existing dashboards; shares and ownership will not be\n    migrated. If the dashboard already exists, shares will be retained, but the API\n    user will be set as the new owner.\n    - 'duplicate': Create a duplicate of existing dashboards without migrating shares\n    or ownership.\n    Default: None. Existing dashboards are skipped, and only new ones are migrated.\n    Unless existing dashboards are different owners, shares will be migrated.\n    **Note:** If an existing dashboard in the target environment has a different owner\n    than the user's token running the SDK, the dashboard will be migrated with a new\n    ID, and its shares and ownership will be migrated from the original source\n    dashboard.\nrepublish : bool, optional\n    Whether to republish dashboards after migration. Default: False.\nmigrate_share : bool, optional\n    Whether to migrate shares for the dashboards. If `True`, shares will be\n    migrated, and ownership migration will be controlled by the `change_ownership` parameter.\n    If `False`, both shares and ownership migration will be skipped. Default: False.\nchange_ownership : bool, optional\n    Whether to change ownership of the target dashboards.\n    Effective only if `migrate_share` is True. Default: False.\nbatch_size : int, optional\n    Number of dashboards to process in each batch. Default: 10.\nsleep_time : int, optional\n    Time (in seconds) to sleep between batches. Default: 10 seconds.\nemit : Callable[[Dict[str, Any]], None], optional\n    Optional callback invoked with structured progress events. If not provided, the method\n    emits no events and only returns a final result.\n\n    Event payloads follow a consistent shape:\n    - ``type``: str (\"started\" | \"progress\" | \"warning\" | \"error\" | \"completed\")\n    - ``step``: str logical step identifier\n    - ``message``: str human-readable message\n    - Additional fields depending on the step (counts, status_code, etc.)\n\nReturns\n-------\ndict\n    A summary of the migration results for all batches, containing lists of succeeded, skipped,\n    and failed dashboards.\n\nNotes\n-----\n- **Batch Processing**: Dashboards are processed in batches to avoid overloading the system.\n- **Best Use Case**: This method is suitable when migrating all dashboards from a source to a\ntarget environment.\n- **Overwrite Action**: When using `overwrite`, shares and ownership will not be migrated.\nIf a dashboard already exists, the target dashboard will be overwritten, retaining its existing shares\nbut setting the API user as the new owner. Subsequent adjustments to shares and ownership will not be\nsupported in this mode.\n- **Duplicate Action**: Creates duplicate dashboards without shares and ownership migration.\n- **Skip Action**: Skips migration for existing dashboards, but new ones are processed normally.",
    "parameters": {
      "type": "object",
      "properties": {
        "action": {
          "type": "string",
          "description": "Determines how to handle existing dashboards in the target environment. Options: - 'skip': Skip existing dashboards in the target; new dashboards are processed normally, including shares and ownership. - 'overwrite': Overwrite existing dashboards; shares and ownership will not be migrated. If the dashboard already exists, shares will be retained, but the API user will be set as the new owner. - 'duplicate': Create a duplicate of existing dashboards without migrating shares or ownership. Default: None. Existing dashboards are skipped, and only new ones are migrated. Unless existing dashboards are different owners, shares will be migrated. **Note:** If an existing dashboard in the target environment has a different owner than the user's token running the SDK, the dashboard will be migrated with a new ID, and its shares and ownership will be migrated from the original source dashboard.",
          "enum": [
            "skip",
            "overwrite",
            "duplicate"
          ]
        },
        "republish": {
          "type": "boolean",
          "description": "Whether to republish dashboards after migration. Default: False."
        },
        "migrate_share": {
          "type": "boolean",
          "description": "Whether to migrate shares for the dashboards. If `True`, shares will be migrated, and ownership migration will be controlled by the `change_ownership` parameter. If `False`, both shares and ownership migration will be skipped. Default: False."
        },
        "change_ownership": {
          "type": "boolean",
          "description": "Whether to change ownership of the target dashboards. Effective only if `migrate_share` is True. Default: False."
        },
        "batch_size": {
          "type": "integer",
          "description": "Number of dashboards to process in each batch. Default: 10."
        },
        "sleep_time": {
          "type": "integer",
          "description": "Time (in seconds) to sleep between batches. Default: 10 seconds."
        },
        "emit": {
          "type": "object",
          "description": "Optional callback invoked with structured progress events. If not provided, the method emits no events and only returns a final result. Event payloads follow a consistent shape: - ``type``: str (\"started\" | \"progress\" | \"warning\" | \"error\" | \"completed\") - ``step``: str logical step identifier - ``message``: str human-readable message - Additional fields depending on the step (counts, status_code, etc.)"
        }
      },
      "required": []
    },
    "mutates": true,
    "tags": [
      "migration",
      "dashboards",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "How can I migrate all dashboards to the target environment, but skip any dashboards that already exist there?",
        "arguments": {
          "action": "skip",
          "republish": false,
          "migrate_share": false,
          "change_ownership": false,
          "batch_size": 10,
          "sleep_time": 10
        },
        "notes": "This call migrates all dashboards from the source to the target environment, skipping any dashboards that already exist in the target. Shares and ownership will not be migrated, and dashboards will not be republished."
      },
      {
        "user_query": "I want to migrate all dashboards and overwrite any existing ones in the target environment. Can I do that?",
        "arguments": {
          "action": "overwrite",
          "republish": true,
          "migrate_share": false,
          "change_ownership": false,
          "batch_size": 20,
          "sleep_time": 5
        },
        "notes": "This call migrates all dashboards to the target environment, overwriting any existing dashboards. Shares and ownership will not be migrated, but the dashboards will be republished after migration. The process will handle 20 dashboards per batch with a 5-second pause between batches."
      },
      {
        "user_query": "Can I duplicate all dashboards in the target environment without migrating shares or ownership?",
        "arguments": {
          "action": "duplicate",
          "republish": false,
          "migrate_share": false,
          "change_ownership": false,
          "batch_size": 15,
          "sleep_time": 10
        },
        "notes": "This call duplicates all dashboards in the target environment without migrating shares or ownership. Dashboards will not be republished, and the process will handle 15 dashboards per batch with a 10-second pause between batches."
      }
    ]
  },
  {
    "tool_id": "migration.migrate_all_datamodels",
    "module": "migration",
    "class": "Migration",
    "method": "migrate_all_datamodels",
    "description": "Migrates all data models from the source environment to the target environment in batches.",
    "full_doc": "Migrates all data models from the source environment to the target environment in batches.\n\nParameters\n----------\ndependencies : list[str] or str or None, default None\n    Dependencies to include in the migration. If None or \"all\", all supported dependencies\n    are included by default.\n\n    Supported values:\n    - \"dataSecurity\" (includes both Data Security and Scope Configuration)\n    - \"formulas\" (for Formulas)\n    - \"hierarchies\" (for Drill Hierarchies)\n    - \"perspectives\" (for Perspectives)\n\nshares : bool, default False\n    Whether to also migrate data model shares after the schema import.\n\nbatch_size : int, default 10\n    Number of data models to migrate per batch.\n\nsleep_time : int, default 5\n    Time (in seconds) to sleep between batches.\n\naction : str or None, default None\n    Strategy to handle existing data models in the target environment.\n\n    - \"overwrite\": Attempts to overwrite an existing model using its original ID via the\n    ``datamodelId`` parameter. If the model is not found in the target environment, it\n    falls back to creating the model.\n    - \"duplicate\": Creates a new model by appending \" (Duplicate)\" to the original name.\n\nemit : Callable[[dict], None] or None, default None\n    Optional callback invoked with structured progress events. If not provided, the method\n    emits no events and only returns a final result.\n\n    Event payloads follow a consistent shape:\n    - ``type``: str (\"started\" | \"progress\" | \"warning\" | \"error\" | \"completed\")\n    - ``step``: str logical step identifier\n    - ``message``: str human-readable message\n    - Additional fields depending on the step (counts, status_code, etc.)\n\nReturns\n-------\ndict\n    A migration summary containing:\n    - ``succeeded``: list\n    - ``skipped``: list\n    - ``failed``: list\n    - Counts/metadata fields (for example: ``total_count``, ``batches_total``, etc.)",
    "parameters": {
      "type": "object",
      "properties": {
        "dependencies": {
          "type": "array",
          "items": {
            "type": "string",
            "enum": [
              "dataSecurity",
              "formulas",
              "hierarchies",
              "perspectives"
            ]
          },
          "description": "Dependencies to include in the migration. If None or \"all\", all supported dependencies are included by default. Supported values: - \"dataSecurity\" (includes both Data Security and Scope Configuration) - \"formulas\" (for Formulas) - \"hierarchies\" (for Drill Hierarchies) - \"perspectives\" (for Perspectives)"
        },
        "shares": {
          "type": "boolean",
          "description": "Whether to also migrate data model shares after the schema import."
        },
        "batch_size": {
          "type": "integer",
          "description": "Number of data models to migrate per batch."
        },
        "sleep_time": {
          "type": "integer",
          "description": "Time (in seconds) to sleep between batches."
        },
        "action": {
          "type": "string",
          "description": "Strategy to handle existing data models in the target environment. - \"overwrite\": Attempts to overwrite an existing model using its original ID via the ``datamodelId`` parameter. If the model is not found in the target environment, it falls back to creating the model. - \"duplicate\": Creates a new model by appending \" (Duplicate)\" to the original name.",
          "enum": [
            "overwrite",
            "duplicate"
          ]
        },
        "emit": {
          "type": "object",
          "description": "Optional callback invoked with structured progress events. If not provided, the method emits no events and only returns a final result. Event payloads follow a consistent shape: - ``type``: str (\"started\" | \"progress\" | \"warning\" | \"error\" | \"completed\") - ``step``: str logical step identifier - ``message``: str human-readable message - Additional fields depending on the step (counts, status_code, etc.)"
        }
      },
      "required": []
    },
    "mutates": true,
    "tags": [
      "migration",
      "datamodel",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "How can I migrate all data models, including data security and formulas, to the target environment while overwriting existing models?",
        "arguments": {
          "dependencies": [
            "dataSecurity",
            "formulas"
          ],
          "shares": false,
          "batch_size": 10,
          "sleep_time": 5,
          "action": "overwrite",
          "emit": null
        },
        "notes": "This call migrates all data models with the specified dependencies (data security and formulas) in batches of 10, overwriting any existing models in the target environment. No progress events are emitted."
      },
      {
        "user_query": "I want to migrate all data models with all dependencies, including shares, and create duplicates for existing models in the target environment.",
        "arguments": {
          "dependencies": null,
          "shares": true,
          "batch_size": 20,
          "sleep_time": 10,
          "action": "duplicate",
          "emit": null
        },
        "notes": "This call migrates all data models with all dependencies (default behavior when dependencies is null) and includes shares. Existing models in the target environment are duplicated. The migration is performed in batches of 20 with a 10-second pause between batches."
      },
      {
        "user_query": "Can I migrate perspectives and hierarchies while monitoring progress through events?",
        "arguments": {
          "dependencies": [
            "perspectives",
            "hierarchies"
          ],
          "shares": false,
          "batch_size": 5,
          "sleep_time": 2,
          "action": "overwrite",
          "emit": {
            "type": "progress",
            "step": "batch_migration",
            "message": "Migrating batch of data models"
          }
        },
        "notes": "This call migrates only perspectives and hierarchies in batches of 5, overwriting existing models in the target environment. Progress events are emitted during the migration to provide updates on each batch."
      }
    ]
  },
  {
    "tool_id": "migration.migrate_all_groups",
    "module": "migration",
    "class": "Migration",
    "method": "migrate_all_groups",
    "description": "Migrate groups from the source environment to the target environment using the bulk endpoint.",
    "full_doc": "Migrate groups from the source environment to the target environment using the bulk endpoint.\n\nThis method supports optional progress emission via the ``emit`` callback. If provided,\nthe method will publish structured progress events at key milestones so a caller (for\nexample an MCP server) can stream updates to a UI while the migration is running.\n\nParameters\n----------\nemit : Callable[[Dict[str, Any]], None], optional\n    Optional callback invoked with structured progress events. If not provided, the\n    method behaves like a standard synchronous call and only returns a final result.\n\n    Each emitted event is a dictionary that typically includes:\n    - ``type``: str, event type (e.g., \"started\", \"progress\", \"completed\", \"error\")\n    - ``step``: str, logical step name\n    - ``message``: str, human-readable message\n    - Additional fields depending on the event (counts, status_code, etc.)\n\nReturns\n-------\nDict[str, Any]\n    Structured result payload with:\n    - ``ok``: bool\n    - ``status``: str (\"success\" | \"failed\" | \"noop\")\n    - ``results``: List[Dict[str, str]] per-group statuses\n    - ``source_count``: int\n    - ``eligible_count``: int\n    - ``success_count``: int\n    - ``failed_count``: int\n    - ``raw_error``: Any\n    - ``warnings``: List[str]",
    "parameters": {
      "type": "object",
      "properties": {
        "emit": {
          "type": "object",
          "description": "Optional callback invoked with structured progress events. If not provided, the method behaves like a standard synchronous call and only returns a final result. Each emitted event is a dictionary that typically includes: - ``type``: str, event type (e.g., \"started\", \"progress\", \"completed\", \"error\") - ``step``: str, logical step name - ``message``: str, human-readable message - Additional fields depending on the event (counts, status_code, etc.)"
        }
      },
      "required": []
    },
    "mutates": true,
    "tags": [
      "migration",
      "groups",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "How can I migrate all user groups from the old environment to the new one and track the progress?",
        "arguments": {
          "emit": {
            "type": "progress",
            "step": "initializing",
            "message": "Starting group migration process"
          }
        },
        "notes": "This call migrates all user groups from the source environment to the target environment and uses the 'emit' callback to provide real-time progress updates."
      },
      {
        "user_query": "I need to migrate all groups but don't need progress updates. How can I do that?",
        "arguments": {},
        "notes": "This call migrates all user groups without providing progress updates. It will return a final result once the migration is complete."
      },
      {
        "user_query": "Can I get detailed progress updates while migrating groups from one environment to another?",
        "arguments": {
          "emit": {
            "type": "progress",
            "step": "migrating",
            "message": "Migrating groups in bulk"
          }
        },
        "notes": "This call migrates all user groups and provides detailed progress updates at key milestones using the 'emit' callback."
      }
    ]
  },
  {
    "tool_id": "migration.migrate_all_users",
    "module": "migration",
    "class": "Migration",
    "method": "migrate_all_users",
    "description": "Migrate all eligible users from the source environment to the target environment using the bulk endpoint.",
    "full_doc": "Migrate all eligible users from the source environment to the target environment using the bulk endpoint.\n\nThis method is designed to support MCP-style streaming by optionally emitting structured\nprogress events via the ``emit`` callback. It remains synchronous, but callers can run it\nin a worker thread and forward events to an event stream.\n\nThe migration performs the following steps:\n1) Fetch users from the source environment (expanded with groups and role).\n2) Fetch roles and groups from the target environment for ID mapping.\n3) Build a bulk payload by mapping role names to role IDs and group names to group IDs.\n4) Submit the payload to the target bulk endpoint.\n5) Return a structured summary and per-user status list.\n\nParameters\n----------\nemit : Callable[[Dict[str, Any]], None], optional\n    Optional callback invoked with structured progress events. If not provided, the method\n    emits no events and only returns a final result.\n\n    Event payloads follow a consistent shape:\n    - ``type``: str (\"started\" | \"progress\" | \"warning\" | \"error\" | \"completed\")\n    - ``step``: str logical step identifier\n    - ``message``: str human-readable message\n    - Additional fields depending on the step (counts, status_code, etc.)\n\nReturns\n-------\nDict[str, Any]\n    Structured result payload:\n    - ``ok``: bool\n    - ``status``: str (\"success\" | \"failed\" | \"noop\")\n    - ``results``: List[Dict[str, str]] per-user statuses (name=email, status)\n    - ``source_count``: int number of users retrieved from source\n    - ``eligible_count``: int number of users included in the bulk payload\n    - ``skipped_super_count``: int number of sysadmin users skipped\n    - ``missing_role_mappings_count``: int number of users with unresolved role mapping\n    - ``missing_group_mappings_count``: int number of group memberships not found in target\n    - ``success_count``: int\n    - ``failed_count``: int\n    - ``raw_error``: Any error payload if the bulk request fails, else None\n    - ``warnings``: List[str]",
    "parameters": {
      "type": "object",
      "properties": {
        "emit": {
          "type": "object",
          "description": "Optional callback invoked with structured progress events. If not provided, the method emits no events and only returns a final result. Event payloads follow a consistent shape: - ``type``: str (\"started\" | \"progress\" | \"warning\" | \"error\" | \"completed\") - ``step``: str logical step identifier - ``message``: str human-readable message - Additional fields depending on the step (counts, status_code, etc.)"
        }
      },
      "required": []
    },
    "mutates": true,
    "tags": [
      "migration",
      "users",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "How can I migrate all users from the source environment to the target environment and monitor the progress?",
        "arguments": {
          "emit": {
            "type": "progress",
            "step": "fetch_users",
            "message": "Fetching users from the source environment"
          }
        },
        "notes": "This call migrates all eligible users while providing real-time progress updates through the 'emit' callback. Use this to monitor the migration process step by step."
      },
      {
        "user_query": "I need to migrate users but don't need progress updates. How can I do that?",
        "arguments": {
          "emit": null
        },
        "notes": "This call migrates all eligible users without emitting progress events. Use this for a simpler, synchronous migration when detailed progress tracking is not required."
      },
      {
        "user_query": "Can I migrate users and get warnings or errors if something goes wrong?",
        "arguments": {
          "emit": {
            "type": "warning",
            "step": "mapping_roles",
            "message": "Some roles could not be mapped to the target environment"
          }
        },
        "notes": "This call migrates users and provides structured warnings or errors if issues arise, such as missing role mappings. Use this to identify and address potential problems during migration."
      }
    ]
  },
  {
    "tool_id": "migration.migrate_dashboard_shares",
    "module": "migration",
    "class": "Migration",
    "method": "migrate_dashboard_shares",
    "description": "Migrates shares for specific dashboards from the source to the target environment.",
    "full_doc": "Migrates shares for specific dashboards from the source to the target environment.\n\nParameters:\n    source_dashboard_ids (list): A list of dashboard IDs from the source environment to fetch shares from.\n    target_dashboard_ids (list): A list of dashboard IDs from the target environment to apply shares to.\n    change_ownership (bool, optional): Whether to change ownership of the target dashboard. Defaults to False.\n\nReturns:\n    dict: A summary of the share migration process with counts of succeeded and failed shares,\n        and details of failed dashboards.\n\nRaises:\n    ValueError: If `source_dashboard_ids` or `target_dashboard_ids` are not provided,\n                or if their lengths do not match.",
    "parameters": {
      "type": "object",
      "properties": {
        "source_dashboard_ids": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "A list of dashboard IDs from the source environment to fetch shares from."
        },
        "target_dashboard_ids": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "A list of dashboard IDs from the target environment to apply shares to."
        },
        "change_ownership": {
          "type": "boolean",
          "description": "Whether to change ownership of the target dashboard. Defaults to False."
        }
      },
      "required": [
        "source_dashboard_ids",
        "target_dashboard_ids"
      ]
    },
    "mutates": true,
    "tags": [
      "migration",
      "dashboards",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "How can I migrate shares from dashboards in my old environment to new dashboards in the target environment?",
        "arguments": {
          "source_dashboard_ids": [
            "dashboard_123",
            "dashboard_456"
          ],
          "target_dashboard_ids": [
            "dashboard_789",
            "dashboard_101"
          ],
          "change_ownership": false
        },
        "notes": "This call migrates shares from two dashboards in the source environment to their corresponding dashboards in the target environment without changing ownership."
      },
      {
        "user_query": "I need to transfer shares from dashboards A and B to dashboards X and Y, and also update ownership to match the target environment.",
        "arguments": {
          "source_dashboard_ids": [
            "dashboard_A",
            "dashboard_B"
          ],
          "target_dashboard_ids": [
            "dashboard_X",
            "dashboard_Y"
          ],
          "change_ownership": true
        },
        "notes": "This call migrates shares from two dashboards in the source environment to their counterparts in the target environment and updates ownership of the target dashboards."
      },
      {
        "user_query": "Can I copy shares from a single dashboard in the source environment to a single dashboard in the target environment?",
        "arguments": {
          "source_dashboard_ids": [
            "dashboard_001"
          ],
          "target_dashboard_ids": [
            "dashboard_002"
          ],
          "change_ownership": false
        },
        "notes": "This call migrates shares from one dashboard in the source environment to one dashboard in the target environment without changing ownership."
      }
    ]
  },
  {
    "tool_id": "migration.migrate_dashboards",
    "module": "migration",
    "class": "Migration",
    "method": "migrate_dashboards",
    "description": "Migrate dashboards from the source to the target environment using Sisense bulk import.",
    "full_doc": "Migrate dashboards from the source to the target environment using Sisense bulk import.\n\nThis method exports dashboards from the source and imports them into the target in a single\nbulk request. It then parses the bulk import response to determine which dashboards were\nsucceeded, skipped, and failed. Finally, it optionally migrates shares/ownership for\ndashboards that were actually created in the target (depending on `migrate_share`, `action`).\n\nParameters\n----------\ndashboard_ids : list[str] or None, default None\n    List of dashboard OIDs to migrate. Provide either `dashboard_ids` or `dashboard_names`.\ndashboard_names : list[str] or None, default None\n    List of dashboard titles to migrate. Provide either `dashboard_ids` or `dashboard_names`.\naction : {\"skip\", \"overwrite\", \"duplicate\"} or None, default None\n    Determines how the target handles conflicts.\n    - None: default Sisense behavior (typically skip existing).\n    - \"skip\": skip if exists.\n    - \"overwrite\": overwrite if exists (shares/ownership migration is skipped).\n    - \"duplicate\": create duplicate (shares/ownership migration is skipped).\nrepublish : bool, default False\n    Whether to republish dashboards after import.\nmigrate_share : bool, default False\n    If True, attempts to migrate shares (and optionally ownership) after dashboards are created.\nchange_ownership : bool, default False\n    If True and `migrate_share=True`, attempts to change ownership on the target dashboards.\n\nReturns\n-------\ndict\n    Migration summary with the following keys:\n    - \"succeeded\": list[dict]\n        Each item includes: {\"title\": str, \"source_id\": str | None, \"target_id\": str | None}\n    - \"skipped\": list[dict]\n        Each item includes: {\"title\": str, \"source_id\": str | None, \"target_id\": str | None, \"reason\": str | None}\n    - \"failed\": list[dict]\n        Each item includes: {\"title\": str | None, \"source_id\": str | None, \"reason\": str}\n    - \"meta\": dict\n        Helpful metadata about request-level status.\n\nNotes\n-----\nResponse parsing strategy:\n1) Primary (source of truth): when bulk import returns a success status (typically 201),\nparse the structured response fields:\n- \"succeded\" / \"succeeded\" (Sisense sometimes uses the misspelling)\n- \"skipped\"\n- \"failed\" (often grouped by error category)\n2) Fallbacks (old failsafes): if the response is missing expected fields, not JSON,\nor indicates a request-level error via keys like \"message\" / \"error\", we treat the\nentire batch as failed and attach the best-available reason from:\n- response JSON \"message\" / \"error\" / \"error.message\"\n- response.text (truncated)",
    "parameters": {
      "type": "object",
      "properties": {
        "dashboard_ids": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "List of dashboard OIDs to migrate. Provide either `dashboard_ids` or `dashboard_names`."
        },
        "dashboard_names": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "List of dashboard titles to migrate. Provide either `dashboard_ids` or `dashboard_names`."
        },
        "action": {
          "type": "string",
          "description": "Determines how the target handles conflicts. - None: default Sisense behavior (typically skip existing). - \"skip\": skip if exists. - \"overwrite\": overwrite if exists (shares/ownership migration is skipped). - \"duplicate\": create duplicate (shares/ownership migration is skipped).",
          "enum": [
            "skip",
            "overwrite",
            "duplicate"
          ]
        },
        "republish": {
          "type": "boolean",
          "description": "Whether to republish dashboards after import."
        },
        "migrate_share": {
          "type": "boolean",
          "description": "If True, attempts to migrate shares (and optionally ownership) after dashboards are created."
        },
        "change_ownership": {
          "type": "boolean",
          "description": "If True and `migrate_share=True`, attempts to change ownership on the target dashboards."
        },
        "emit": {
          "type": "string",
          "description": "emit parameter"
        }
      },
      "required": []
    },
    "mutates": true,
    "tags": [
      "migration",
      "dashboards",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "How can I migrate dashboards with IDs 'dashboard1', 'dashboard2' and overwrite any existing ones in the target environment?",
        "arguments": {
          "dashboard_ids": [
            "dashboard1",
            "dashboard2"
          ],
          "action": "overwrite",
          "republish": true,
          "migrate_share": true,
          "change_ownership": true
        },
        "notes": "This call migrates the specified dashboards by their IDs, overwriting any existing dashboards in the target environment. It republishes the dashboards and migrates their shares and ownership."
      },
      {
        "user_query": "I want to migrate dashboards named 'Sales Overview' and 'Marketing Performance' without changing ownership or republishing them.",
        "arguments": {
          "dashboard_names": [
            "Sales Overview",
            "Marketing Performance"
          ],
          "action": "skip",
          "republish": false,
          "migrate_share": false,
          "change_ownership": false
        },
        "notes": "This call migrates dashboards by their names, skipping any that already exist in the target environment. It avoids republishing and does not migrate shares or ownership."
      },
      {
        "user_query": "Can I duplicate dashboards 'dashboard3' and 'dashboard4' in the target environment without migrating shares?",
        "arguments": {
          "dashboard_ids": [
            "dashboard3",
            "dashboard4"
          ],
          "action": "duplicate",
          "republish": false,
          "migrate_share": false,
          "change_ownership": false
        },
        "notes": "This call duplicates the specified dashboards in the target environment without republishing or migrating shares and ownership."
      }
    ]
  },
  {
    "tool_id": "migration.migrate_datamodels",
    "module": "migration",
    "class": "Migration",
    "method": "migrate_datamodels",
    "description": "Migrates specific data models from the source environment to the target environment.",
    "full_doc": "Migrates specific data models from the source environment to the target environment.\n\nParameters\n----------\ndatamodel_ids : list[str] or None, default None\n    A list of data model IDs to migrate. Either `datamodel_ids` or `datamodel_names` must be provided.\ndatamodel_names : list[str] or None, default None\n    A list of data model names to migrate. Either `datamodel_ids` or `datamodel_names` must be provided.\nprovider_connection_map : dict[str, str] or None, default None\n    A dictionary mapping provider names to connection IDs. This allows specifying different connections\n    per provider.\n    Example:\n    {\n        \"Databricks\": \"Connection ID\",\n        \"GoogleBigQuery\": \"Connection ID\"\n    }\ndependencies : list[str] or str or None, default None\n    A list of dependencies to include in the migration. If not provided or if \"all\" is passed, all\n    dependencies are selected by default.\n\n    Possible values:\n    - \"dataSecurity\" (includes both Data Security and Scope Configuration)\n    - \"formulas\" (for Formulas)\n    - \"hierarchies\" (for Drill Hierarchies)\n    - \"perspectives\" (for Perspectives)\n\n    If left blank or set to \"all\", all dependencies are included by default.\nshares : bool, default False\n    Whether to also migrate the data model's shares.\naction : str or None, default None\n    Strategy to handle existing data models in the target environment.\n\n        - \"overwrite\": Attempts to overwrite existing model using its original ID via the datamodelId parameter.\n        If the model is not found in target environment, it will automatically fall back and create the model.\n        - \"duplicate\": Creates a new model by passing a `new_title` to the `newTitle` parameter of the import API.\n        If `new_title` is not provided, the original title will be used with \" (Duplicate)\" appended.\nnew_title : str or None, default None\n    New name for the duplicated data model. Used only when `action='duplicate'`.\nemit : Callable[[dict], None] or None, default None\n    Optional callback invoked with structured progress events. If not provided, the method emits no events\n    and only returns a final result.\n\n    Event payloads follow a consistent shape:\n    - ``type``: str (\"started\" | \"progress\" | \"warning\" | \"error\" | \"completed\")\n    - ``step``: str logical step identifier\n    - ``message``: str human-readable message\n    - Additional fields depending on the step (counts, status_code, etc.)\n\nReturns\n-------\ndict\n    A summary of the migration results containing lists of succeeded, skipped, and failed data models,\n    plus counts/metadata in a dashboard-consistent format.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_ids": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "A list of data model IDs to migrate. Either `datamodel_ids` or `datamodel_names` must be provided."
        },
        "datamodel_names": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "A list of data model names to migrate. Either `datamodel_ids` or `datamodel_names` must be provided."
        },
        "provider_connection_map": {
          "type": "object",
          "description": "A dictionary mapping provider names to connection IDs. This allows specifying different connections per provider. Example: { \"Databricks\": \"Connection ID\", \"GoogleBigQuery\": \"Connection ID\" }"
        },
        "dependencies": {
          "type": "array",
          "items": {
            "type": "string",
            "enum": [
              "dataSecurity",
              "formulas",
              "hierarchies",
              "perspectives"
            ]
          },
          "description": "A list of dependencies to include in the migration. If not provided or if \"all\" is passed, all dependencies are selected by default. Possible values: - \"dataSecurity\" (includes both Data Security and Scope Configuration) - \"formulas\" (for Formulas) - \"hierarchies\" (for Drill Hierarchies) - \"perspectives\" (for Perspectives) If left blank or set to \"all\", all dependencies are included by default."
        },
        "shares": {
          "type": "boolean",
          "description": "Whether to also migrate the data model's shares."
        },
        "action": {
          "type": "string",
          "description": "Strategy to handle existing data models in the target environment. - \"overwrite\": Attempts to overwrite existing model using its original ID via the datamodelId parameter. If the model is not found in target environment, it will automatically fall back and create the model. - \"duplicate\": Creates a new model by passing a `new_title` to the `newTitle` parameter of the import API. If `new_title` is not provided, the original title will be used with \" (Duplicate)\" appended.",
          "enum": [
            "overwrite",
            "duplicate"
          ]
        },
        "new_title": {
          "type": "string",
          "description": "New name for the duplicated data model. Used only when `action='duplicate'`."
        },
        "emit": {
          "type": "object",
          "description": "Optional callback invoked with structured progress events. If not provided, the method emits no events and only returns a final result. Event payloads follow a consistent shape: - ``type``: str (\"started\" | \"progress\" | \"warning\" | \"error\" | \"completed\") - ``step``: str logical step identifier - ``message``: str human-readable message - Additional fields depending on the step (counts, status_code, etc.)"
        }
      },
      "required": []
    },
    "mutates": true,
    "tags": [
      "migration",
      "datamodel",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "Migrate the data models with IDs 'model_123' and 'model_456' to the target environment, including all dependencies and shares.",
        "arguments": {
          "datamodel_ids": [
            "model_123",
            "model_456"
          ],
          "datamodel_names": null,
          "provider_connection_map": null,
          "dependencies": [
            "dataSecurity",
            "formulas",
            "hierarchies",
            "perspectives"
          ],
          "shares": true,
          "action": "overwrite",
          "new_title": null,
          "emit": null
        },
        "notes": "This call migrates two specific data models by their IDs, including all dependencies and shares, and overwrites any existing models in the target environment."
      },
      {
        "user_query": "Duplicate the data model named 'Sales_Data' to the target environment with a new title 'Sales_Data_Copy', including only formulas and hierarchies.",
        "arguments": {
          "datamodel_ids": null,
          "datamodel_names": [
            "Sales_Data"
          ],
          "provider_connection_map": null,
          "dependencies": [
            "formulas",
            "hierarchies"
          ],
          "shares": false,
          "action": "duplicate",
          "new_title": "Sales_Data_Copy",
          "emit": null
        },
        "notes": "This call duplicates a data model by its name, includes only formulas and hierarchies, and assigns a new title to the duplicated model in the target environment."
      },
      {
        "user_query": "Migrate data models with IDs 'model_789' and 'model_101' using specific provider connections, including all dependencies but excluding shares.",
        "arguments": {
          "datamodel_ids": [
            "model_789",
            "model_101"
          ],
          "datamodel_names": null,
          "provider_connection_map": {
            "Databricks": "conn_001",
            "GoogleBigQuery": "conn_002"
          },
          "dependencies": [
            "dataSecurity",
            "formulas",
            "hierarchies",
            "perspectives"
          ],
          "shares": false,
          "action": "overwrite",
          "new_title": null,
          "emit": null
        },
        "notes": "This call migrates two specific data models by their IDs, uses custom provider connections, includes all dependencies, and excludes shares. Existing models in the target environment will be overwritten."
      }
    ]
  },
  {
    "tool_id": "migration.migrate_groups",
    "module": "migration",
    "class": "Migration",
    "method": "migrate_groups",
    "description": "Migrates specific groups from the source environment to the target environment using the bulk endpoint.",
    "full_doc": "Migrates specific groups from the source environment to the target environment using the bulk endpoint.\n\nParameters:\n    group_name_list (list): A list of group names to migrate.\n\nReturns:\n    list: A list of group migration results, including any errors encountered during the process.",
    "parameters": {
      "type": "object",
      "properties": {
        "group_name_list": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "A list of group names to migrate."
        }
      },
      "required": [
        "group_name_list"
      ]
    },
    "mutates": true,
    "tags": [
      "migration",
      "groups",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "How can I migrate the 'Sales Team' and 'Marketing Team' groups to the target environment?",
        "arguments": {
          "group_name_list": [
            "Sales Team",
            "Marketing Team"
          ]
        },
        "notes": "This call migrates the 'Sales Team' and 'Marketing Team' groups from the source environment to the target environment. Use this when you need to transfer specific groups."
      },
      {
        "user_query": "I need to migrate the 'Admins' group to the new environment. How can I do that?",
        "arguments": {
          "group_name_list": [
            "Admins"
          ]
        },
        "notes": "This call migrates the 'Admins' group to the target environment. Use this when you want to migrate a single group."
      },
      {
        "user_query": "Can I migrate multiple groups like 'HR', 'Finance', and 'IT' at once?",
        "arguments": {
          "group_name_list": [
            "HR",
            "Finance",
            "IT"
          ]
        },
        "notes": "This call migrates the 'HR', 'Finance', and 'IT' groups to the target environment in a single operation. Use this for bulk group migrations."
      }
    ]
  },
  {
    "tool_id": "migration.migrate_users",
    "module": "migration",
    "class": "Migration",
    "method": "migrate_users",
    "description": "Migrates specific users from the source environment to the target environment.",
    "full_doc": "Migrates specific users from the source environment to the target environment.\n\nParameters:\n    user_name_list (list): A list of user names to migrate.\n\nReturns:\n    list: A list of user migration results, including any errors encountered during the process.",
    "parameters": {
      "type": "object",
      "properties": {
        "user_name_list": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "A list of user names to migrate."
        }
      },
      "required": [
        "user_name_list"
      ]
    },
    "mutates": true,
    "tags": [
      "migration",
      "users",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "Migrate the users 'john_doe' and 'jane_smith' from the old environment to the new one.",
        "arguments": {
          "user_name_list": [
            "john_doe",
            "jane_smith"
          ]
        },
        "notes": "This call migrates the specified users from the source environment to the target environment. Use this when you need to transfer specific user accounts."
      },
      {
        "user_query": "Move the users 'admin_user', 'data_analyst', and 'project_manager' to the new Sisense environment.",
        "arguments": {
          "user_name_list": [
            "admin_user",
            "data_analyst",
            "project_manager"
          ]
        },
        "notes": "Use this call to migrate multiple users at once, ensuring their accounts are transferred to the target environment."
      },
      {
        "user_query": "Transfer the user 'team_lead' to the new environment for access to updated dashboards.",
        "arguments": {
          "user_name_list": [
            "team_lead"
          ]
        },
        "notes": "This call is useful for migrating individual users who need immediate access to the target environment."
      }
    ]
  },
  {
    "tool_id": "wellcheck.check_dashboard_structure",
    "module": "wellcheck",
    "class": "WellCheck",
    "method": "check_dashboard_structure",
    "description": "Analyze the structure of one or more dashboards.",
    "full_doc": "Analyze the structure of one or more dashboards.\n\nThis method:\n  - Counts pivot widgets (pivot / pivot2)\n  - Counts tabber widgets (WidgetsTabber)\n  - Counts accordion widgets via accordionConfig on widgets\n  - Counts jump-to-dashboard (JTD) instances, treated as child dashboards\n\nA \"child dashboard\" is treated as any dashboard referenced as a\njump-to-dashboard target from the parent dashboard, either via\nwidget options (drillTarget) or via dashboard script\n(prism.jumpToDashboard calls).\n\nThis method is intended to back wellcheck tasks and agentic tools that\nrespond to prompts such as:\n  - \"check child dashboards for this dashboard\"\n  - \"check jump-to dashboards on XYZ\"\n  - \"analyze dashboard structure / complexity\"\n\nParameters\n----------\ndashboards : list of str, optional\n    One or more dashboard references to analyze. Each reference can be:\n      - a Sisense dashboard ID, or\n      - a dashboard title (name).\n    At least one dashboard reference is required. At runtime this\n    method is tolerant of a single string being passed instead of a\n    list, and will normalize it to a one-element list.\n\nReturns\n-------\nlist of dict\n    A list with one entry per successfully processed dashboard. Each\n    entry has the keys:\n      - dashboard_id (str): Resolved dashboard ID.\n      - dashboard_title (str): Resolved dashboard title.\n      - pivot_count (int): Number of pivot/pivot2 widgets.\n      - tabber_count (int): Number of WidgetsTabber widgets.\n      - accordion_count (int): Number of accordion widgets detected\n        via accordionConfig on widgets.\n      - jtd_count (int): Number of jump-to-dashboard (JTD) instances\n        (child dashboards) detected in widget options and scripts.\n    If no dashboards are successfully processed, an empty list is\n    returned and details are available in the logs.",
    "parameters": {
      "type": "object",
      "properties": {
        "dashboards": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "One or more dashboard references to analyze. Each reference can be: - a Sisense dashboard ID, or - a dashboard title (name). At least one dashboard reference is required. At runtime this method is tolerant of a single string being passed instead of a list, and will normalize it to a one-element list."
        }
      },
      "required": []
    },
    "mutates": false,
    "tags": [
      "wellcheck",
      "dashboards",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "Analyze the structure of the dashboard titled 'Sales Overview'.",
        "arguments": {
          "dashboards": [
            "Sales Overview"
          ]
        },
        "notes": "This call analyzes the structure of the 'Sales Overview' dashboard, counting widget types and identifying child dashboards."
      },
      {
        "user_query": "Check the complexity of multiple dashboards: 'Marketing Performance' and 'Customer Insights'.",
        "arguments": {
          "dashboards": [
            "Marketing Performance",
            "Customer Insights"
          ]
        },
        "notes": "This call processes two dashboards, providing a breakdown of widget counts and jump-to-dashboard references for each."
      },
      {
        "user_query": "Analyze the dashboard with ID 'dashboard_12345'.",
        "arguments": {
          "dashboards": [
            "dashboard_12345"
          ]
        },
        "notes": "This call analyzes the structure of the dashboard identified by the ID 'dashboard_12345', useful for cases where the ID is known but the title is not."
      }
    ]
  },
  {
    "tool_id": "wellcheck.check_dashboard_widget_counts",
    "module": "wellcheck",
    "class": "WellCheck",
    "method": "check_dashboard_widget_counts",
    "description": "Compute widget counts for one or more dashboards.",
    "full_doc": "Compute widget counts for one or more dashboards.\n\nThis method retrieves each specified dashboard definition, counts the\nnumber of widgets on that dashboard, and returns a per-dashboard\nsummary.\n\nParameters\n----------\ndashboards : list of str, optional\n    One or more dashboard references to analyze. Each reference can be:\n      - a Sisense dashboard ID, or\n      - a dashboard title (name).\n    At least one dashboard reference is required. At runtime this\n    method is tolerant of a single string being passed instead of a\n    list, and will normalize it to a one-element list.\n\nReturns\n-------\nlist of dict\n    A list with one entry per successfully processed dashboard. Each\n    entry contains:\n      - dashboard_id (str): Resolved dashboard ID.\n      - dashboard_title (str): Resolved dashboard title.\n      - widget_count (int): Number of widgets on the dashboard.\n\n    If no dashboards are successfully processed, an empty list is\n    returned and details are available in the logs.",
    "parameters": {
      "type": "object",
      "properties": {
        "dashboards": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "One or more dashboard references to analyze. Each reference can be: - a Sisense dashboard ID, or - a dashboard title (name). At least one dashboard reference is required. At runtime this method is tolerant of a single string being passed instead of a list, and will normalize it to a one-element list."
        }
      },
      "required": []
    },
    "mutates": false,
    "tags": [
      "wellcheck",
      "dashboards",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "How many widgets are on the dashboards named 'Sales Overview' and 'Marketing Performance'?",
        "arguments": {
          "dashboards": [
            "Sales Overview",
            "Marketing Performance"
          ]
        },
        "notes": "This call analyzes the widget counts for two dashboards by their titles. Use this when you want to check widget counts for dashboards you know by name."
      },
      {
        "user_query": "Can you check the widget count for the dashboard with ID 'dashboard_12345'?",
        "arguments": {
          "dashboards": [
            "dashboard_12345"
          ]
        },
        "notes": "This call retrieves the widget count for a single dashboard using its Sisense ID. Use this when you have the dashboard ID available."
      },
      {
        "user_query": "I need a summary of widget counts for dashboards 'dashboard_67890' and 'dashboard_54321'.",
        "arguments": {
          "dashboards": [
            "dashboard_67890",
            "dashboard_54321"
          ]
        },
        "notes": "This call computes widget counts for two dashboards identified by their Sisense IDs. Use this when you need to analyze multiple dashboards by ID."
      }
    ]
  },
  {
    "tool_id": "wellcheck.check_datamodel_custom_tables",
    "module": "wellcheck",
    "class": "WellCheck",
    "method": "check_datamodel_custom_tables",
    "description": "Inspect custom tables in one or more data models and flag the use of UNION.",
    "full_doc": "Inspect custom tables in one or more data models and flag the use of UNION.\n\nThis method resolves each data model reference (ID or title), retrieves\nits schema from the Sisense API, iterates through all datasets/tables,\nand returns one row per custom table with a flag indicating whether its\nSQL expression contains the word \"union\" (case-insensitive).\n\nParameters\n----------\ndatamodels : list of str, optional\n    One or more data model references to analyze. Each reference can be:\n      - a Sisense data model ID, or\n      - a data model title (name).\n    At least one data model reference is required. At runtime this\n    method is tolerant of a single string being passed instead of a\n    list, and will normalize it to a one-element list.\n\nReturns\n-------\nlist of dict\n    A list with one entry per custom table. Each entry contains:\n      - data_model (str): Data model title.\n      - table (str): Table name.\n      - has_union (str): \"yes\" if the custom table expression contains\n        \"union\" (case-insensitive), otherwise \"no\".\n\n    If no data models are successfully processed, an empty list is\n    returned and details are available in the logs.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodels": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "One or more data model references to analyze. Each reference can be: - a Sisense data model ID, or - a data model title (name). At least one data model reference is required. At runtime this method is tolerant of a single string being passed instead of a list, and will normalize it to a one-element list."
        }
      },
      "required": []
    },
    "mutates": false,
    "tags": [
      "wellcheck",
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "Check if any custom tables in the 'Sales Dashboard' data model use UNION.",
        "arguments": {
          "datamodels": [
            "Sales Dashboard"
          ]
        },
        "notes": "This call analyzes the 'Sales Dashboard' data model to identify custom tables with SQL expressions containing the word 'UNION'."
      },
      {
        "user_query": "Inspect the custom tables in multiple data models, including 'Marketing Analytics' and '12345'.",
        "arguments": {
          "datamodels": [
            "Marketing Analytics",
            "12345"
          ]
        },
        "notes": "This call checks the custom tables in the 'Marketing Analytics' data model and the data model with ID '12345' for the presence of UNION in their SQL expressions."
      },
      {
        "user_query": "Analyze the data model with ID '67890' for any custom tables using UNION.",
        "arguments": {
          "datamodels": [
            "67890"
          ]
        },
        "notes": "This call inspects the data model identified by the ID '67890' to flag any custom tables that use UNION in their SQL expressions."
      }
    ]
  },
  {
    "tool_id": "wellcheck.check_datamodel_import_queries",
    "module": "wellcheck",
    "class": "WellCheck",
    "method": "check_datamodel_import_queries",
    "description": "Inspect tables in one or more data models for import queries.",
    "full_doc": "Inspect tables in one or more data models for import queries.\n\nThis method resolves each data model reference (ID or title), loads its\nschema, and checks every table for a ``configOptions.importQuery``\nconfiguration. For each table, it returns a row indicating whether an\nimport query is configured.\n\nParameters\n----------\ndatamodels : list of str, optional\n    One or more data model references to analyze. Each reference can be:\n      - a Sisense data model ID, or\n      - a data model title (name).\n    At least one data model reference is required. At runtime this\n    method is tolerant of a single string being passed instead of a\n    list, and will normalize it to a one-element list.\n\nReturns\n-------\nlist of dict\n    A list with one entry per table across all successfully processed\n    data models. Each entry contains:\n      - data_model (str): Resolved data model title.\n      - table (str): Table name.\n      - has_import_query (str): \"yes\" if an importQuery is present in\n        the table config options, otherwise \"no\".\n\n    If no data models are successfully processed, an empty list is\n    returned and details are available in the logs.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodels": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "One or more data model references to analyze. Each reference can be: - a Sisense data model ID, or - a data model title (name). At least one data model reference is required. At runtime this method is tolerant of a single string being passed instead of a list, and will normalize it to a one-element list."
        }
      },
      "required": []
    },
    "mutates": false,
    "tags": [
      "wellcheck",
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "Check if tables in the 'Sales Analytics' data model have import queries configured.",
        "arguments": {
          "datamodels": [
            "Sales Analytics"
          ]
        },
        "notes": "This call inspects all tables in the 'Sales Analytics' data model by its title to determine if import queries are configured."
      },
      {
        "user_query": "Analyze tables in multiple data models, including 'Marketing Insights' and '12345', for import query configurations.",
        "arguments": {
          "datamodels": [
            "Marketing Insights",
            "12345"
          ]
        },
        "notes": "This call checks tables across two data models: one referenced by title ('Marketing Insights') and another by ID ('12345')."
      },
      {
        "user_query": "Verify import query configurations for the data model with ID '67890'.",
        "arguments": {
          "datamodels": [
            "67890"
          ]
        },
        "notes": "This call inspects all tables in the data model identified by ID '67890' to check for import query configurations."
      }
    ]
  },
  {
    "tool_id": "wellcheck.check_datamodel_island_tables",
    "module": "wellcheck",
    "class": "WellCheck",
    "method": "check_datamodel_island_tables",
    "description": "Identify island tables (tables with no relationships) in one or more data models.",
    "full_doc": "Identify island tables (tables with no relationships) in one or more data models.\n\nThis method retrieves the schema for each specified data model, inspects\nits relations and tables, and returns information about tables that do\nnot participate in any relationship (often called \"island tables\").\n\nParameters\n----------\ndatamodels : list of str, optional\n    One or more data model references to analyze. Each reference can be:\n      - a Sisense data model ID, or\n      - a data model title (name).\n    At least one data model reference is required. At runtime this\n    method is tolerant of a single string being passed instead of a\n    list, and will normalize it to a one-element list.\n\nReturns\n-------\nlist of dict\n    A list with one entry per island table. Each entry contains:\n      - datamodel (str): Data model title.\n      - datamodel_oid (str): Data model ID.\n      - table (str): Table name.\n      - table_oid (str): Table ID.\n      - type (str): Table type (e.g., 'live', 'custom').\n      - relation (str): Always \"no\" for island tables.\n\n    If no data models are successfully processed, an empty list is\n    returned and details are available in the logs.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodels": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "One or more data model references to analyze. Each reference can be: - a Sisense data model ID, or - a data model title (name). At least one data model reference is required. At runtime this method is tolerant of a single string being passed instead of a list, and will normalize it to a one-element list."
        }
      },
      "required": []
    },
    "mutates": false,
    "tags": [
      "wellcheck",
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "Which tables in the 'Sales Analytics' data model are island tables?",
        "arguments": {
          "datamodels": [
            "Sales Analytics"
          ]
        },
        "notes": "This call checks the 'Sales Analytics' data model for tables that have no relationships with other tables."
      },
      {
        "user_query": "Find island tables in the data models with IDs 'dm_12345' and 'dm_67890'.",
        "arguments": {
          "datamodels": [
            "dm_12345",
            "dm_67890"
          ]
        },
        "notes": "This call analyzes two data models, identified by their IDs, to find tables that do not participate in any relationships."
      },
      {
        "user_query": "Are there any island tables in the 'Marketing Dashboard' data model?",
        "arguments": {
          "datamodels": [
            "Marketing Dashboard"
          ]
        },
        "notes": "This call inspects the 'Marketing Dashboard' data model to identify any tables that are not connected to others via relationships."
      }
    ]
  },
  {
    "tool_id": "wellcheck.check_datamodel_m2m_relationships",
    "module": "wellcheck",
    "class": "WellCheck",
    "method": "check_datamodel_m2m_relationships",
    "description": "Check for potential many-to-many (M2M) relationships between tables",
    "full_doc": "Check for potential many-to-many (M2M) relationships between tables\nin one or more data models.\n\nFor each data model, this method inspects the relation graph, builds\ntable/column pairs from the relations, and runs aggregate SQL queries\nagainst the data source to detect whether both sides of the relation\ncontain duplicate keys. Pairs where each side has more than one\noccurrence of its key are flagged as many-to-many.\n\nParameters\n----------\ndatamodels : list of str, optional\n    One or more data model references to analyze. Each reference can be:\n      - a Sisense data model ID, or\n      - a data model title (name).\n    At least one data model reference is required. At runtime this\n    method is tolerant of a single string being passed instead of a\n    list, and will normalize it to a one-element list.\n\nReturns\n-------\nlist of dict\n    A list with one entry per relation field pair checked. Each entry\n    contains:\n      - data_model (str): Data model title.\n      - left_table (str): Name of the left table.\n      - left_column (str): Name of the left column.\n      - right_table (str): Name of the right table.\n      - right_column (str): Name of the right column.\n      - is_m2m (bool): True when both sides have more than one\n        occurrence of their key, False otherwise.\n\n    If no data models are successfully processed, an empty list is\n    returned and details are available in the logs.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodels": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "One or more data model references to analyze. Each reference can be: - a Sisense data model ID, or - a data model title (name). At least one data model reference is required. At runtime this method is tolerant of a single string being passed instead of a list, and will normalize it to a one-element list."
        }
      },
      "required": []
    },
    "mutates": false,
    "tags": [
      "wellcheck",
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "Check for many-to-many relationships in the 'Sales Data' data model.",
        "arguments": {
          "datamodels": [
            "Sales Data"
          ]
        },
        "notes": "This call analyzes the 'Sales Data' data model to identify potential many-to-many relationships between tables."
      },
      {
        "user_query": "Analyze multiple data models, 'Finance Data' and 'HR Analytics', for M2M relationships.",
        "arguments": {
          "datamodels": [
            "Finance Data",
            "HR Analytics"
          ]
        },
        "notes": "This call checks both the 'Finance Data' and 'HR Analytics' data models for many-to-many relationships, returning detailed results for each relation pair."
      },
      {
        "user_query": "Inspect the data model with ID 'dm_12345' for any many-to-many relationships.",
        "arguments": {
          "datamodels": [
            "dm_12345"
          ]
        },
        "notes": "This call uses the data model ID 'dm_12345' to analyze its relationships and detect any many-to-many issues."
      }
    ]
  },
  {
    "tool_id": "wellcheck.check_datamodel_rls_datatypes",
    "module": "wellcheck",
    "class": "WellCheck",
    "method": "check_datamodel_rls_datatypes",
    "description": "Inspect row-level security (RLS) rules for one or more data models and",
    "full_doc": "Inspect row-level security (RLS) rules for one or more data models and\nreport the datatype of the columns used in those rules.\n\nThis method resolves each data model reference, fetches its RLS (data\nsecurity) rules from the appropriate API endpoint based on the data\nmodel type (extract or live), and returns one row per unique\n(datamodel, table, column, datatype) combination.\n\nParameters\n----------\ndatamodels : list of str, optional\n    One or more data model references to analyze. Each reference can be:\n      - a Sisense data model ID, or\n      - a data model title (name).\n    At least one data model reference is required. At runtime this\n    method is tolerant of a single string being passed instead of a\n    list, and will normalize it to a one-element list.\n\nReturns\n-------\nlist of dict\n    A list with one entry per unique RLS column. Each entry contains:\n      - datamodel (str): Data model title.\n      - table (str): Table name where RLS is applied.\n      - column (str): Column name used in the RLS rule.\n      - datatype (str): Datatype reported by Sisense for that column.\n\n    If no data models are successfully processed, an empty list is\n    returned and details are available in the logs.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodels": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "One or more data model references to analyze. Each reference can be: - a Sisense data model ID, or - a data model title (name). At least one data model reference is required. At runtime this method is tolerant of a single string being passed instead of a list, and will normalize it to a one-element list."
        }
      },
      "required": []
    },
    "mutates": false,
    "tags": [
      "wellcheck",
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "What are the datatypes of the columns used in RLS rules for the 'Sales Data' model?",
        "arguments": {
          "datamodels": [
            "Sales Data"
          ]
        },
        "notes": "This call inspects the RLS rules for the 'Sales Data' model and returns the datatype of each column used in those rules."
      },
      {
        "user_query": "Can you check the RLS column datatypes for the models with IDs 'model_12345' and 'model_67890'?",
        "arguments": {
          "datamodels": [
            "model_12345",
            "model_67890"
          ]
        },
        "notes": "This call analyzes the RLS rules for two data models identified by their IDs and returns the datatype of each column used in those rules."
      },
      {
        "user_query": "I need to verify the RLS column datatypes for the 'Customer Analytics' model.",
        "arguments": {
          "datamodels": [
            "Customer Analytics"
          ]
        },
        "notes": "This call inspects the RLS rules for the 'Customer Analytics' model and provides the datatype of each column used in those rules."
      }
    ]
  },
  {
    "tool_id": "wellcheck.check_pivot_widget_fields",
    "module": "wellcheck",
    "class": "WellCheck",
    "method": "check_pivot_widget_fields",
    "description": "Analyze pivot widgets on one or more dashboards and report those with many fields.",
    "full_doc": "Analyze pivot widgets on one or more dashboards and report those with many fields.\n\nThis method retrieves each specified dashboard, scans all pivot widgets\n(types containing ``\"pivot\"`` or ``\"pivot2\"``), counts how many fields\n(items) are attached to those widgets, and returns a per-widget summary\nfor any pivot with more than ``max_fields`` fields.\n\nParameters\n----------\ndashboards : list of str, optional\n    One or more dashboard references to analyze. Each reference can be:\n      - a Sisense dashboard ID, or\n      - a dashboard title (name).\n    At least one dashboard reference is required. At runtime this\n    method is tolerant of a single string being passed instead of a\n    list, and will normalize it to a one-element list.\nmax_fields : int, optional\n    Threshold for the number of fields on a pivot widget. Only pivot\n    widgets with more than this number of fields are included in the\n    returned data. Defaults to 20.\n\nReturns\n-------\nlist of dict\n    A list of dictionaries, each describing a pivot widget that exceeds\n    the configured field threshold. Each entry contains:\n      - dashboard_id (str): Resolved dashboard ID.\n      - dashboard_title (str): Resolved dashboard title.\n      - widget_id (str): Pivot widget ID.\n      - has_more_fields (bool): Always True for returned rows, since\n        only widgets above the threshold are included.\n      - field_count (int): Total number of fields (items) in the widget.\n\n    If no dashboards are successfully processed, or no pivot widgets\n    exceed the threshold, an empty list is returned and details are\n    available in the logs.",
    "parameters": {
      "type": "object",
      "properties": {
        "dashboards": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "One or more dashboard references to analyze. Each reference can be: - a Sisense dashboard ID, or - a dashboard title (name). At least one dashboard reference is required. At runtime this method is tolerant of a single string being passed instead of a list, and will normalize it to a one-element list."
        },
        "max_fields": {
          "type": "integer",
          "description": "Threshold for the number of fields on a pivot widget. Only pivot widgets with more than this number of fields are included in the returned data. Defaults to 20."
        }
      },
      "required": []
    },
    "mutates": false,
    "tags": [
      "wellcheck",
      "read"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "Find pivot widgets with too many fields on the Sales and Marketing dashboards.",
        "arguments": {
          "dashboards": [
            "Sales Dashboard",
            "Marketing Dashboard"
          ],
          "max_fields": 25
        },
        "notes": "This call analyzes the 'Sales Dashboard' and 'Marketing Dashboard' to identify pivot widgets with more than 25 fields."
      },
      {
        "user_query": "Check all pivot widgets on the dashboard with ID '12345' and report those exceeding 30 fields.",
        "arguments": {
          "dashboards": [
            "12345"
          ],
          "max_fields": 30
        },
        "notes": "This call targets a specific dashboard by its ID ('12345') and identifies pivot widgets with more than 30 fields."
      },
      {
        "user_query": "Analyze the Operations dashboard for pivot widgets with more than 20 fields.",
        "arguments": {
          "dashboards": [
            "Operations Dashboard"
          ]
        },
        "notes": "This call uses the default threshold of 20 fields to analyze pivot widgets on the 'Operations Dashboard'."
      }
    ]
  },
  {
    "tool_id": "wellcheck.run_full_wellcheck",
    "module": "wellcheck",
    "class": "WellCheck",
    "method": "run_full_wellcheck",
    "description": "Run a composite \"full\" wellcheck across dashboards and data models.",
    "full_doc": "Run a composite \"full\" wellcheck across dashboards and data models.\n\nThis method is a convenience wrapper that orchestrates multiple\ndashboard-level and data-model-level checks and returns a structured\nreport that groups their results.\n\nParameters\n----------\ndashboards : str or list of str, optional\n    One or more dashboard references to analyze. Each reference can be:\n      - a Sisense dashboard ID, or\n      - a dashboard title (name).\n    At runtime this parameter is tolerant of a single string and will\n    normalize it to a one-element list.\ndatamodels : str or list of str, optional\n    One or more data model references to analyze. Each reference can be:\n      - a data model ID, or\n      - a data model title (name).\n    At runtime this parameter is tolerant of a single string and will\n    normalize it to a one-element list.\nmax_pivot_fields : int, optional\n    Threshold used by the pivot-fields check. Any pivot widget with\n    more than this number of fields is flagged.\n\nReturns\n-------\ndict\n    A dictionary with two top-level sections:\n\n    - \"dashboards\": {\n          \"structure\": [...],\n          \"widget_counts\": [...],\n          \"pivot_widget_fields\": [...],\n      }\n\n    - \"datamodels\": {\n          \"custom_tables\": [...],\n          \"island_tables\": [...],\n          \"rls_datatypes\": [...],\n          \"import_queries\": [...],\n          \"m2m_relationships\": [...],\n          \"unused_columns\": [...],\n      }\n\n    Each subsection contains the list of rows returned by the\n    corresponding check method. If a given set of references is not\n    provided or no assets are successfully processed, that subsection\n    will be an empty list.",
    "parameters": {
      "type": "object",
      "properties": {
        "dashboards": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "One or more dashboard references to analyze. Each reference can be: - a Sisense dashboard ID, or - a dashboard title (name). At runtime this parameter is tolerant of a single string and will normalize it to a one-element list."
        },
        "datamodels": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "One or more data model references to analyze. Each reference can be: - a data model ID, or - a data model title (name). At runtime this parameter is tolerant of a single string and will normalize it to a one-element list."
        },
        "max_pivot_fields": {
          "type": "integer",
          "description": "Threshold used by the pivot-fields check. Any pivot widget with more than this number of fields is flagged."
        }
      },
      "required": []
    },
    "mutates": true,
    "tags": [
      "wellcheck",
      "write"
    ],
    "sdk_version": "0.2.1",
    "updated_at": "2026-01-02T21:28:19Z",
    "examples": [
      {
        "user_query": "Analyze all dashboards related to sales performance and flag pivot widgets with more than 5 fields.",
        "arguments": {
          "dashboards": [
            "Sales Dashboard",
            "Regional Sales Overview"
          ],
          "datamodels": [],
          "max_pivot_fields": 5
        },
        "notes": "This call runs a full wellcheck on two sales-related dashboards, focusing on identifying pivot widgets with excessive fields."
      },
      {
        "user_query": "Check the data models 'Finance_Model' and 'HR_Model' for unused columns and other issues.",
        "arguments": {
          "dashboards": [],
          "datamodels": [
            "Finance_Model",
            "HR_Model"
          ],
          "max_pivot_fields": null
        },
        "notes": "This call targets two specific data models to identify structural issues, such as unused columns or problematic relationships."
      },
      {
        "user_query": "Perform a comprehensive analysis on the 'Executive Dashboard' and 'Marketing_Model' with a pivot field threshold of 10.",
        "arguments": {
          "dashboards": [
            "Executive Dashboard"
          ],
          "datamodels": [
            "Marketing_Model"
          ],
          "max_pivot_fields": 10
        },
        "notes": "This call combines dashboard and data model checks, focusing on the 'Executive Dashboard' and 'Marketing_Model' while flagging pivot widgets with more than 10 fields."
      }
    ]
  }
]