[
  {
    "tool_id": "access.change_folder_and_dashboard_ownership",
    "module": "access",
    "class": "AccessManagement",
    "method": "change_folder_and_dashboard_ownership",
    "description": "Method to change the ownership of folders and optionally dashboards.",
    "full_doc": "Method to change the ownership of folders and optionally dashboards.\nThis method changes the ownership of a target folder and the entire\ntree structure surrounding it, including subfolders, sibling folders,\nand parent folders.\nOptionally, it will also change the ownership of dashboards associated\nwith these folders.\n\nParameters:\n    user_name (str): The user running the tool. This is necessary for\n    API access checks.\n    folder_name (str): The target folder whose ownership needs to be\n    changed.\n    new_owner_name (str): The new owner to whom the folder (and\n    optionally dashboards) ownership will be transferred.\n    original_owner_rule (str, optional): Specifies the ownership rule\n    to set original owner after changing ownership('edit' or 'view').\n    Default is 'edit'.\n    change_dashboard_ownership (bool, optional): Specifies whether to\n    also change the ownership of dashboards in the folder tree. Default\n    is True.",
    "parameters": {
      "type": "object",
      "properties": {
        "executing_user": {
          "type": "string",
          "description": "executing_user parameter"
        },
        "folder_name": {
          "type": "string",
          "description": "The target folder whose ownership needs to be changed."
        },
        "new_owner_name": {
          "type": "string",
          "description": "The new owner to whom the folder (and optionally dashboards) ownership will be transferred."
        },
        "original_owner_rule": {
          "type": "string",
          "description": "Specifies the ownership rule to set original owner after changing ownership('edit' or 'view'). Default is 'edit'."
        },
        "change_dashboard_ownership": {
          "type": "boolean",
          "description": "Specifies whether to also change the ownership of dashboards in the folder tree. Default is True."
        }
      },
      "required": [
        "executing_user",
        "folder_name",
        "new_owner_name"
      ]
    },
    "mutates": true,
    "tags": [
      "access",
      "dashboards",
      "write"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "Transfer ownership of the 'Sales Reports' folder to the new team lead, including all dashboards inside it.",
        "arguments": {
          "executing_user": "admin@sisense.com",
          "folder_name": "Sales Reports",
          "new_owner_name": "teamlead@sisense.com",
          "original_owner_rule": "edit",
          "change_dashboard_ownership": true
        },
        "notes": "This call transfers ownership of the 'Sales Reports' folder and all associated dashboards to the new team lead. The original owner will retain 'edit' permissions."
      },
      {
        "user_query": "Change ownership of the 'Marketing Data' folder to a new owner but keep the dashboards under the original owner.",
        "arguments": {
          "executing_user": "admin@sisense.com",
          "folder_name": "Marketing Data",
          "new_owner_name": "marketing.manager@sisense.com",
          "original_owner_rule": "view",
          "change_dashboard_ownership": false
        },
        "notes": "This call transfers ownership of the 'Marketing Data' folder to the marketing manager but leaves the dashboards under the original owner's control with 'view' permissions."
      },
      {
        "user_query": "Reassign the 'Q3 Financials' folder and its dashboards to the CFO, while keeping 'edit' permissions for the original owner.",
        "arguments": {
          "executing_user": "admin@sisense.com",
          "folder_name": "Q3 Financials",
          "new_owner_name": "cfo@sisense.com",
          "original_owner_rule": "edit",
          "change_dashboard_ownership": true
        },
        "notes": "This call transfers ownership of the 'Q3 Financials' folder and its dashboards to the CFO, while ensuring the original owner retains 'edit' permissions."
      }
    ]
  },
  {
    "tool_id": "access.create_schedule_build",
    "module": "access",
    "class": "AccessManagement",
    "method": "create_schedule_build",
    "description": "Method to create a schedule build for a DataModel.",
    "full_doc": "Method to create a schedule build for a DataModel.\n\nSupports both cron-based schedules (e.g., every Monday at 9:00 UTC)\nand interval-based schedules (e.g., every 2 days, 1 hour, 30 minutes).\n\nParameters:\n    datamodel_name (str): The name of the DataModel.\n    build_type (str): Optional. Type of the build (e.g., \"ACCUMULATE\", \"FULL\",\n    \"SCHEMA_CHANGES\"). Defaults to \"ACCUMULATE\".\n    days (list, optional): List of days for cron schedule. Eg.: [\"SUN\", \"MON\", \"TUE\", \"WED\", \"THU\", \"FRI\",\n    \"SAT\"] or [\"*\"] for all days.\n    hour (int, optional): Hour in 24-hour format (UTC).\n    minute (int, optional): Minute of the hour (UTC).\n    interval_days (int, optional): Interval in days.\n    interval_hours (int, optional): Interval in hours.\n    interval_minutes (int, optional): Interval in minutes.\n\nReturns:\n    dict: API response or error.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "The name of the DataModel."
        },
        "build_type": {
          "type": "string",
          "description": "Optional. Type of the build (e.g., \"ACCUMULATE\", \"FULL\", \"SCHEMA_CHANGES\"). Defaults to \"ACCUMULATE\"."
        },
        "days": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "List of days for cron schedule. Eg.: [\"SUN\", \"MON\", \"TUE\", \"WED\", \"THU\", \"FRI\", \"SAT\"] or [\"*\"] for all days."
        },
        "hour": {
          "type": "integer",
          "description": "Hour in 24-hour format (UTC)."
        },
        "minute": {
          "type": "integer",
          "description": "Minute of the hour (UTC)."
        },
        "interval_days": {
          "type": "integer",
          "description": "Interval in days."
        },
        "interval_hours": {
          "type": "integer",
          "description": "Interval in hours."
        },
        "interval_minutes": {
          "type": "integer",
          "description": "Interval in minutes."
        }
      },
      "required": [
        "datamodel_name"
      ]
    },
    "mutates": true,
    "tags": [
      "access",
      "write"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "Schedule a full build for the 'Sales Analytics' DataModel to run every Monday at 2:00 AM UTC.",
        "arguments": {
          "datamodel_name": "Sales Analytics",
          "build_type": "FULL",
          "days": [
            "MON"
          ],
          "hour": 2,
          "minute": 0
        },
        "notes": "This schedules a full build of the 'Sales Analytics' DataModel to occur every Monday at 2:00 AM UTC."
      },
      {
        "user_query": "Set up an interval-based build for the 'Marketing Insights' DataModel to run every 3 days and 4 hours.",
        "arguments": {
          "datamodel_name": "Marketing Insights",
          "interval_days": 3,
          "interval_hours": 4
        },
        "notes": "This creates an interval-based schedule for the 'Marketing Insights' DataModel to build every 3 days and 4 hours."
      },
      {
        "user_query": "Create a cron-based schedule for the 'Customer Retention' DataModel to build daily at 11:30 PM UTC.",
        "arguments": {
          "datamodel_name": "Customer Retention",
          "days": [
            "*"
          ],
          "hour": 23,
          "minute": 30
        },
        "notes": "This sets up a daily build for the 'Customer Retention' DataModel at 11:30 PM UTC using a cron-based schedule."
      }
    ]
  },
  {
    "tool_id": "access.create_user",
    "module": "access",
    "class": "AccessManagement",
    "method": "create_user",
    "description": "Creates a new user by processing the provided user data to replace role",
    "full_doc": "Creates a new user by processing the provided user data to replace role\nnames and group names with their corresponding IDs, then sends a POST\nrequest to create the user.\n\nParameters:\n    user_data (dict): A dictionary containing user details such as\n    email, firstName, lastName, role (role name), groups (list of group\n    names), and preferences.\n\nReturns:\n    dict: The response from the API if successful,\n        or a dictionary with an 'error' key if the operation fails.",
    "parameters": {
      "type": "object",
      "properties": {
        "user_data": {
          "type": "object",
          "description": "A dictionary containing user details such as email, firstName, lastName, role (role name), groups (list of group names), and preferences."
        }
      },
      "required": [
        "user_data"
      ]
    },
    "mutates": true,
    "tags": [
      "access",
      "users",
      "write"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "Create a new user with the role of 'dataDesigner' and add them to the 'Marketing' and 'Sales' groups.",
        "arguments": {
          "user_data": {
            "email": "jane.doe@example.com",
            "firstName": "Jane",
            "lastName": "Doe",
            "role": "dataDesigner",
            "groups": [
              "Marketing",
              "Sales"
            ],
            "preferences": {
              "language": "en-US"
            }
          }
        },
        "notes": "This call creates a new user with specific role and group memberships, and sets their language preference to English (US)."
      },
      {
        "user_query": "Add a new user with the default 'viewer' role and no group memberships.",
        "arguments": {
          "user_data": {
            "email": "john.smith@example.com",
            "firstName": "John",
            "lastName": "Smith",
            "role": "viewer",
            "groups": [],
            "preferences": {}
          }
        },
        "notes": "This call creates a user with the default 'viewer' role and no assigned groups or preferences."
      },
      {
        "user_query": "Create a user with the 'admin' role, assign them to the 'Admins' group, and set their preferred language to French.",
        "arguments": {
          "user_data": {
            "email": "admin.user@example.com",
            "firstName": "Admin",
            "lastName": "User",
            "role": "admin",
            "groups": [
              "Admins"
            ],
            "preferences": {
              "language": "fr-FR"
            }
          }
        },
        "notes": "This call creates an admin user with access to the 'Admins' group and sets their language preference to French."
      }
    ]
  },
  {
    "tool_id": "access.delete_user",
    "module": "access",
    "class": "AccessManagement",
    "method": "delete_user",
    "description": "Deletes a user by their email (username).",
    "full_doc": "Deletes a user by their email (username).\n\nParameters:\n    user_name (str): The email or username of the user to be deleted.\n\nReturns:\n    dict: Response from the API if successful,\n        or an error message dict.",
    "parameters": {
      "type": "object",
      "properties": {
        "user_name": {
          "type": "string",
          "description": "The email or username of the user to be deleted."
        }
      },
      "required": [
        "user_name"
      ]
    },
    "mutates": true,
    "tags": [
      "access",
      "users",
      "write"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "Remove the user with the email john.doe@example.com from the system.",
        "arguments": {
          "user_name": "john.doe@example.com"
        },
        "notes": "This call deletes the user identified by the email 'john.doe@example.com'. Use this when a user needs to be permanently removed from the system."
      },
      {
        "user_query": "Delete the user with the username admin.user@sisense.com.",
        "arguments": {
          "user_name": "admin.user@sisense.com"
        },
        "notes": "This call removes the user with the username 'admin.user@sisense.com'. Typically used when an admin account is no longer required."
      },
      {
        "user_query": "Remove the user with the email jane.smith@company.com from the platform.",
        "arguments": {
          "user_name": "jane.smith@company.com"
        },
        "notes": "This operation deletes the user 'jane.smith@company.com'. Use this to clean up user accounts that are no longer active."
      }
    ]
  },
  {
    "tool_id": "access.get_all_dashboard_shares",
    "module": "access",
    "class": "AccessManagement",
    "method": "get_all_dashboard_shares",
    "description": "Method to retrieve all dashboard shares, including user and group details for each shared dashboard.",
    "full_doc": "Method to retrieve all dashboard shares, including user and group details for each shared dashboard.\n\nThis method uses pagination to retrieve all dashboards and their share information, and it collects\ncorresponding user and group details for each share.\n\nReturns:\n    list: A list of dictionaries containing the dashboard title, share type (user or group),\n    and share name (email or group name).",
    "parameters": {
      "type": "object",
      "properties": {},
      "required": []
    },
    "mutates": false,
    "tags": [
      "access",
      "dashboards",
      "read"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "How can I see all the dashboards shared with users and groups in my organization?",
        "arguments": {},
        "notes": "This retrieves a list of all dashboards along with their sharing details, including the users and groups they are shared with. Useful for auditing dashboard access."
      },
      {
        "user_query": "I need to review the sharing settings for all dashboards to ensure compliance with our data access policies.",
        "arguments": {},
        "notes": "This call provides a comprehensive overview of dashboard sharing settings, helping administrators verify that sharing aligns with organizational policies."
      },
      {
        "user_query": "Can I get a list of all dashboards and the users or groups they are shared with?",
        "arguments": {},
        "notes": "This retrieves all dashboards and their associated sharing information, including the type of share (user or group) and the specific names or emails of the recipients."
      }
    ]
  },
  {
    "tool_id": "access.get_datamodel_columns",
    "module": "access",
    "class": "AccessManagement",
    "method": "get_datamodel_columns",
    "description": "Retrieves columns from a DataModel by collecting them from its datasets and tables.",
    "full_doc": "Retrieves columns from a DataModel by collecting them from its datasets and tables.\n\nParameters:\n    datamodel_name (str): The name of the DataModel from which to extract columns.\n\nReturns:\n    list: A list of dictionaries where each dictionary contains DataModel ID, DataModel name,\n    table name, and column name.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "The name of the DataModel from which to extract columns."
        }
      },
      "required": [
        "datamodel_name"
      ]
    },
    "mutates": false,
    "tags": [
      "access",
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "What columns are available in the 'Sales Analytics' DataModel?",
        "arguments": {
          "datamodel_name": "Sales Analytics"
        },
        "notes": "This call retrieves all columns from the 'Sales Analytics' DataModel, including columns from its datasets and tables. Use this to understand the structure of the DataModel."
      },
      {
        "user_query": "Can you list all columns in the 'Customer Insights' DataModel?",
        "arguments": {
          "datamodel_name": "Customer Insights"
        },
        "notes": "This call extracts column details from the 'Customer Insights' DataModel. Useful for auditing or preparing data for analysis."
      },
      {
        "user_query": "I need to see the columns in the 'Marketing Performance' DataModel for reporting purposes.",
        "arguments": {
          "datamodel_name": "Marketing Performance"
        },
        "notes": "This call gathers column information from the 'Marketing Performance' DataModel, helping to identify fields for reporting and dashboard creation."
      }
    ]
  },
  {
    "tool_id": "access.get_group",
    "module": "access",
    "class": "AccessManagement",
    "method": "get_group",
    "description": "Retrieves group details by their name.",
    "full_doc": "Retrieves group details by their name.\n\nParameters:\n    name (str): The name of the group to be retrieved.\n\nReturns:\n    dict: Group details, or {'error': ...} if retrieval fails or not\n    found.",
    "parameters": {
      "type": "object",
      "properties": {
        "name": {
          "type": "string",
          "description": "The name of the group to be retrieved."
        }
      },
      "required": [
        "name"
      ]
    },
    "mutates": false,
    "tags": [
      "access",
      "groups",
      "read"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "What are the details of the 'Admins' group?",
        "arguments": {
          "name": "Admins"
        },
        "notes": "This retrieves detailed information about the 'Admins' group, including its members and permissions. Use this to audit or manage group settings."
      },
      {
        "user_query": "Can you fetch the details for the 'Data Analysts' group?",
        "arguments": {
          "name": "Data Analysts"
        },
        "notes": "This call retrieves information about the 'Data Analysts' group. Useful for verifying group configurations or troubleshooting access issues."
      },
      {
        "user_query": "I need to check the configuration of the 'Marketing Team' group.",
        "arguments": {
          "name": "Marketing Team"
        },
        "notes": "This fetches the details of the 'Marketing Team' group, which can help ensure the group is set up correctly for dashboard access and permissions."
      }
    ]
  },
  {
    "tool_id": "access.get_unused_columns",
    "module": "access",
    "class": "AccessManagement",
    "method": "get_unused_columns",
    "description": "Identify unused columns in a given DataModel by comparing all available columns against the columns",
    "full_doc": "Identify unused columns in a given DataModel by comparing all available columns against the columns\nreferenced in associated dashboards.\n\nCovers:\n- Dashboard Filters: Dashboard-level filters, Widget filters, Dependent Filters.\n- Widget Panels: Includes Row, Values, Column panels, and Measured Filters.\n\nParameters:\n    datamodel_name (str): The name of the DataModel to analyze.\n\nReturns:\n    list: A list of dictionaries containing unused column details with a \"used\" field set to True or False.\n\nRaises:\n    ValueError: If no columns are found for the given DataModel (for example, if it does not exist\n                or is not accessible).",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "The name of the DataModel to analyze."
        }
      },
      "required": [
        "datamodel_name"
      ]
    },
    "mutates": false,
    "tags": [
      "access",
      "read"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "Which columns in the 'Sales DataModel' are not being used in any dashboards?",
        "arguments": {
          "datamodel_name": "Sales DataModel"
        },
        "notes": "This call identifies all unused columns in the 'Sales DataModel' by comparing them against columns referenced in dashboards. Useful for cleaning up unused fields."
      },
      {
        "user_query": "Can you find unused columns in the 'Customer Insights' DataModel?",
        "arguments": {
          "datamodel_name": "Customer Insights"
        },
        "notes": "This call analyzes the 'Customer Insights' DataModel to determine which columns are not utilized in any associated dashboards. Ideal for optimizing DataModel design."
      },
      {
        "user_query": "Are there any unused columns in the 'Marketing Analytics' DataModel?",
        "arguments": {
          "datamodel_name": "Marketing Analytics"
        },
        "notes": "This call checks the 'Marketing Analytics' DataModel for columns that are not being used in dashboards. Helps identify redundant fields for potential removal."
      }
    ]
  },
  {
    "tool_id": "access.get_unused_columns_bulk",
    "module": "access",
    "class": "AccessManagement",
    "method": "get_unused_columns_bulk",
    "description": "Run unused-column analysis for one or more data models and return a",
    "full_doc": "Run unused-column analysis for one or more data models and return a\ncombined result set.\n\nParameters\n----------\ndatamodels : str or list of str, optional\n    One or more data model references to analyze. Each reference can be:\n      - a data model ID, or\n      - a data model title (name).\n    At least one data model reference is required. At runtime this\n    parameter is tolerant of a single string and will normalize it to a\n    one-element list.\n\nReturns\n-------\nlist of dict\n    A flat list of rows across all processed data models. Each row has\n    the same structure as returned by get_unused_columns().\n    If no data models are successfully processed, an empty list is\n    returned and details are available in the logs.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodels": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "One or more data model references to analyze. Each reference can be: - a data model ID, or - a data model title (name). At least one data model reference is required. At runtime this parameter is tolerant of a single string and will normalize it to a one-element list."
        }
      },
      "required": []
    },
    "mutates": false,
    "tags": [
      "access",
      "read"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "Find all unused columns in the 'Sales Data' and 'Marketing Analytics' data models.",
        "arguments": {
          "datamodels": [
            "Sales Data",
            "Marketing Analytics"
          ]
        },
        "notes": "This call analyzes the 'Sales Data' and 'Marketing Analytics' data models to identify columns that are not used in any dashboards or widgets."
      },
      {
        "user_query": "Check for unused columns in the data model with ID '12345-abcde-67890-fghij'.",
        "arguments": {
          "datamodels": [
            "12345-abcde-67890-fghij"
          ]
        },
        "notes": "This call targets a specific data model by its unique ID to identify unused columns. Useful when the data model ID is known."
      },
      {
        "user_query": "Analyze unused columns across multiple data models, including 'Finance_Model' and the model with ID '98765-zyxwv-43210-lmnop'.",
        "arguments": {
          "datamodels": [
            "Finance_Model",
            "98765-zyxwv-43210-lmnop"
          ]
        },
        "notes": "This call combines unused-column analysis for both a named data model ('Finance_Model') and a data model identified by its unique ID."
      }
    ]
  },
  {
    "tool_id": "access.get_user",
    "module": "access",
    "class": "AccessManagement",
    "method": "get_user",
    "description": "Retrieves user details by their email (username) and expands the",
    "full_doc": "Retrieves user details by their email (username) and expands the\nresponse to include group and role information.\n\nParameters:\n    user_name (str): The email or username of the user to be retrieved.\n\nReturns:\n    dict: User details on success, or {'error': 'message'} on failure.",
    "parameters": {
      "type": "object",
      "properties": {
        "user_name": {
          "type": "string",
          "description": "The email or username of the user to be retrieved."
        }
      },
      "required": [
        "user_name"
      ]
    },
    "mutates": false,
    "tags": [
      "access",
      "users",
      "read"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "What are the details of the user with the email john.doe@example.com?",
        "arguments": {
          "user_name": "john.doe@example.com"
        },
        "notes": "This retrieves detailed information about the user with the email 'john.doe@example.com', including their roles and group memberships."
      },
      {
        "user_query": "Can you fetch the user details for the account associated with admin@sisense.com?",
        "arguments": {
          "user_name": "admin@sisense.com"
        },
        "notes": "This call is used to get the details of the user with the email 'admin@sisense.com', which might be an admin account."
      },
      {
        "user_query": "I need to verify the user information for jane.smith@company.com. Can you provide it?",
        "arguments": {
          "user_name": "jane.smith@company.com"
        },
        "notes": "This retrieves the user details for 'jane.smith@company.com', including their associated roles and groups, to verify their account information."
      }
    ]
  },
  {
    "tool_id": "access.get_users_all",
    "module": "access",
    "class": "AccessManagement",
    "method": "get_users_all",
    "description": "Retrieves user details along with tenant, group, and role information.",
    "full_doc": "Retrieves user details along with tenant, group, and role information.\nRemoves \"Everyone\" group from users if they belong to other groups, but\nkeeps the \"Everyone\" group if it's the only group the user belongs to.\n\nReturns:\n    list: List of user details dicts, or [{'error': ...}] if retrieval\n    fails.",
    "parameters": {
      "type": "object",
      "properties": {},
      "required": []
    },
    "mutates": false,
    "tags": [
      "access",
      "users",
      "read"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "Show me all users in the system along with their roles and groups.",
        "arguments": {},
        "notes": "This retrieves a complete list of all users in the system, including their tenant, group, and role information."
      },
      {
        "user_query": "I need to export a list of all users and their associated roles to a CSV file.",
        "arguments": {},
        "notes": "Use this to fetch all user details, which can then be exported to a CSV file for reporting or auditing purposes."
      },
      {
        "user_query": "Can I get a detailed list of all users, including which groups they belong to?",
        "arguments": {},
        "notes": "This call provides a detailed breakdown of all users, including their group memberships, roles, and tenant information."
      }
    ]
  },
  {
    "tool_id": "access.update_user",
    "module": "access",
    "class": "AccessManagement",
    "method": "update_user",
    "description": "Updates a user by their User Name.",
    "full_doc": "Updates a user by their User Name.\n\nParameters:\n    user_name (str): The email or username of the user to be updated.\n    user_data (dict): A dictionary containing user details to update,\n    such as role, groups, etc.\n\nReturns:\n    dict: The response from the API if successful,\n        or a dictionary with an 'error' key if the operation fails.",
    "parameters": {
      "type": "object",
      "properties": {
        "user_name": {
          "type": "string",
          "description": "The email or username of the user to be updated."
        },
        "user_data": {
          "type": "object",
          "description": "A dictionary containing user details to update, such as role, groups, etc."
        }
      },
      "required": [
        "user_name",
        "user_data"
      ]
    },
    "mutates": true,
    "tags": [
      "access",
      "users",
      "write"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "Update the role and groups for user john.doe@example.com.",
        "arguments": {
          "user_name": "john.doe@example.com",
          "user_data": {
            "role": "dataDesigner",
            "groups": [
              "SalesTeam",
              "AnalyticsGroup"
            ]
          }
        },
        "notes": "This call updates the user's role to 'dataDesigner' and assigns them to the 'SalesTeam' and 'AnalyticsGroup' groups."
      },
      {
        "user_query": "Change the language preference for user jane.smith@example.com to French.",
        "arguments": {
          "user_name": "jane.smith@example.com",
          "user_data": {
            "preferences": {
              "language": "fr-FR"
            }
          }
        },
        "notes": "This call updates the user's language preference to French (fr-FR). Use this when a user requests a change in their UI language settings."
      },
      {
        "user_query": "Add user mike.jones@example.com to the 'AdminGroup' and update their last name.",
        "arguments": {
          "user_name": "mike.jones@example.com",
          "user_data": {
            "lastName": "Jones",
            "groups": [
              "AdminGroup"
            ]
          }
        },
        "notes": "This call updates the user's last name to 'Jones' and assigns them to the 'AdminGroup'. Use this to modify personal details and group memberships."
      }
    ]
  },
  {
    "tool_id": "access.users_per_group",
    "module": "access",
    "class": "AccessManagement",
    "method": "users_per_group",
    "description": "Retrieves all users within a specific group by name.",
    "full_doc": "Retrieves all users within a specific group by name.\n\nParameters:\n    group_name (str): The name of the group.\n\nReturns:\n    list or dict: A list of users in the group if successful, or a\n    dictionary containing an 'error' key if the operation fails.",
    "parameters": {
      "type": "object",
      "properties": {
        "group_name": {
          "type": "string",
          "description": "The name of the group."
        }
      },
      "required": [
        "group_name"
      ]
    },
    "mutates": false,
    "tags": [
      "access",
      "users",
      "groups",
      "read"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "Who are all the users in the Admins group?",
        "arguments": {
          "group_name": "Admins"
        },
        "notes": "This call retrieves a list of all users who belong to the 'Admins' group. Useful for auditing or managing admin-level access."
      },
      {
        "user_query": "Can you list all users in the Data Analysts group?",
        "arguments": {
          "group_name": "Data Analysts"
        },
        "notes": "This call fetches all users in the 'Data Analysts' group. Ideal for reviewing team memberships or assigning new tasks."
      },
      {
        "user_query": "Get all users in the Marketing Team group.",
        "arguments": {
          "group_name": "Marketing Team"
        },
        "notes": "This call returns all users in the 'Marketing Team' group. Useful for verifying group memberships or preparing reports."
      }
    ]
  },
  {
    "tool_id": "access.users_per_group_all",
    "module": "access",
    "class": "AccessManagement",
    "method": "users_per_group_all",
    "description": "Retrieves all groups and maps them with the users belonging to those",
    "full_doc": "Retrieves all groups and maps them with the users belonging to those\ngroups.\nGroups like 'Everyone' and 'All users in system' are excluded.\nUsers with roles like 'admin', 'dataAdmin', and 'sysAdmin' are mapped\nto the existing 'Admins' group.\nGroups with no users are also included in the final result.\n\nReturns:\n    list: A list of dictionaries, where each dictionary contains a\n    group name and the list of usernames in that group.",
    "parameters": {
      "type": "object",
      "properties": {},
      "required": []
    },
    "mutates": false,
    "tags": [
      "access",
      "users",
      "groups",
      "read"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "Show me all groups and the users assigned to each group.",
        "arguments": {},
        "notes": "This call retrieves a complete mapping of all groups in the system to their associated users, excluding system groups like 'Everyone'."
      },
      {
        "user_query": "I need a list of all groups and their members to audit user access.",
        "arguments": {},
        "notes": "Use this to generate an overview of group-user associations for auditing purposes. Groups with no users will also be included."
      },
      {
        "user_query": "Can I get a breakdown of all groups and the users in each group for reporting?",
        "arguments": {},
        "notes": "This call provides a detailed list of groups and their members, which can be exported for reporting or further analysis."
      }
    ]
  },
  {
    "tool_id": "datamodel.add_datamodel_shares",
    "module": "datamodel",
    "class": "DataModel",
    "method": "add_datamodel_shares",
    "description": "Adds share entries (users and groups) to a DataModel.",
    "full_doc": "Adds share entries (users and groups) to a DataModel.\n\nParameters:\n    datamodel_name (str): Name of the DataModel to add shares to.\n    shares (list): List of dictionaries containing share details. Each dictionary should have:\n        - name: Name of the user or group\n        - type: Type of the party (user or group)\n        - permission: Permission level (EDIT, READ, USE)\n\nReturns:\n    dict: Result of the share addition operation.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel to add shares to."
        },
        "shares": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "List of dictionaries containing share details. Each dictionary should have: - name: Name of the user or group - type: Type of the party (user or group) - permission: Permission level (EDIT, READ, USE)"
        }
      },
      "required": [
        "datamodel_name",
        "shares"
      ]
    },
    "mutates": true,
    "tags": [
      "datamodel",
      "write"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "I want to give edit permissions to a user and read permissions to a group for the Sales DataModel.",
        "arguments": {
          "datamodel_name": "Sales_DataModel",
          "shares": [
            {
              "name": "john.doe@sisense.com",
              "type": "user",
              "permission": "EDIT"
            },
            {
              "name": "Sales_Team",
              "type": "group",
              "permission": "READ"
            }
          ]
        },
        "notes": "This call adds sharing permissions to the 'Sales_DataModel' for a specific user and group. Use this to manage access control for users and groups."
      },
      {
        "user_query": "How can I allow a group to use the Marketing DataModel but restrict editing?",
        "arguments": {
          "datamodel_name": "Marketing_DataModel",
          "shares": [
            {
              "name": "Marketing_Analysts",
              "type": "group",
              "permission": "USE"
            }
          ]
        },
        "notes": "This call grants 'USE' permissions to the 'Marketing_Analysts' group for the 'Marketing_DataModel'. This is useful when you want the group to access the DataModel without editing it."
      },
      {
        "user_query": "I need to add multiple users with different permissions to the Finance DataModel.",
        "arguments": {
          "datamodel_name": "Finance_DataModel",
          "shares": [
            {
              "name": "alice.smith@sisense.com",
              "type": "user",
              "permission": "READ"
            },
            {
              "name": "bob.jones@sisense.com",
              "type": "user",
              "permission": "EDIT"
            },
            {
              "name": "Finance_Team",
              "type": "group",
              "permission": "USE"
            }
          ]
        },
        "notes": "This call adds multiple users and a group to the 'Finance_DataModel' with varying levels of access. Use this to set up granular permissions for different parties."
      }
    ]
  },
  {
    "tool_id": "datamodel.create_connections",
    "module": "datamodel",
    "class": "DataModel",
    "method": "create_connections",
    "description": "Creates a new connection using the provided payload.",
    "full_doc": "Creates a new connection using the provided payload.\n\nParameters:\n    connection_payload (dict): The configuration payload for the connection.\n\nReturns:\n    dict or None: JSON response with connection details if successful, otherwise None.",
    "parameters": {
      "type": "object",
      "properties": {
        "connection_payload": {
          "type": "object",
          "description": "The configuration payload for the connection."
        }
      },
      "required": [
        "connection_payload"
      ]
    },
    "mutates": true,
    "tags": [
      "datamodel",
      "write"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "Create a new connection for an Athena data source.",
        "arguments": {
          "connection_payload": {
            "name": "athena_connection",
            "description": "Connection to Athena for analytics",
            "region": "us-west-2",
            "s3_output_location": "s3://sisense-athena-output/",
            "aws_access_key": "AKIAEXAMPLE123",
            "aws_secret_key": "SECRETEXAMPLE123",
            "schema": "analytics",
            "additional_parameters": "timeout=120;"
          }
        },
        "notes": "This call creates a new connection to an AWS Athena data source, specifying the region, S3 output location, and credentials."
      },
      {
        "user_query": "Set up a connection to a Redshift cluster for reporting.",
        "arguments": {
          "connection_payload": {
            "name": "redshift_reporting",
            "description": "Redshift connection for reporting database",
            "server": "redshift-cluster.example.com:5439",
            "username": "admin_user",
            "password": "securepassword123",
            "default_database": "reporting_db",
            "additional_parameters": "ssl=true;"
          }
        },
        "notes": "This call creates a connection to a Redshift database, providing the server address, credentials, and default database name."
      },
      {
        "user_query": "Add a new connection to a BigQuery project for data analysis.",
        "arguments": {
          "connection_payload": {
            "name": "bigquery_analysis",
            "description": "BigQuery connection for analysis project",
            "use_service_account": true,
            "service_account_key_path": "/path/to/service_account_key.json",
            "use_proxy_server": false,
            "use_dynamic_schema": true,
            "record_field_flattening_level": "2",
            "unnest_arrays": true,
            "allow_large_results": true,
            "use_storage_api": true,
            "additional_parameters": "timeout=300;",
            "database": "analysis_project"
          }
        },
        "notes": "This call sets up a connection to a BigQuery project using a service account key and enables advanced options like dynamic schema and unnesting arrays."
      }
    ]
  },
  {
    "tool_id": "datamodel.create_datamodel",
    "module": "datamodel",
    "class": "DataModel",
    "method": "create_datamodel",
    "description": "Creates a new DataModel in Sisense.",
    "full_doc": "Creates a new DataModel in Sisense.\n\nParameters:\n    datamodel_name (str): Name of the DataModel.\n    datamodel_type (str): Type of the DataModel. Should be either \"extract\" (for Elasticube)\n        or \"live\" (for Live).\n\nReturns:\n    dict: Dictionary with the DataModel ID if created successfully, or an error message.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel."
        },
        "datamodel_type": {
          "type": "string",
          "description": "Either 'extract' (Elasticube/EC) or 'live'. If user says 'elasticube' or 'ec', normalize to 'extract'.",
          "enum": [
            "extract",
            "live"
          ],
          "x-aliases": {
            "extract": [
              "ec",
              "elasticube",
              "elastic cube",
              "cube",
              "elastic-cube"
            ],
            "live": [
              "realtime",
              "real-time",
              "live model"
            ]
          }
        }
      },
      "required": [
        "datamodel_name",
        "datamodel_type"
      ]
    },
    "mutates": true,
    "tags": [
      "datamodel",
      "write"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "Create a new Elasticube DataModel named 'SalesDataCube'.",
        "arguments": {
          "datamodel_name": "SalesDataCube",
          "datamodel_type": "extract"
        },
        "notes": "This call creates a new Elasticube DataModel for storing and querying sales data. Use this when you need an extract-based model for offline data processing."
      },
      {
        "user_query": "Set up a live DataModel named 'RealTimeAnalytics'.",
        "arguments": {
          "datamodel_name": "RealTimeAnalytics",
          "datamodel_type": "live"
        },
        "notes": "This call creates a live DataModel for real-time analytics. Use this when you need to connect directly to a live data source without extracting data."
      },
      {
        "user_query": "Create a new DataModel for product inventory using an Elasticube.",
        "arguments": {
          "datamodel_name": "ProductInventoryCube",
          "datamodel_type": "extract"
        },
        "notes": "This call sets up an Elasticube DataModel for managing and analyzing product inventory data. Ideal for scenarios where data needs to be extracted and stored for advanced querying."
      }
    ]
  },
  {
    "tool_id": "datamodel.create_dataset",
    "module": "datamodel",
    "class": "DataModel",
    "method": "create_dataset",
    "description": "Creates a new dataset in the specified DataModel.",
    "full_doc": "Creates a new dataset in the specified DataModel.\n\nParameters:\n    datamodel_name (str): Name of the DataModel where the dataset will be created.\n    connection_name (str): Name of the connection to use.\n    database_name (str): Name of the data source database.\n    schema_name (str): Name of the data source schema.\n    dataset_name (str, optional): Name of the dataset. Defaults to schema name if not provided.\n\nReturns:\n    dict: A dictionary containing the full dataset object on success, or an error message on failure.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel where the dataset will be created."
        },
        "connection_name": {
          "type": "string",
          "description": "Name of the connection to use."
        },
        "database_name": {
          "type": "string",
          "description": "Name of the data source database."
        },
        "schema_name": {
          "type": "string",
          "description": "Name of the data source schema."
        },
        "dataset_name": {
          "type": "string",
          "description": "Name of the dataset. Defaults to schema name if not provided."
        }
      },
      "required": [
        "datamodel_name",
        "connection_name",
        "database_name",
        "schema_name"
      ]
    },
    "mutates": true,
    "tags": [
      "datamodel",
      "write"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "Create a new dataset in the 'SalesDataModel' using the 'PostgreSQL_SalesDB' connection and the 'public' schema.",
        "arguments": {
          "datamodel_name": "SalesDataModel",
          "connection_name": "PostgreSQL_SalesDB",
          "database_name": "sales_database",
          "schema_name": "public",
          "dataset_name": "SalesDataset"
        },
        "notes": "This call creates a dataset named 'SalesDataset' in the 'SalesDataModel' DataModel using the specified PostgreSQL connection and schema."
      },
      {
        "user_query": "Add a dataset to the 'MarketingModel' DataModel using the 'BigQuery_Marketing' connection and default the dataset name to the schema name.",
        "arguments": {
          "datamodel_name": "MarketingModel",
          "connection_name": "BigQuery_Marketing",
          "database_name": "marketing_data",
          "schema_name": "campaigns"
        },
        "notes": "This call creates a dataset in the 'MarketingModel' DataModel. Since 'dataset_name' is not provided, it defaults to the schema name 'campaigns'."
      },
      {
        "user_query": "Create a dataset in the 'FinanceCube' DataModel using the 'Snowflake_Finance' connection and the 'finance_schema' schema.",
        "arguments": {
          "datamodel_name": "FinanceCube",
          "connection_name": "Snowflake_Finance",
          "database_name": "corporate_finance",
          "schema_name": "finance_schema",
          "dataset_name": "FinanceDataset"
        },
        "notes": "This call creates a dataset named 'FinanceDataset' in the 'FinanceCube' DataModel using the Snowflake connection and schema."
      }
    ]
  },
  {
    "tool_id": "datamodel.create_table",
    "module": "datamodel",
    "class": "DataModel",
    "method": "create_table",
    "description": "Create a new table in the specified DataModel.",
    "full_doc": "Create a new table in the specified DataModel.\n\nParameters:\n    datamodel_name (str): Name of the DataModel where the table will be created.\n    table_name (str): Name of the table to create.\n    database_name (str, optional): Name of the data source database.\n        If not provided, will try to infer from the DataModel.\n    schema_name (str, optional): Name of the data source schema.\n        If not provided, will try to infer from the DataModel.\n    dataset_id (str, optional): ID of the dataset where the table will be created.\n        If not provided, will try to infer from the DataModel.\n    import_query (str, optional): SQL statement used as custom import query. Defaults to None.\n    description (str, optional): Description for the table. Defaults to an empty string.\n    tags (list, optional): List of tags to apply to the table. Defaults to None.\n    build_behavior_config (dict, optional): Configuration for table build behavior.\n\nReturns:\n    dict: Table object if created successfully or an error message.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel where the table will be created."
        },
        "table_name": {
          "type": "string",
          "description": "Name of the table to create."
        },
        "database_name": {
          "type": "string",
          "description": "Name of the data source database. If not provided, will try to infer from the DataModel."
        },
        "schema_name": {
          "type": "string",
          "description": "Name of the data source schema. If not provided, will try to infer from the DataModel."
        },
        "dataset_id": {
          "type": "string",
          "description": "ID of the dataset where the table will be created. If not provided, will try to infer from the DataModel."
        },
        "import_query": {
          "type": "string",
          "description": "SQL statement used as custom import query. Defaults to None."
        },
        "description": {
          "type": "string",
          "description": "Description for the table. Defaults to an empty string."
        },
        "tags": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "List of tags to apply to the table. Defaults to None."
        },
        "build_behavior_config": {
          "type": "object",
          "description": "Configuration for table build behavior."
        }
      },
      "required": [
        "datamodel_name",
        "table_name"
      ]
    },
    "mutates": true,
    "tags": [
      "datamodel",
      "write"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "Create a new table named 'sales_data' in the 'RetailAnalytics' DataModel using a custom SQL query.",
        "arguments": {
          "datamodel_name": "RetailAnalytics",
          "table_name": "sales_data",
          "database_name": "retail_db",
          "schema_name": "public",
          "import_query": "SELECT * FROM sales WHERE year = 2023",
          "description": "Sales data for the current year",
          "tags": [
            "sales",
            "2023",
            "analytics"
          ],
          "build_behavior_config": {
            "mode": "increment",
            "column_name": "transaction_date"
          }
        },
        "notes": "This call creates a new table with a custom SQL query and incremental build behavior, useful for analyzing sales data for 2023."
      },
      {
        "user_query": "Add a table named 'customer_info' to the 'CustomerInsights' DataModel without specifying a custom query.",
        "arguments": {
          "datamodel_name": "CustomerInsights",
          "table_name": "customer_info",
          "database_name": "customer_db",
          "schema_name": "public",
          "description": "Customer information table",
          "tags": [
            "customer",
            "info"
          ]
        },
        "notes": "This call creates a new table in the 'CustomerInsights' DataModel using the default schema and database settings."
      },
      {
        "user_query": "Create a table named 'inventory' in the 'WarehouseOps' DataModel with a specific dataset ID and build configuration.",
        "arguments": {
          "datamodel_name": "WarehouseOps",
          "table_name": "inventory",
          "dataset_id": "123e4567-e89b-12d3-a456-426614174000",
          "description": "Inventory data for warehouse operations",
          "tags": [
            "inventory",
            "warehouse"
          ],
          "build_behavior_config": {
            "mode": "replace"
          }
        },
        "notes": "This call creates a new table in the 'WarehouseOps' DataModel, explicitly linking it to a dataset and setting the build mode to replace."
      }
    ]
  },
  {
    "tool_id": "datamodel.deploy_datamodel",
    "module": "datamodel",
    "class": "DataModel",
    "method": "deploy_datamodel",
    "description": "Deploy (build or publish) the specified DataModel based on its type.",
    "full_doc": "Deploy (build or publish) the specified DataModel based on its type.\n\nThis method supports both Elasticube (EXTRACT) and Live models.\nThe behavior and required parameters differ based on model type:\n\n- For Elasticube models:\n    - build_type (str): Type of deployment. Options:\n        * \"schema_changes\" \u2014 Build only schema changes\n        * \"by_table\" \u2014 Build based on each table's config (e.g. incremental, accumulative)\n        * \"full\" \u2014 Rebuild the entire model from scratch (default)\n    - row_limit (int): Maximum number of rows to process. Defaults to 0 (no limit).\n    - schema_origin (str): Schema source. Options:\n        * \"latest\" \u2014 Build the schema as seen in the Data page (default)\n        * \"running\" \u2014 Build from the last successfully built version\n\n- For Live models:\n    - Only the `build_type` parameter is used internally and will be set to \"publish\"\n    - `row_limit` and `schema_origin` are ignored\n\nParameters:\n    datamodel_name (str): Name of the DataModel to deploy.\n    build_type (str): Type of deployment. Required for EXTRACT only.\n    row_limit (int): Row limit for build. Applicable only for EXTRACT.\n    schema_origin (str): Schema origin for build. Applicable only for EXTRACT.\n\nReturns:\n    dict: Deployment result including status, or error details.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel to deploy."
        },
        "build_type": {
          "type": "string",
          "description": "Build strategy for extract models. Omit for live/publish.",
          "enum": [
            "full",
            "by_table"
          ],
          "x-aliases": {
            "full": [
              "build",
              "rebuild",
              "start",
              "run",
              "execute",
              "refresh"
            ],
            "by_table": [
              "by-table",
              "table-wise",
              "incremental-tables"
            ]
          }
        },
        "row_limit": {
          "type": "integer",
          "description": "Row limit for build. Applicable only for EXTRACT.",
          "minimum": 1
        },
        "schema_origin": {
          "type": "string",
          "description": "Schema origin for build. Applicable only for EXTRACT.",
          "enum": [
            "latest",
            "schema_changes"
          ]
        }
      },
      "required": [
        "datamodel_name"
      ]
    },
    "mutates": true,
    "tags": [
      "datamodel",
      "write"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "How can I deploy the 'SalesDataModel' with a full rebuild?",
        "arguments": {
          "datamodel_name": "SalesDataModel",
          "build_type": "full",
          "row_limit": 0,
          "schema_origin": "latest"
        },
        "notes": "This call performs a full rebuild of the 'SalesDataModel' Elasticube, processing all rows and using the latest schema."
      },
      {
        "user_query": "I want to deploy the 'MarketingCube' with incremental table builds and limit the rows to 10,000. How do I do that?",
        "arguments": {
          "datamodel_name": "MarketingCube",
          "build_type": "by_table",
          "row_limit": 10000,
          "schema_origin": "latest"
        },
        "notes": "This call deploys the 'MarketingCube' Elasticube using incremental table builds, limiting the build to 10,000 rows, and using the latest schema."
      },
      {
        "user_query": "How do I publish the 'LiveAnalyticsModel' DataModel?",
        "arguments": {
          "datamodel_name": "LiveAnalyticsModel"
        },
        "notes": "This call publishes the 'LiveAnalyticsModel' Live DataModel. Parameters like 'build_type', 'row_limit', and 'schema_origin' are ignored for Live models."
      }
    ]
  },
  {
    "tool_id": "datamodel.describe_datamodel",
    "module": "datamodel",
    "class": "DataModel",
    "method": "describe_datamodel",
    "description": "Retrieve detailed datamodel structure in a flat, row-based format suitable for DataFrame or CSV export.",
    "full_doc": "Retrieve detailed datamodel structure in a flat, row-based format suitable for DataFrame or CSV export.\n\nParameters:\n    datamodel_name (str): Name of the DataModel to describe.\n\nReturns:\n    list: List of dictionaries, each representing a single table row with context (datamodel, dataset, table).",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel to describe."
        }
      },
      "required": [
        "datamodel_name"
      ]
    },
    "mutates": false,
    "tags": [
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "Can you provide the structure of the 'sales_dashboard_ec' DataModel?",
        "arguments": {
          "datamodel_name": "sales_dashboard_ec"
        },
        "notes": "This call retrieves the detailed structure of the 'sales_dashboard_ec' DataModel, including datasets and tables, in a format suitable for exporting to CSV or analyzing in a DataFrame."
      },
      {
        "user_query": "I need to export the schema of the 'marketing_analytics_live' DataModel for reporting purposes.",
        "arguments": {
          "datamodel_name": "marketing_analytics_live"
        },
        "notes": "Use this call to describe the 'marketing_analytics_live' DataModel, providing a flat structure of its datasets and tables for further analysis or export."
      },
      {
        "user_query": "What tables and datasets are included in the 'finance_reporting_ec' DataModel?",
        "arguments": {
          "datamodel_name": "finance_reporting_ec"
        },
        "notes": "This call helps to retrieve the structure of the 'finance_reporting_ec' DataModel, useful for understanding its composition and exporting its details for documentation or analysis."
      }
    ]
  },
  {
    "tool_id": "datamodel.describe_datamodel_raw",
    "module": "datamodel",
    "class": "DataModel",
    "method": "describe_datamodel_raw",
    "description": "Retrieve detailed information about a specific DataModel, including share details.",
    "full_doc": "Retrieve detailed information about a specific DataModel, including share details.\n\nParameters:\n    datamodel_name (str): Name of the DataModel to describe.\n\nReturns:\n    dict: Detailed information about the DataModel, or an error message if not found.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel to describe."
        }
      },
      "required": [
        "datamodel_name"
      ]
    },
    "mutates": true,
    "tags": [
      "datamodel",
      "write"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "Can you provide detailed information about the DataModel named 'sales_performance_ec'?",
        "arguments": {
          "datamodel_name": "sales_performance_ec"
        },
        "notes": "This call retrieves raw metadata and share details for the 'sales_performance_ec' DataModel. Useful for understanding the structure and access permissions of the DataModel."
      },
      {
        "user_query": "I need to check the share details and metadata for the DataModel 'customer_insights_live'.",
        "arguments": {
          "datamodel_name": "customer_insights_live"
        },
        "notes": "This call provides detailed raw information about the 'customer_insights_live' DataModel, including its share settings and structure. Ideal for auditing or troubleshooting access issues."
      },
      {
        "user_query": "What are the raw details for the DataModel 'marketing_analytics_2023'?",
        "arguments": {
          "datamodel_name": "marketing_analytics_2023"
        },
        "notes": "This call retrieves raw metadata and share details for the 'marketing_analytics_2023' DataModel. Useful for reviewing its configuration and access permissions."
      }
    ]
  },
  {
    "tool_id": "datamodel.generate_connections_payload",
    "module": "datamodel",
    "class": "DataModel",
    "method": "generate_connections_payload",
    "description": "Generates the appropriate connections payload based on the datasource type.",
    "full_doc": "Generates the appropriate connections payload based on the datasource type.\n\nParameters:\n    datasource_type (str): Type of datasource (e.g., \"ATHENA\", \"SNOWFLAKE\", \"ORACLE\").\n    connection_params (dict): Connection details for the datasource.\n\nReturns:\n    dict: Connections payload.",
    "parameters": {
      "type": "object",
      "properties": {
        "datasource_type": {
          "type": "string",
          "description": "Type of datasource (e.g., \"ATHENA\", \"SNOWFLAKE\", \"ORACLE\")."
        },
        "connection_params": {
          "type": "object",
          "description": "Connection details for the datasource."
        }
      },
      "required": [
        "datasource_type",
        "connection_params"
      ]
    },
    "mutates": false,
    "tags": [
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "How do I generate a connection payload for an Athena data source?",
        "arguments": {
          "datasource_type": "ATHENA",
          "connection_params": {
            "name": "athena_connection",
            "description": "Connection to Athena for analytics",
            "region": "us-east-1",
            "s3_output_location": "s3://sisense-analytics/athena-output/",
            "aws_access_key": "ABC123XYZ456",
            "aws_secret_key": "SECRET123456",
            "schema": "analytics_schema",
            "additional_parameters": "timeout=120;"
          }
        },
        "notes": "This call generates a connection payload for an Athena data source, which can then be used to create a connection in Sisense."
      },
      {
        "user_query": "Can you help me create a connection payload for Snowflake?",
        "arguments": {
          "datasource_type": "SNOWFLAKE",
          "connection_params": {
            "name": "snowflake_connection",
            "description": "Snowflake connection for sales data",
            "account": "mycompany.snowflakecomputing.com",
            "username": "admin_user",
            "password": "securepassword123",
            "warehouse": "COMPUTE_WH",
            "database": "SALES_DB",
            "schema": "PUBLIC",
            "role": "SYSADMIN",
            "additional_parameters": "query_tag=SisenseSDK;"
          }
        },
        "notes": "This call generates a connection payload for a Snowflake data source, which is useful for integrating Snowflake with Sisense."
      },
      {
        "user_query": "I need a connection payload for an Oracle database. Can you provide an example?",
        "arguments": {
          "datasource_type": "ORACLE",
          "connection_params": {
            "name": "oracle_connection",
            "description": "Oracle connection for HR data",
            "host": "oracle-db.mycompany.com",
            "port": 1521,
            "service_name": "ORCL",
            "username": "hr_user",
            "password": "password123",
            "schema": "HR",
            "additional_parameters": "connect_timeout=60;"
          }
        },
        "notes": "This example demonstrates how to generate a connection payload for an Oracle database, which can be used to connect Sisense to Oracle."
      }
    ]
  },
  {
    "tool_id": "datamodel.get_all_datamodel",
    "module": "datamodel",
    "class": "DataModel",
    "method": "get_all_datamodel",
    "description": "Retrieves metadata details of all DataModels using an undocumented internal API.",
    "full_doc": "Retrieves metadata details of all DataModels using an undocumented internal API.\nThis includes additional fields like build status, size, and timestamps that may\nnot be available through the standard public endpoints.\n\nReturns:\n    dict: Parsed metadata details of all DataModels, or a dictionary with an error message.",
    "parameters": {
      "type": "object",
      "properties": {},
      "required": []
    },
    "mutates": false,
    "tags": [
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "List all DataModels in the system with their metadata.",
        "arguments": {},
        "notes": "This retrieves metadata for all DataModels, including details like build status, size, and timestamps. Useful for auditing or monitoring purposes."
      },
      {
        "user_query": "Fetch details of all available DataModels to analyze their build status and sizes.",
        "arguments": {},
        "notes": "Use this to get a comprehensive overview of all DataModels, including their types and statuses, to identify any that may need attention."
      },
      {
        "user_query": "Get a list of all DataModels to export their metadata for reporting.",
        "arguments": {},
        "notes": "This call is ideal for exporting DataModel metadata to a file or external system for further analysis or reporting."
      }
    ]
  },
  {
    "tool_id": "datamodel.get_connection",
    "module": "datamodel",
    "class": "DataModel",
    "method": "get_connection",
    "description": "Retrieves a Connection by its name.",
    "full_doc": "Retrieves a Connection by its name.\n\nParameters:\n    connection_name (str): Name of the connection to filter by.\n\nReturns:\n    List: Connection details if found, or a dictionary with an error message.",
    "parameters": {
      "type": "object",
      "properties": {
        "connection_name": {
          "type": "string",
          "description": "Name of the connection to filter by."
        }
      },
      "required": [
        "connection_name"
      ]
    },
    "mutates": false,
    "tags": [
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "What are the details of the connection named 'salesforce_connection'?",
        "arguments": {
          "connection_name": "salesforce_connection"
        },
        "notes": "This call retrieves the details of the connection named 'salesforce_connection'. Use this to get metadata or verify the connection configuration."
      },
      {
        "user_query": "Can you fetch the connection information for 'google_analytics_data'?",
        "arguments": {
          "connection_name": "google_analytics_data"
        },
        "notes": "This call fetches the connection details for 'google_analytics_data'. It is useful for checking the configuration or troubleshooting issues with the connection."
      },
      {
        "user_query": "I need the connection details for 'azure_sql_server'. Can you provide them?",
        "arguments": {
          "connection_name": "azure_sql_server"
        },
        "notes": "This call retrieves the connection details for 'azure_sql_server'. It can be used to confirm the connection settings or to verify its existence in the system."
      }
    ]
  },
  {
    "tool_id": "datamodel.get_data",
    "module": "datamodel",
    "class": "DataModel",
    "method": "get_data",
    "description": "Retrieves data from a specific table in a DataModel and returns it as a list of dicts",
    "full_doc": "Retrieves data from a specific table in a DataModel and returns it as a list of dicts\n(row-based format) compatible with to_dataframe.\n\nParameters:\n    datamodel_name (str): Name of the DataModel.\n    table_name (str): Name of the table to retrieve data from.\n    query (str): Optional SQL query to filter the data.\n\nReturns:\n    list: List of dictionaries where each dict represents a row.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel."
        },
        "table_name": {
          "type": "string",
          "description": "Name of the table to retrieve data from."
        },
        "query": {
          "type": "string",
          "description": "Optional SQL query to filter the data."
        }
      },
      "required": [
        "datamodel_name",
        "table_name"
      ]
    },
    "mutates": false,
    "tags": [
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "Retrieve all data from the 'sales_data' table in the 'global_sales' DataModel.",
        "arguments": {
          "datamodel_name": "global_sales",
          "table_name": "sales_data"
        },
        "notes": "This call retrieves all rows and columns from the 'sales_data' table in the 'global_sales' DataModel without applying any filters."
      },
      {
        "user_query": "Get the total number of orders from the 'orders' table in the 'ecommerce_analytics' DataModel.",
        "arguments": {
          "datamodel_name": "ecommerce_analytics",
          "table_name": "orders",
          "query": "SELECT COUNT(*) AS total_orders FROM orders"
        },
        "notes": "This call applies a custom SQL query to count the total number of rows in the 'orders' table of the 'ecommerce_analytics' DataModel."
      },
      {
        "user_query": "Fetch data for the 'customers' table in the 'crm_data' DataModel, filtering for customers in the US.",
        "arguments": {
          "datamodel_name": "crm_data",
          "table_name": "customers",
          "query": "SELECT * FROM customers WHERE country = 'US'"
        },
        "notes": "This call retrieves rows from the 'customers' table in the 'crm_data' DataModel where the 'country' column equals 'US'."
      }
    ]
  },
  {
    "tool_id": "datamodel.get_datamodel",
    "module": "datamodel",
    "class": "DataModel",
    "method": "get_datamodel",
    "description": "Retrieves a DataModel by its name.",
    "full_doc": "Retrieves a DataModel by its name.\n\nParameters:\n    datamodel_name (str): Name of the DataModel to retrieve.\n\nReturns:\n    dict: DataModel details if found, or a dictionary with an error message.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel to retrieve."
        }
      },
      "required": [
        "datamodel_name"
      ]
    },
    "mutates": false,
    "tags": [
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "How can I retrieve the details of the DataModel named 'SalesAnalytics'?",
        "arguments": {
          "datamodel_name": "SalesAnalytics"
        },
        "notes": "This call retrieves the full details of the 'SalesAnalytics' DataModel, including its metadata and configuration."
      },
      {
        "user_query": "I need to find the configuration for the DataModel called 'CustomerInsights'. How can I do that?",
        "arguments": {
          "datamodel_name": "CustomerInsights"
        },
        "notes": "Use this call to fetch all details of the 'CustomerInsights' DataModel, such as its type, status, and associated datasets."
      },
      {
        "user_query": "Can you get me the information for the DataModel named 'MarketingPerformance'?",
        "arguments": {
          "datamodel_name": "MarketingPerformance"
        },
        "notes": "This retrieves the full details of the 'MarketingPerformance' DataModel, which can be useful for reviewing its structure or troubleshooting."
      }
    ]
  },
  {
    "tool_id": "datamodel.get_datamodel_shares",
    "module": "datamodel",
    "class": "DataModel",
    "method": "get_datamodel_shares",
    "description": "Retrieves all share entries (users and groups) for a given DataModel in flat row format.",
    "full_doc": "Retrieves all share entries (users and groups) for a given DataModel in flat row format.\n\nParameters:\n    datamodel_name (str): Name of the DataModel to retrieve shares for.\n\nReturns:\n    list: List of dicts with datamodel name, party name, type, and permission.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel to retrieve shares for."
        }
      },
      "required": [
        "datamodel_name"
      ]
    },
    "mutates": false,
    "tags": [
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "Who has access to the DataModel named 'Sales_Analytics'?",
        "arguments": {
          "datamodel_name": "Sales_Analytics"
        },
        "notes": "This call retrieves all users and groups with access to the 'Sales_Analytics' DataModel, along with their permission levels."
      },
      {
        "user_query": "Can you list the sharing permissions for the 'Customer_Insights' DataModel?",
        "arguments": {
          "datamodel_name": "Customer_Insights"
        },
        "notes": "This call provides a detailed list of all sharing entries (users and groups) for the 'Customer_Insights' DataModel."
      },
      {
        "user_query": "I need to review the sharing settings for the 'Marketing_Performance' DataModel.",
        "arguments": {
          "datamodel_name": "Marketing_Performance"
        },
        "notes": "Use this call to fetch all share entries for the 'Marketing_Performance' DataModel, including the type of access granted to each user or group."
      }
    ]
  },
  {
    "tool_id": "datamodel.get_datasecurity",
    "module": "datamodel",
    "class": "DataModel",
    "method": "get_datasecurity",
    "description": "Retrieves datasecurity table and column entries for a given DataModel in flat row format.",
    "full_doc": "Retrieves datasecurity table and column entries for a given DataModel in flat row format.\n\nParameters:\n    datamodel_name (str): Name of the DataModel to retrieve datasecurity for.\n\nReturns:\n    list: List of dicts with datamodel name, table name, column name, and security type.\n        If no rules exist, a single row is returned with empty values and the datamodel name.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel to retrieve datasecurity for."
        }
      },
      "required": [
        "datamodel_name"
      ]
    },
    "mutates": false,
    "tags": [
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "What are the datasecurity rules for the DataModel named 'sales_dashboard'?",
        "arguments": {
          "datamodel_name": "sales_dashboard"
        },
        "notes": "This retrieves all datasecurity table and column rules for the 'sales_dashboard' DataModel in a flat row format."
      },
      {
        "user_query": "Can you show me the data security configuration for the 'customer_insights' DataModel?",
        "arguments": {
          "datamodel_name": "customer_insights"
        },
        "notes": "Use this to fetch datasecurity entries for the 'customer_insights' DataModel, including table and column-level security rules."
      },
      {
        "user_query": "I need to check the datasecurity settings for the 'finance_analytics' DataModel.",
        "arguments": {
          "datamodel_name": "finance_analytics"
        },
        "notes": "This call retrieves the datasecurity rules for the 'finance_analytics' DataModel, showing any restrictions applied to tables or columns."
      }
    ]
  },
  {
    "tool_id": "datamodel.get_datasecurity_detail",
    "module": "datamodel",
    "class": "DataModel",
    "method": "get_datasecurity_detail",
    "description": "Retrieves detailed datasecurity rules for a specific DataModel, including share-level visibility.",
    "full_doc": "Retrieves detailed datasecurity rules for a specific DataModel, including share-level visibility.\nEach row represents a unique column-level rule and is repeated per share for clarity.\n\nSpecial handling is applied to interpret member values:\n- If \"members\" is an empty list and \"exclusionary\" is missing/null => interpreted as \"Nothing\"\n- If \"members\" is empty and \"exclusionary\" is False => interpreted as \"Everything\"\n- If values exist and \"exclusionary\" is True => treated as restricted subset\n\nParameters:\n    datamodel_name (str): Name of the DataModel to retrieve datasecurity rules for.\n\nReturns:\n    list: A list of dictionaries representing datasecurity rules in flat, share-resolved format.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel to retrieve datasecurity rules for."
        }
      },
      "required": [
        "datamodel_name"
      ]
    },
    "mutates": false,
    "tags": [
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "What are the detailed datasecurity rules for the DataModel named 'sales_performance_ec'?",
        "arguments": {
          "datamodel_name": "sales_performance_ec"
        },
        "notes": "This call retrieves detailed datasecurity rules for the 'sales_performance_ec' DataModel, including share-level visibility for each column."
      },
      {
        "user_query": "Can you show me the column-level datasecurity rules for the 'customer_data_live' DataModel?",
        "arguments": {
          "datamodel_name": "customer_data_live"
        },
        "notes": "Use this to get a detailed view of the datasecurity rules applied to the 'customer_data_live' DataModel, including which shares have access to specific columns."
      },
      {
        "user_query": "I need to review the access rules for the 'financial_analytics' DataModel. Can you provide the details?",
        "arguments": {
          "datamodel_name": "financial_analytics"
        },
        "notes": "This example fetches all detailed datasecurity rules for the 'financial_analytics' DataModel, showing column-level restrictions and share-specific visibility."
      }
    ]
  },
  {
    "tool_id": "datamodel.get_model_schema",
    "module": "datamodel",
    "class": "DataModel",
    "method": "get_model_schema",
    "description": "Retrieves the schema of a DataModel, including tables and columns.",
    "full_doc": "Retrieves the schema of a DataModel, including tables and columns.\n\nParameters:\n    datamodel_name (str): Name of the DataModel to retrieve the schema for.\n\nReturns:\n    list: A list of dictionaries containing schema information (one per column),\n        or an error message if not found.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel to retrieve the schema for."
        }
      },
      "required": [
        "datamodel_name"
      ]
    },
    "mutates": false,
    "tags": [
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "What is the schema for the DataModel named 'sales_performance_2023'?",
        "arguments": {
          "datamodel_name": "sales_performance_2023"
        },
        "notes": "This retrieves the schema for the 'sales_performance_2023' DataModel, including all tables and columns. Use this to understand the structure of the DataModel."
      },
      {
        "user_query": "Can you show me the tables and columns in the 'customer_insights' DataModel?",
        "arguments": {
          "datamodel_name": "customer_insights"
        },
        "notes": "This call fetches the schema of the 'customer_insights' DataModel. It's useful for reviewing the available data structure before querying or modifying the model."
      },
      {
        "user_query": "I need to see the schema details for the 'marketing_analytics' DataModel.",
        "arguments": {
          "datamodel_name": "marketing_analytics"
        },
        "notes": "This retrieves the schema for the 'marketing_analytics' DataModel. Use this to verify the tables and columns available in the model."
      }
    ]
  },
  {
    "tool_id": "datamodel.get_row_count",
    "module": "datamodel",
    "class": "DataModel",
    "method": "get_row_count",
    "description": "Retrieves the row count for each table in a specific DataModel",
    "full_doc": "Retrieves the row count for each table in a specific DataModel\nand returns it in a flat row-based structure suitable for tabular representation.\n\nParameters:\n    datamodel_name (str): Name of the DataModel.\n\nReturns:\n    list: List of dictionaries, each with 'table_name' and 'row_count'.\n        Includes an additional row for total row count.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel."
        }
      },
      "required": [
        "datamodel_name"
      ]
    },
    "mutates": false,
    "tags": [
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "How many rows are in each table of the DataModel named 'sales_performance_ec'?",
        "arguments": {
          "datamodel_name": "sales_performance_ec"
        },
        "notes": "This retrieves the row count for all tables in the 'sales_performance_ec' DataModel, useful for understanding table sizes."
      },
      {
        "user_query": "Get the row count for all tables in the 'customer_insights_live' DataModel.",
        "arguments": {
          "datamodel_name": "customer_insights_live"
        },
        "notes": "This call is used to fetch the row count for each table in the 'customer_insights_live' DataModel, which is a live model."
      },
      {
        "user_query": "Can you provide the row counts for the tables in the 'marketing_analytics_ec' DataModel?",
        "arguments": {
          "datamodel_name": "marketing_analytics_ec"
        },
        "notes": "This retrieves the row count for each table in the 'marketing_analytics_ec' DataModel, helpful for monitoring data growth or validating data ingestion."
      }
    ]
  },
  {
    "tool_id": "datamodel.get_table_schema",
    "module": "datamodel",
    "class": "DataModel",
    "method": "get_table_schema",
    "description": "Retrieves the schema of a table in a specified connection from Data Source.",
    "full_doc": "Retrieves the schema of a table in a specified connection from Data Source.\nThis method uses an undocumented Sisense API endpoint to fetch the table schema details.\nNOTE: This endpoint is undocumented and may change in future versions of Sisense.\nIt is recommended to use this method with caution.\n\nParameters:\n    connection_name (str): Name of the connection.\n    database_name (str): Name of the database.\n    schema_name (str): Name of the schema.\n    table_name (str): Name of the table.\n\nReturns:\n    dict: Table schema details if found, or a dictionary with an error message.",
    "parameters": {
      "type": "object",
      "properties": {
        "connection_name": {
          "type": "string",
          "description": "Name of the connection."
        },
        "database_name": {
          "type": "string",
          "description": "Name of the database."
        },
        "schema_name": {
          "type": "string",
          "description": "Name of the schema."
        },
        "table_name": {
          "type": "string",
          "description": "Name of the table."
        }
      },
      "required": [
        "connection_name",
        "database_name",
        "schema_name",
        "table_name"
      ]
    },
    "mutates": false,
    "tags": [
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "What is the schema of the 'sales_data' table in the 'analytics' database under the 'public' schema for the 'main_connection' connection?",
        "arguments": {
          "connection_name": "main_connection",
          "database_name": "analytics",
          "schema_name": "public",
          "table_name": "sales_data"
        },
        "notes": "This retrieves the schema details for the 'sales_data' table, including column names, types, and other metadata. Useful for understanding the structure of the table before querying or integrating it into a DataModel."
      },
      {
        "user_query": "Can you fetch the schema for the 'customer_orders' table in the 'ecommerce' database, within the 'orders' schema, using the 'ecommerce_connection'?",
        "arguments": {
          "connection_name": "ecommerce_connection",
          "database_name": "ecommerce",
          "schema_name": "orders",
          "table_name": "customer_orders"
        },
        "notes": "This call is used to get detailed information about the 'customer_orders' table, which is part of the 'orders' schema in the 'ecommerce' database. This is helpful for verifying table structure before creating a dataset or table in a DataModel."
      },
      {
        "user_query": "What columns and data types are in the 'inventory' table in the 'warehouse' database under the 'supply_chain' schema for the 'supply_chain_connection'?",
        "arguments": {
          "connection_name": "supply_chain_connection",
          "database_name": "warehouse",
          "schema_name": "supply_chain",
          "table_name": "inventory"
        },
        "notes": "This retrieves the schema for the 'inventory' table, providing details such as column names, data types, and sizes. This is typically used to confirm the table's structure before adding it to a Sisense DataModel."
      }
    ]
  },
  {
    "tool_id": "datamodel.resolve_datamodel_reference",
    "module": "datamodel",
    "class": "DataModel",
    "method": "resolve_datamodel_reference",
    "description": "Resolve a data model reference (ID or title) to a concrete data model ID and title.",
    "full_doc": "Resolve a data model reference (ID or title) to a concrete data model ID and title.\n\nThis helper accepts a single string that may be either:\n- a Sisense data model ID, or\n- a data model title (schema name).\n\nIt first attempts to treat the reference as an ID using\n`/api/v2/datamodels/{id}/schema`. If that fails, it falls back to\ncalling `/api/v2/datamodels/schema` with a `title` query parameter.\n\nParameters\n----------\ndatamodel_ref : str\n    Data model reference to resolve. This can be either an ID or a name.\n\nReturns\n-------\ndict\n    A dictionary with the following keys:\n    - success (bool): True if the reference was resolved to a data model.\n    - status_code (int): 200 if resolved successfully, 404 if not found,\n      or 500 if an unexpected error occurred.\n    - datamodel_id (str or None): Resolved data model ID (oid) if found,\n      otherwise None.\n    - datamodel_title (str or None): Resolved data model title if found,\n      otherwise None.\n    - error (str or None): Error message if success is False, otherwise None.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_ref": {
          "type": "string",
          "description": "Data model reference to resolve. This can be either an ID or a name."
        }
      },
      "required": [
        "datamodel_ref"
      ]
    },
    "mutates": false,
    "tags": [
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "I have a data model ID '123e4567-e89b-12d3-a456-426614174000'. Can you resolve it to its name?",
        "arguments": {
          "datamodel_ref": "123e4567-e89b-12d3-a456-426614174000"
        },
        "notes": "This call resolves the provided data model ID to its corresponding title. Use this when you have the ID but need the name for further operations."
      },
      {
        "user_query": "I need to find the ID of the data model titled 'SalesAnalytics'.",
        "arguments": {
          "datamodel_ref": "SalesAnalytics"
        },
        "notes": "This call resolves the provided data model title to its corresponding ID. Use this when you have the name but need the ID for API calls."
      },
      {
        "user_query": "Can you confirm the ID and title for the reference 'MarketingCube'?",
        "arguments": {
          "datamodel_ref": "MarketingCube"
        },
        "notes": "This call checks whether the reference 'MarketingCube' corresponds to a valid data model and retrieves its ID and title. Useful for verifying references before performing actions."
      }
    ]
  },
  {
    "tool_id": "datamodel.setup_datamodel",
    "module": "datamodel",
    "class": "DataModel",
    "method": "setup_datamodel",
    "description": "Setup a DataModel using existing connection and by creating a datamodel, dataset, and table.",
    "full_doc": "Setup a DataModel using existing connection and by creating a datamodel, dataset, and table.\n\nParameters:\n    datamodel_name (str): Name of the DataModel.\n    datamodel_type (str): Type of DataModel. Should be either \"extract\" (for Elasticube) or \"live\" (for Live).\n    connection_name (str): Name of the connection to use.\n    database_name (str): Name of the data source database.\n    schema_name (str): Name of the data source schema.\n    dataset_name (str, optional): Name of the dataset. Defaults to schema name if not provided.\n    tables (list): List of tables to create in the DataModel.\n        Each table should be a dictionary with keys:\n        \"table_name\", \"import_query\", \"description\", \"tags\", and \"build_behavior_config\".\n        import_query (str, optional): SQL statement used as custom import query. Defaults to None.\n        description (str, optional): Description for the table. Defaults to an empty string.\n        tags (list, optional): List of tags to apply to the table. Defaults to None.\n        build_behavior_config (dict, optional): Configuration for table build behavior.\n\nReturns:\n    dict: A dictionary containing the full DataModel object on success or an error message on failure.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_name": {
          "type": "string",
          "description": "Name of the DataModel."
        },
        "datamodel_type": {
          "type": "string",
          "description": "Type of DataModel. Should be either \"extract\" (for Elasticube) or \"live\" (for Live).",
          "enum": [
            "extract",
            "live"
          ],
          "x-aliases": {
            "extract": [
              "ec",
              "elasticube",
              "elastic cube",
              "cube",
              "elastic-cube"
            ],
            "live": [
              "realtime",
              "real-time",
              "live model"
            ]
          }
        },
        "connection_name": {
          "type": "string",
          "description": "Name of the connection to use."
        },
        "database_name": {
          "type": "string",
          "description": "Name of the data source database."
        },
        "schema_name": {
          "type": "string",
          "description": "Name of the data source schema."
        },
        "tables": {
          "type": "array",
          "description": "List of tables to add. For 'live' models, build_behavior_config is ignored. For 'extract' models, set build_behavior_config as needed.",
          "items": {
            "type": "object",
            "properties": {
              "database_name": {
                "type": "string",
                "description": "Optional override of the table's database. Defaults to top-level database_name if omitted."
              },
              "schema_name": {
                "type": "string",
                "description": "Optional override of the table's schema. Defaults to top-level schema_name if omitted."
              },
              "table_name": {
                "type": "string",
                "description": "Physical table name to add, or a logical name when using import_query."
              },
              "import_query": {
                "type": "string",
                "description": "Optional custom SQL to import rows (e.g., SELECT ... LIMIT 10). If provided, supersedes physical table fetch."
              },
              "description": {
                "type": "string",
                "description": "Optional table description."
              },
              "tags": {
                "type": "array",
                "description": "Optional list of tags for the table.",
                "items": {
                  "type": "string"
                }
              },
              "build_behavior_config": {
                "type": "object",
                "description": "Only used for 'extract' models. Ignored for 'live'. For 'increment' mode, column_name is required.",
                "properties": {
                  "mode": {
                    "type": "string",
                    "enum": [
                      "replace",
                      "replace_changes",
                      "append",
                      "increment"
                    ],
                    "description": "Table build behavior for extract models."
                  },
                  "column_name": {
                    "type": "string",
                    "description": "Required when mode='increment'; ignored otherwise."
                  }
                }
              }
            },
            "required": [
              "table_name"
            ]
          },
          "minItems": 1
        },
        "dataset_name": {
          "type": "string",
          "description": "Name of the dataset. Defaults to schema name if not provided."
        },
        "row_limit": {
          "type": "integer",
          "minimum": 1
        }
      },
      "required": [
        "datamodel_name",
        "datamodel_type",
        "connection_name",
        "database_name",
        "schema_name",
        "tables"
      ]
    },
    "mutates": true,
    "tags": [
      "datamodel",
      "write"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "Create a live DataModel for analyzing NYC taxi data with two tables.",
        "arguments": {
          "datamodel_name": "NYCTaxi_LiveModel",
          "datamodel_type": "live",
          "connection_name": "nyc_taxi_connection",
          "database_name": "nyc_data",
          "schema_name": "public",
          "dataset_name": "NYCTaxiDataset",
          "tables": [
            {
              "database_name": "nyc_data",
              "schema_name": "public",
              "table_name": "trips",
              "import_query": "SELECT * FROM public.trips WHERE trip_distance > 0",
              "description": "Taxi trips data for analysis",
              "tags": [
                "trips",
                "live"
              ],
              "build_behavior_config": {}
            },
            {
              "database_name": "nyc_data",
              "schema_name": "public",
              "table_name": "drivers",
              "description": "Driver information",
              "tags": [
                "drivers",
                "dimension"
              ],
              "build_behavior_config": {}
            }
          ]
        },
        "notes": "This call sets up a live DataModel for NYC taxi data with two tables: trips and drivers. Use this for real-time analysis."
      },
      {
        "user_query": "Create an Elasticube DataModel for sales data with incremental updates.",
        "arguments": {
          "datamodel_name": "Sales_Elasticube",
          "datamodel_type": "extract",
          "connection_name": "sales_db_connection",
          "database_name": "sales_data",
          "schema_name": "analytics",
          "dataset_name": "SalesDataset",
          "tables": [
            {
              "database_name": "sales_data",
              "schema_name": "analytics",
              "table_name": "transactions",
              "import_query": "SELECT * FROM analytics.transactions WHERE transaction_date >= '2023-01-01'",
              "description": "Sales transactions data",
              "tags": [
                "transactions",
                "fact"
              ],
              "build_behavior_config": {
                "mode": "increment",
                "column_name": "transaction_date"
              }
            },
            {
              "database_name": "sales_data",
              "schema_name": "analytics",
              "table_name": "products",
              "description": "Product catalog",
              "tags": [
                "products",
                "dimension"
              ],
              "build_behavior_config": {
                "mode": "replace"
              }
            }
          ]
        },
        "notes": "This call creates an Elasticube DataModel for sales data with incremental updates for the transactions table and a replace mode for the products table."
      },
      {
        "user_query": "Set up a live DataModel for customer and order data from a PostgreSQL database.",
        "arguments": {
          "datamodel_name": "CustomerOrders_Live",
          "datamodel_type": "live",
          "connection_name": "postgres_connection",
          "database_name": "customer_orders",
          "schema_name": "public",
          "dataset_name": "CustomerOrdersDataset",
          "tables": [
            {
              "database_name": "customer_orders",
              "schema_name": "public",
              "table_name": "customers",
              "description": "Customer details",
              "tags": [
                "customers",
                "dimension"
              ],
              "build_behavior_config": {}
            },
            {
              "database_name": "customer_orders",
              "schema_name": "public",
              "table_name": "orders",
              "import_query": "SELECT * FROM public.orders WHERE order_status = 'completed'",
              "description": "Completed orders data",
              "tags": [
                "orders",
                "fact"
              ],
              "build_behavior_config": {}
            }
          ]
        },
        "notes": "This call sets up a live DataModel for customer and order data, filtering completed orders for real-time reporting."
      }
    ]
  },
  {
    "tool_id": "dashboard.add_dashboard_script",
    "module": "dashboard",
    "class": "Dashboard",
    "method": "add_dashboard_script",
    "description": "Adds or overwrites a script to a dashboard, temporarily changing ownership if required.",
    "full_doc": "Adds or overwrites a script to a dashboard, temporarily changing ownership if required.\n\nParameters:\n    dashboard_id (str): The ID of the dashboard where the script will be added.\n    script (str): The JavaScript script as either:\n                - A properly formatted JSON string.\n                - A raw Python docstring (multi-line string).\n    executing_user (str, optional): The username of the API user. This is used to temporarily change\n                                the owner of the dashboard, as only the owner can add scripts.\n                                If not provided, assumes the dashboard owner is the same as the API user.\n\nReturns:\n    str: Success message or error details.",
    "parameters": {
      "type": "object",
      "properties": {
        "dashboard_id": {
          "type": "string",
          "description": "The ID of the dashboard where the script will be added."
        },
        "script": {
          "type": "string",
          "description": "The JavaScript script as either: - A properly formatted JSON string. - A raw Python docstring (multi-line string)."
        },
        "executing_user": {
          "type": "string",
          "description": "The username of the API user. This is used to temporarily change the owner of the dashboard, as only the owner can add scripts. If not provided, assumes the dashboard owner is the same as the API user."
        }
      },
      "required": [
        "dashboard_id",
        "script"
      ]
    },
    "mutates": true,
    "tags": [
      "dashboard",
      "dashboards",
      "write"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "How can I add a custom script to change the background color of my dashboard?",
        "arguments": {
          "dashboard_id": "12345abcde67890fghij",
          "script": "dashboard.on('ready', function() { document.body.style.backgroundColor = '#f0f0f0'; });",
          "executing_user": "admin@sisense.com"
        },
        "notes": "This call adds a script to change the dashboard's background color to light gray. Useful for customizing the dashboard's appearance."
      },
      {
        "user_query": "I need to add a script that modifies the layout of my dashboard, but I don't want to specify an executing user.",
        "arguments": {
          "dashboard_id": "67890fghij12345abcde",
          "script": "dashboard.on('widgetready', function() { $('.dashboard-layout').css('padding', '20px'); });"
        },
        "notes": "This call adds a script to adjust the padding of the dashboard layout. The executing user is not specified, so it assumes the API user is the dashboard owner."
      },
      {
        "user_query": "Can I add a script to highlight specific widgets in my dashboard?",
        "arguments": {
          "dashboard_id": "abcdef1234567890ghij",
          "script": "dashboard.on('widgetready', function(widget) { if (widget.title === 'Sales Data') { widget.element.style.border = '2px solid red'; } });",
          "executing_user": "developer@sisense.com"
        },
        "notes": "This call adds a script to highlight widgets with the title 'Sales Data' by adding a red border. Useful for drawing attention to specific widgets."
      }
    ]
  },
  {
    "tool_id": "dashboard.add_dashboard_shares",
    "module": "dashboard",
    "class": "Dashboard",
    "method": "add_dashboard_shares",
    "description": "Adds or updates shares for a dashboard, specifying users and groups along with their access rules.",
    "full_doc": "Adds or updates shares for a dashboard, specifying users and groups along with their access rules.\n\nParameters:\n    dashboard_id (str): The ID of the dashboard to which the shares will be added.\n    shares (list of dicts): A list of dictionaries, each containing:\n        - \"name\" (str): The username or group name.\n        - \"type\" (str): Either \"user\" or \"group\" to indicate the share type.\n        - \"rule\" (str): The access level (e.g., \"view\", \"edit\").\n\nReturns:\n    str: Success message or error details.",
    "parameters": {
      "type": "object",
      "properties": {
        "dashboard_id": {
          "type": "string",
          "description": "The ID of the dashboard to which the shares will be added."
        },
        "shares": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "A list of dictionaries, each containing: - \"name\" (str): The username or group name. - \"type\" (str): Either \"user\" or \"group\" to indicate the share type. - \"rule\" (str): The access level (e.g., \"view\", \"edit\")."
        }
      },
      "required": [
        "dashboard_id",
        "shares"
      ]
    },
    "mutates": true,
    "tags": [
      "dashboard",
      "dashboards",
      "write"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "Share the dashboard with ID '12345abcde' with a user and a group, giving the user edit access and the group view access.",
        "arguments": {
          "dashboard_id": "12345abcde",
          "shares": [
            {
              "name": "jane.doe@example.com",
              "type": "user",
              "rule": "edit"
            },
            {
              "name": "sales_team",
              "type": "group",
              "rule": "view"
            }
          ]
        },
        "notes": "This call shares the specified dashboard with a user and a group, assigning different access levels to each."
      },
      {
        "user_query": "Grant view access to the dashboard '67890fghij' for two users and one group.",
        "arguments": {
          "dashboard_id": "67890fghij",
          "shares": [
            {
              "name": "john.smith@example.com",
              "type": "user",
              "rule": "view"
            },
            {
              "name": "mary.jones@example.com",
              "type": "user",
              "rule": "view"
            },
            {
              "name": "marketing_team",
              "type": "group",
              "rule": "view"
            }
          ]
        },
        "notes": "This call ensures that two users and one group have view-only access to the specified dashboard."
      },
      {
        "user_query": "Update the sharing settings for dashboard 'abc123xyz' to give edit access to a group and view access to a user.",
        "arguments": {
          "dashboard_id": "abc123xyz",
          "shares": [
            {
              "name": "engineering_team",
              "type": "group",
              "rule": "edit"
            },
            {
              "name": "viewer@example.com",
              "type": "user",
              "rule": "view"
            }
          ]
        },
        "notes": "This call modifies the sharing settings for the dashboard, granting edit access to a group and view access to a specific user."
      }
    ]
  },
  {
    "tool_id": "dashboard.add_widget_script",
    "module": "dashboard",
    "class": "Dashboard",
    "method": "add_widget_script",
    "description": "Adds or overwrites a script for a specific widget within a dashboard.",
    "full_doc": "Adds or overwrites a script for a specific widget within a dashboard.\n\nIf required, temporarily changes the dashboard ownership, as only the owner can modify widget scripts.\n\nParameters:\n    dashboard_id (str): The ID of the dashboard containing the widget.\n    widget_id (str): The ID of the widget where the script will be added.\n    script (str): The JavaScript script as either:\n                - A properly formatted JSON string.\n                - A raw Python docstring (multi-line string).\n    executing_user (str, optional): The username of the API user. This is used to temporarily change\n                                the owner of the dashboard, as only the owner can add scripts.\n                                If not provided, assumes the dashboard owner is the same as the API user.\n\nReturns:\n    str: Success message or error details.",
    "parameters": {
      "type": "object",
      "properties": {
        "dashboard_id": {
          "type": "string",
          "description": "The ID of the dashboard containing the widget."
        },
        "widget_id": {
          "type": "string",
          "description": "The ID of the widget where the script will be added."
        },
        "script": {
          "type": "string",
          "description": "The JavaScript script as either: - A properly formatted JSON string. - A raw Python docstring (multi-line string)."
        },
        "executing_user": {
          "type": "string",
          "description": "The username of the API user. This is used to temporarily change the owner of the dashboard, as only the owner can add scripts. If not provided, assumes the dashboard owner is the same as the API user."
        }
      },
      "required": [
        "dashboard_id",
        "widget_id",
        "script"
      ]
    },
    "mutates": true,
    "tags": [
      "dashboard",
      "write"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "How can I add a custom script to modify the legend layout of a specific widget in my dashboard?",
        "arguments": {
          "dashboard_id": "67dc928ae72ce30033bc6680",
          "widget_id": "67dc929be72ce30033bc6682",
          "script": "widget.on('beforeviewloaded', function(se, ev){\n    var legendAlign = 'left',\n         verticalAlign= 'top',\n        layout =  'horizontal',\n        x = 0,\n        y = 0;\n    var legend = ev.options.legend;\n    legend.align = legendAlign;\n    legend.verticalAlign = verticalAlign;\n    legend.layout = layout;\n    legend.x = x;\n    legend.y = y;\n});",
          "executing_user": "admin@sisense.com"
        },
        "notes": "This call adds a script to a specific widget in the dashboard to customize the legend layout, including alignment and positioning."
      },
      {
        "user_query": "I want to add a script to a widget that changes the background color of the widget when it loads. How can I do that?",
        "arguments": {
          "dashboard_id": "85dc1234abcd5678efgh9012",
          "widget_id": "85dc5678abcd1234ijkl3456",
          "script": "widget.on('ready', function(){\n    $('#widget-container').css('background-color', '#ffcc00');\n});",
          "executing_user": "developer@sisense.com"
        },
        "notes": "This example demonstrates how to add a script to a widget that changes its background color to yellow when it loads. Useful for customizing the widget's appearance."
      },
      {
        "user_query": "Can I add a script to a widget that displays a custom tooltip when hovering over a data point?",
        "arguments": {
          "dashboard_id": "90ab1234cdef5678ghij9012",
          "widget_id": "90ab5678cdef1234klmn3456",
          "script": "widget.on('datapointhover', function(event, data){\n    var tooltipContent = 'Value: ' + data.value + '\\nCategory: ' + data.category;\n    $('#custom-tooltip').text(tooltipContent).show();\n});",
          "executing_user": "analyst@sisense.com"
        },
        "notes": "This call adds a script to a widget that displays a custom tooltip with data point details when hovering over a chart. Ideal for enhancing data visualization."
      }
    ]
  },
  {
    "tool_id": "dashboard.get_all_dashboards",
    "module": "dashboard",
    "class": "Dashboard",
    "method": "get_all_dashboards",
    "description": "Retrieves all dashboards from the Sisense server.",
    "full_doc": "Retrieves all dashboards from the Sisense server.\n\nReturns:\n    list or dict: A list of dashboards if successful,\n                or a dict containing an error message.",
    "parameters": {
      "type": "object",
      "properties": {},
      "required": []
    },
    "mutates": false,
    "tags": [
      "dashboard",
      "dashboards",
      "read"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "How can I retrieve all dashboards available on the Sisense server?",
        "arguments": {},
        "notes": "This call fetches all dashboards accessible to the authenticated user. Use it to get a complete list of dashboards for analysis or further operations."
      },
      {
        "user_query": "I need a list of all dashboards to export their metadata to a CSV file. How do I do that?",
        "arguments": {},
        "notes": "This retrieves all dashboards from the server. The response can be processed to generate a CSV file or used for reporting purposes."
      },
      {
        "user_query": "Can I get a full list of dashboards to check which ones are shared with specific users?",
        "arguments": {},
        "notes": "This retrieves all dashboards, which can then be filtered or analyzed to check sharing settings or other metadata."
      }
    ]
  },
  {
    "tool_id": "dashboard.get_dashboard_by_id",
    "module": "dashboard",
    "class": "Dashboard",
    "method": "get_dashboard_by_id",
    "description": "Retrieves a specific dashboard by its ID.",
    "full_doc": "Retrieves a specific dashboard by its ID.\n\nParameters:\n    dashboard_id (str): The ID of the dashboard to retrieve.\n\nReturns:\n    dict: A dictionary containing dashboard details if found,\n        or a dict with an error message if the request fails.",
    "parameters": {
      "type": "object",
      "properties": {
        "dashboard_id": {
          "type": "string",
          "description": "The ID of the dashboard to retrieve."
        }
      },
      "required": [
        "dashboard_id"
      ]
    },
    "mutates": false,
    "tags": [
      "dashboard",
      "dashboards",
      "read"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "Can you fetch the details of the dashboard with ID '12345abcde'?",
        "arguments": {
          "dashboard_id": "12345abcde"
        },
        "notes": "This retrieves the metadata and details of the dashboard with the specified ID. Use this when you know the exact ID of the dashboard you want to access."
      },
      {
        "user_query": "I need to get information about the dashboard with ID '67890fghij'.",
        "arguments": {
          "dashboard_id": "67890fghij"
        },
        "notes": "This call fetches the details of the dashboard identified by the given ID. Useful for retrieving specific dashboard configurations or metadata."
      },
      {
        "user_query": "What are the details of the dashboard with ID 'abc123xyz'?",
        "arguments": {
          "dashboard_id": "abc123xyz"
        },
        "notes": "This retrieves the details of the dashboard associated with the provided ID. Ideal for scenarios where you need to inspect or validate a specific dashboard's properties."
      }
    ]
  },
  {
    "tool_id": "dashboard.get_dashboard_by_name",
    "module": "dashboard",
    "class": "Dashboard",
    "method": "get_dashboard_by_name",
    "description": "Retrieves a specific dashboard by its name.",
    "full_doc": "Retrieves a specific dashboard by its name.\n\nParameters:\n    dashboard_name (str): The name of the dashboard to retrieve.\n\nReturns:\n    dict or list: A dictionary containing dashboard details if found,\n                or {'error': 'message'} if not found or failed.",
    "parameters": {
      "type": "object",
      "properties": {
        "dashboard_name": {
          "type": "string",
          "description": "The name of the dashboard to retrieve."
        }
      },
      "required": [
        "dashboard_name"
      ]
    },
    "mutates": false,
    "tags": [
      "dashboard",
      "dashboards",
      "read"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "Find the dashboard named 'Sales Performance Q3' and retrieve its details.",
        "arguments": {
          "dashboard_name": "Sales Performance Q3"
        },
        "notes": "This call retrieves metadata and details for the 'Sales Performance Q3' dashboard, such as its widgets, filters, and layout."
      },
      {
        "user_query": "I need to get the dashboard information for 'Marketing Campaign Analysis'.",
        "arguments": {
          "dashboard_name": "Marketing Campaign Analysis"
        },
        "notes": "Use this call to fetch the details of the 'Marketing Campaign Analysis' dashboard, which might include its structure and associated data sources."
      },
      {
        "user_query": "Retrieve the dashboard named 'Executive Overview' to verify its configuration.",
        "arguments": {
          "dashboard_name": "Executive Overview"
        },
        "notes": "This call is useful for confirming the setup and content of the 'Executive Overview' dashboard, ensuring it meets the required specifications."
      }
    ]
  },
  {
    "tool_id": "dashboard.get_dashboard_columns",
    "module": "dashboard",
    "class": "Dashboard",
    "method": "get_dashboard_columns",
    "description": "Retrieves columns from a specific dashboard, including both widget and filter-level columns.",
    "full_doc": "Retrieves columns from a specific dashboard, including both widget and filter-level columns.\n\nThis method:\n- Uses the `get_dashboard_by_name` method to fetch the dashboard.\n- Extracts columns from widgets and filters.\n- Deduplicates the final column list.\n\nParameters:\n    dashboard_name (str): The name of the dashboard to retrieve columns from.\n\nReturns:\n    list: A list of dictionaries containing distinct table and column information from the dashboard.",
    "parameters": {
      "type": "object",
      "properties": {
        "dashboard_name": {
          "type": "string",
          "description": "The name of the dashboard to retrieve columns from."
        }
      },
      "required": [
        "dashboard_name"
      ]
    },
    "mutates": false,
    "tags": [
      "dashboard",
      "dashboards",
      "read"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "What columns are used in the 'Sales Performance' dashboard?",
        "arguments": {
          "dashboard_name": "Sales Performance"
        },
        "notes": "This retrieves all columns, including widget and filter-level columns, from the 'Sales Performance' dashboard. Use this to analyze the data structure of the dashboard."
      },
      {
        "user_query": "Can you list the columns from the 'Marketing Analytics' dashboard?",
        "arguments": {
          "dashboard_name": "Marketing Analytics"
        },
        "notes": "This call extracts and deduplicates all columns from the 'Marketing Analytics' dashboard. Useful for understanding the data sources used in the dashboard."
      },
      {
        "user_query": "I need to know the columns in the 'Customer Insights' dashboard for a report. Can you help?",
        "arguments": {
          "dashboard_name": "Customer Insights"
        },
        "notes": "This retrieves a list of unique columns from the 'Customer Insights' dashboard, including those used in widgets and filters. Ideal for preparing reports or verifying data sources."
      }
    ]
  },
  {
    "tool_id": "dashboard.get_dashboard_share",
    "module": "dashboard",
    "class": "Dashboard",
    "method": "get_dashboard_share",
    "description": "Retrieves share details (users and groups) for a specific dashboard by title.",
    "full_doc": "Retrieves share details (users and groups) for a specific dashboard by title.\n\nParameters:\n    dashboard_name (str): The title of the dashboard to retrieve share information for.\n\nReturns:\n    list: A list of dictionaries containing share type (user or group), and share name (email or group name),\n        or an empty list if the dashboard is not found or has no shares.",
    "parameters": {
      "type": "object",
      "properties": {
        "dashboard_name": {
          "type": "string",
          "description": "The title of the dashboard to retrieve share information for."
        }
      },
      "required": [
        "dashboard_name"
      ]
    },
    "mutates": false,
    "tags": [
      "dashboard",
      "dashboards",
      "read"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "Who has access to the 'Sales Performance Dashboard'?",
        "arguments": {
          "dashboard_name": "Sales Performance Dashboard"
        },
        "notes": "This call retrieves the sharing details for the 'Sales Performance Dashboard', including users and groups with access."
      },
      {
        "user_query": "Can you show me the sharing settings for the 'Marketing Insights' dashboard?",
        "arguments": {
          "dashboard_name": "Marketing Insights"
        },
        "notes": "Use this call to inspect the sharing configuration for the 'Marketing Insights' dashboard, including permissions for users and groups."
      },
      {
        "user_query": "I need to check who can view or edit the 'Executive Summary' dashboard.",
        "arguments": {
          "dashboard_name": "Executive Summary"
        },
        "notes": "This call retrieves the list of users and groups who have been granted access to the 'Executive Summary' dashboard."
      }
    ]
  },
  {
    "tool_id": "dashboard.resolve_dashboard_reference",
    "module": "dashboard",
    "class": "Dashboard",
    "method": "resolve_dashboard_reference",
    "description": "Resolve a dashboard reference (ID or name) to a concrete dashboard ID and title.",
    "full_doc": "Resolve a dashboard reference (ID or name) to a concrete dashboard ID and title.\n\nThis helper accepts a single string that may be either:\n- a Sisense dashboard ID (24-character ID), or\n- a dashboard title (name).\n\nIt first attempts to treat the reference as an ID using\n`get_dashboard_by_id`. If that fails or the reference does not look\nlike an ID, it falls back to `get_dashboard_by_name`. The underlying\nmethods are reused as-is.\n\nParameters\n----------\ndashboard_ref : str\n    Dashboard reference to resolve. This can be either an ID or a name.\n\nReturns\n-------\ndict\n    A dictionary with the following keys:\n    - success (bool): True if the reference was resolved to a dashboard.\n    - status_code (int): 200 if resolved successfully, 404 if not found,\n      or 500 if an unexpected error occurred.\n    - dashboard_id (str or None): Resolved dashboard ID (oid) if found,\n      otherwise None.\n    - dashboard_title (str or None): Resolved dashboard title if found,\n      otherwise None.\n    - error (str or None): Error message if success is False, otherwise None.",
    "parameters": {
      "type": "object",
      "properties": {
        "dashboard_ref": {
          "type": "string",
          "description": "Dashboard reference to resolve. This can be either an ID or a name."
        }
      },
      "required": [
        "dashboard_ref"
      ]
    },
    "mutates": false,
    "tags": [
      "dashboard",
      "dashboards",
      "read"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "What is the ID and title of the dashboard named 'Sales Performance'?",
        "arguments": {
          "dashboard_ref": "Sales Performance"
        },
        "notes": "This resolves the dashboard name 'Sales Performance' to its corresponding ID and title. Use this when you know the dashboard name but need its ID for further API calls."
      },
      {
        "user_query": "I have a dashboard ID '64b7f2c9e4b0a123456789ab'. Can you confirm its title?",
        "arguments": {
          "dashboard_ref": "64b7f2c9e4b0a123456789ab"
        },
        "notes": "This resolves the dashboard ID '64b7f2c9e4b0a123456789ab' to its corresponding title. Use this when you have the ID but need to verify the dashboard's title."
      },
      {
        "user_query": "Find the details of the dashboard referenced as 'Marketing Overview'.",
        "arguments": {
          "dashboard_ref": "Marketing Overview"
        },
        "notes": "This resolves the dashboard name 'Marketing Overview' to its ID and title. Useful when you need to confirm the exact dashboard details for further operations."
      }
    ]
  },
  {
    "tool_id": "migration.migrate_all_dashboards",
    "module": "migration",
    "class": "Migration",
    "method": "migrate_all_dashboards",
    "description": "Migrates all dashboards from the source to the target environment in batches.",
    "full_doc": "Migrates all dashboards from the source to the target environment in batches.\n\nParameters:\n    action (str, optional): Determines how to handle existing dashboards in the target environment.\n                            Options:\n                            - 'skip': Skip existing dashboards in the target; new dashboards are processed\n                              normally, including shares and ownership.\n                            - 'overwrite': Overwrite existing dashboards; shares and ownership will not be\n                              migrated. If the dashboard already exists, shares will be retained, but the API\n                              user will be set as the new owner.\n                            - 'duplicate': Create a duplicate of existing dashboards without migrating shares\n                              or ownership.\n                            Default: None. Existing dashboards are skipped, and only new ones are migrated.\n                            Unless existing dashboards are different owners, shares will be migrated.\n                            **Note:** If an existing dashboard in the target environment has a different owner\n                             than the user's token running the SDK, the dashboard will be migrated with a new\n                             ID, and its shares and ownership will be migrated from the original source\n                             dashboard.\n    republish (bool, optional): Whether to republish dashboards after migration. Default: False.\n    migrate_share (bool, optional): Whether to migrate shares for the dashboards. If `True`, shares will be\n        migrated, and ownership migration will be controlled by the `change_ownership` parameter.\n        If `False`, both shares and ownership migration will be skipped. Default: False.\n    change_ownership (bool, optional): Whether to change ownership of the target dashboards.\n        Effective only if `migrate_share` is True. Default: False.\n    batch_size (int, optional): Number of dashboards to process in each batch. Default: 10.\n    sleep_time (int, optional): Time (in seconds) to sleep between batches. Default: 10 seconds.\n\nReturns:\n    dict: A summary of the migration results for all batches, containing lists of succeeded, skipped,\n        and failed dashboards.\n\nNotes:\n    - **Batch Processing**: Dashboards are processed in batches to avoid overloading the system.\n    - **Best Use Case**: This method is suitable when migrating all dashboards from a source to a\n      target environment.\n    - **Overwrite Action**: When using `overwrite`, shares and ownership will not be migrated.\n      If a dashboard already exists, the target dashboard will be overwritten, retaining its existing shares\n      but setting the API user as the new owner. Subsequent adjustments to shares and ownership will not be\n      supported in this mode.\n    - **Duplicate Action**: Creates duplicate dashboards without shares and ownership migration.\n    - **Skip Action**: Skips migration for existing dashboards, but new ones are processed normally.",
    "parameters": {
      "type": "object",
      "properties": {
        "action": {
          "type": "string",
          "description": "Determines how to handle existing dashboards in the target environment. Options: - 'skip': Skip existing dashboards in the target; new dashboards are processed normally, including shares and ownership. - 'overwrite': Overwrite existing dashboards; shares and ownership will not be migrated. If the dashboard already exists, shares will be retained, but the API user will be set as the new owner. - 'duplicate': Create a duplicate of existing dashboards without migrating shares or ownership. Default: None. Existing dashboards are skipped, and only new ones are migrated. Unless existing dashboards are different owners, shares will be migrated. **Note:** If an existing dashboard in the target environment has a different owner than the user's token running the SDK, the dashboard will be migrated with a new ID, and its shares and ownership will be migrated from the original source dashboard.",
          "enum": [
            "skip",
            "overwrite",
            "duplicate"
          ]
        },
        "republish": {
          "type": "boolean",
          "description": "Whether to republish dashboards after migration. Default: False."
        },
        "migrate_share": {
          "type": "boolean",
          "description": "Whether to migrate shares for the dashboards. If `True`, shares will be migrated, and ownership migration will be controlled by the `change_ownership` parameter. If `False`, both shares and ownership migration will be skipped. Default: False."
        },
        "change_ownership": {
          "type": "boolean",
          "description": "Whether to change ownership of the target dashboards. Effective only if `migrate_share` is True. Default: False."
        },
        "batch_size": {
          "type": "integer",
          "description": "Number of dashboards to process in each batch. Default: 10."
        },
        "sleep_time": {
          "type": "integer",
          "description": "Time (in seconds) to sleep between batches. Default: 10 seconds."
        }
      },
      "required": []
    },
    "mutates": true,
    "tags": [
      "migration",
      "dashboards",
      "write"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "Migrate all dashboards to the target environment, overwriting existing ones and republishing them.",
        "arguments": {
          "action": "overwrite",
          "republish": true,
          "migrate_share": false,
          "change_ownership": false,
          "batch_size": 10,
          "sleep_time": 10
        },
        "notes": "This call migrates all dashboards from the source to the target environment, overwriting any existing dashboards in the target. Shares and ownership are not migrated, but dashboards are republished after migration. Suitable for environments where dashboards need to be updated with new data or configurations."
      },
      {
        "user_query": "Migrate all dashboards to the target environment, skipping existing ones and migrating shares.",
        "arguments": {
          "action": "skip",
          "republish": false,
          "migrate_share": true,
          "change_ownership": true,
          "batch_size": 5,
          "sleep_time": 15
        },
        "notes": "This call migrates all dashboards from the source to the target environment, skipping dashboards that already exist in the target. Shares and ownership are migrated for new dashboards. This is ideal for environments where existing dashboards should remain untouched, but new dashboards need to be added with their associated shares."
      },
      {
        "user_query": "Duplicate all dashboards in the target environment without migrating shares or ownership.",
        "arguments": {
          "action": "duplicate",
          "republish": false,
          "migrate_share": false,
          "change_ownership": false,
          "batch_size": 20,
          "sleep_time": 5
        },
        "notes": "This call duplicates all dashboards from the source to the target environment, creating new copies without migrating shares or ownership. Useful for scenarios where dashboards need to be cloned for testing or development purposes without altering the original dashboards in the target environment."
      }
    ]
  },
  {
    "tool_id": "migration.migrate_all_datamodels",
    "module": "migration",
    "class": "Migration",
    "method": "migrate_all_datamodels",
    "description": "Migrates all data models from the source environment to the target environment in batches.",
    "full_doc": "Migrates all data models from the source environment to the target environment in batches.\n\nParameters:\n    dependencies (list, optional): A list of dependencies to include in the migration.\n        If not provided or if 'all' is passed, all dependencies are selected by default.\n        Possible values for `dependencies` are:\n        - \"dataSecurity\" (includes both Data Security and Scope Configuration)\n        - \"formulas\" (for Formulas)\n        - \"hierarchies\" (for Drill Hierarchies)\n        - \"perspectives\" (for Perspectives)\n        If left blank or set to \"all\", all dependencies are included by default.\n    shares (bool, optional): Whether to also migrate the data model's shares. Default is False.\n    batch_size (int, optional): Number of data models to migrate in each batch. Default is 10.\n    sleep_time (int, optional): Time in seconds to wait between processing batches. Default is 5 seconds.\n    action (str, optional): Strategy to handle existing data models in the target environment.\n        - \"overwrite\": Attempts to overwrite an existing model using its original ID via the\n          datamodelId parameter. If the model is not found in the target environment, it will\n          automatically fall back and create the model.\n        - \"duplicate\": Creates a new model by appending \" (Duplicate)\" to the original name.\n\nReturns:\n    dict: A summary of the migration results with lists of succeeded, skipped, and failed data models.",
    "parameters": {
      "type": "object",
      "properties": {
        "dependencies": {
          "type": "array",
          "items": {
            "type": "string",
            "enum": [
              "dataSecurity",
              "formulas",
              "hierarchies",
              "perspectives"
            ]
          },
          "description": "A list of dependencies to include in the migration. If not provided or if 'all' is passed, all dependencies are selected by default. Possible values for `dependencies` are: - \"dataSecurity\" (includes both Data Security and Scope Configuration) - \"formulas\" (for Formulas) - \"hierarchies\" (for Drill Hierarchies) - \"perspectives\" (for Perspectives) If left blank or set to \"all\", all dependencies are included by default."
        },
        "shares": {
          "type": "boolean",
          "description": "Whether to also migrate the data model's shares. Default is False."
        },
        "batch_size": {
          "type": "integer",
          "description": "Number of data models to migrate in each batch. Default is 10."
        },
        "sleep_time": {
          "type": "integer",
          "description": "Time in seconds to wait between processing batches. Default is 5 seconds."
        },
        "action": {
          "type": "string",
          "description": "Strategy to handle existing data models in the target environment. - \"overwrite\": Attempts to overwrite an existing model using its original ID via the datamodelId parameter. If the model is not found in the target environment, it will automatically fall back and create the model. - \"duplicate\": Creates a new model by appending \" (Duplicate)\" to the original name.",
          "enum": [
            "overwrite",
            "duplicate"
          ]
        }
      },
      "required": []
    },
    "mutates": true,
    "tags": [
      "migration",
      "datamodel",
      "write"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "Migrate all data models with formulas and hierarchies, including shares, in batches of 5 models at a time.",
        "arguments": {
          "dependencies": [
            "formulas",
            "hierarchies"
          ],
          "shares": true,
          "batch_size": 5,
          "sleep_time": 10,
          "action": "overwrite"
        },
        "notes": "This call migrates all data models from the source environment to the target environment, including formulas and hierarchies, while also migrating shares. It processes 5 models per batch and waits 10 seconds between batches. Use this when you want to overwrite existing models in the target environment."
      },
      {
        "user_query": "Duplicate all data models with all dependencies, skipping shares, in batches of 10 models at a time.",
        "arguments": {
          "dependencies": [
            "dataSecurity",
            "formulas",
            "hierarchies",
            "perspectives"
          ],
          "shares": false,
          "batch_size": 10,
          "sleep_time": 5,
          "action": "duplicate"
        },
        "notes": "This call duplicates all data models from the source environment to the target environment, including all dependencies but excluding shares. It processes 10 models per batch and waits 5 seconds between batches. Use this when you want to create new copies of the models in the target environment."
      },
      {
        "user_query": "Migrate all data models with data security and perspectives, excluding shares, in batches of 20 models at a time.",
        "arguments": {
          "dependencies": [
            "dataSecurity",
            "perspectives"
          ],
          "shares": false,
          "batch_size": 20,
          "sleep_time": 15,
          "action": "overwrite"
        },
        "notes": "This call migrates all data models from the source environment to the target environment, including data security and perspectives, but excludes shares. It processes 20 models per batch and waits 15 seconds between batches. Use this when you want to overwrite existing models in the target environment with specific dependencies."
      }
    ]
  },
  {
    "tool_id": "migration.migrate_all_groups",
    "module": "migration",
    "class": "Migration",
    "method": "migrate_all_groups",
    "description": "Migrates all groups from the source environment to the target environment using the bulk endpoint.",
    "full_doc": "Migrates all groups from the source environment to the target environment using the bulk endpoint.\n\nReturns:\n    list: A list of group migration results, including any errors encountered during the process.",
    "parameters": {
      "type": "object",
      "properties": {},
      "required": []
    },
    "mutates": true,
    "tags": [
      "migration",
      "groups",
      "write"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "How can I migrate all user groups from my development environment to the production environment?",
        "arguments": {},
        "notes": "This call migrates all user groups from the source environment to the target environment using the bulk endpoint. Use this when you want to ensure all groups are synchronized between environments."
      },
      {
        "user_query": "I need to replicate all groups from one Sisense instance to another. How can I do that?",
        "arguments": {},
        "notes": "This operation transfers all groups from the source environment to the target environment. It is useful for bulk group migrations during environment setup or updates."
      },
      {
        "user_query": "Can I migrate every group from my staging environment to production in one go?",
        "arguments": {},
        "notes": "This method migrates all groups in a single operation. Ideal for scenarios where you want to ensure all groups are available in the target environment without specifying them individually."
      }
    ]
  },
  {
    "tool_id": "migration.migrate_all_users",
    "module": "migration",
    "class": "Migration",
    "method": "migrate_all_users",
    "description": "Migrates all users from the source environment to the target environment using the bulk endpoint.",
    "full_doc": "Migrates all users from the source environment to the target environment using the bulk endpoint.\n\nReturns:\n    list: A list of user migration results, including any errors encountered during the process.",
    "parameters": {
      "type": "object",
      "properties": {},
      "required": []
    },
    "mutates": true,
    "tags": [
      "migration",
      "users",
      "write"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "How can I migrate all users from my development environment to the production environment?",
        "arguments": {},
        "notes": "This call migrates all users from the source environment to the target environment using the bulk endpoint. Useful for full user migration during environment setup or updates."
      },
      {
        "user_query": "I need to ensure all users are copied over to the new Sisense instance. What should I do?",
        "arguments": {},
        "notes": "This operation transfers all users from the source environment to the target environment. Ideal for initial migrations or syncing environments."
      },
      {
        "user_query": "Can I bulk migrate all users to the target environment without specifying individual user details?",
        "arguments": {},
        "notes": "This method performs a bulk migration of all users without requiring specific user details. Recommended for comprehensive user migrations."
      }
    ]
  },
  {
    "tool_id": "migration.migrate_dashboard_shares",
    "module": "migration",
    "class": "Migration",
    "method": "migrate_dashboard_shares",
    "description": "Migrates shares for specific dashboards from the source to the target environment.",
    "full_doc": "Migrates shares for specific dashboards from the source to the target environment.\n\nParameters:\n    source_dashboard_ids (list): A list of dashboard IDs from the source environment to fetch shares from.\n    target_dashboard_ids (list): A list of dashboard IDs from the target environment to apply shares to.\n    change_ownership (bool, optional): Whether to change ownership of the target dashboard. Defaults to False.\n\nReturns:\n    dict: A summary of the share migration process with counts of succeeded and failed shares,\n        and details of failed dashboards.\n\nRaises:\n    ValueError: If `source_dashboard_ids` or `target_dashboard_ids` are not provided,\n                or if their lengths do not match.",
    "parameters": {
      "type": "object",
      "properties": {
        "source_dashboard_ids": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "A list of dashboard IDs from the source environment to fetch shares from."
        },
        "target_dashboard_ids": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "A list of dashboard IDs from the target environment to apply shares to."
        },
        "change_ownership": {
          "type": "boolean",
          "description": "Whether to change ownership of the target dashboard. Defaults to False."
        }
      },
      "required": [
        "source_dashboard_ids",
        "target_dashboard_ids"
      ]
    },
    "mutates": true,
    "tags": [
      "migration",
      "dashboards",
      "write"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "Migrate shares from dashboards in the source environment to corresponding dashboards in the target environment.",
        "arguments": {
          "source_dashboard_ids": [
            "12345abcde67890fghij",
            "54321zyxwv09876lkjih"
          ],
          "target_dashboard_ids": [
            "67890fghij12345abcde",
            "09876lkjih54321zyxwv"
          ],
          "change_ownership": false
        },
        "notes": "This call migrates shares from two dashboards in the source environment to their counterparts in the target environment without changing ownership."
      },
      {
        "user_query": "Transfer shares and ownership for a set of dashboards from the source to the target environment.",
        "arguments": {
          "source_dashboard_ids": [
            "abc123def456ghi789jkl",
            "xyz987uvw654rst321opq"
          ],
          "target_dashboard_ids": [
            "ghi789jkl123abc456def",
            "rst321opq987xyz654uvw"
          ],
          "change_ownership": true
        },
        "notes": "Use this call when you need to migrate shares and also transfer ownership of the dashboards to the target environment."
      },
      {
        "user_query": "Migrate shares for a single dashboard from source to target without changing ownership.",
        "arguments": {
          "source_dashboard_ids": [
            "dashboard123source"
          ],
          "target_dashboard_ids": [
            "dashboard123target"
          ],
          "change_ownership": false
        },
        "notes": "This call is useful for migrating shares for a single dashboard while keeping the ownership unchanged."
      }
    ]
  },
  {
    "tool_id": "migration.migrate_dashboards",
    "module": "migration",
    "class": "Migration",
    "method": "migrate_dashboards",
    "description": "Migrates specific dashboards from the source to the target environment using the bulk endpoint.",
    "full_doc": "Migrates specific dashboards from the source to the target environment using the bulk endpoint.\n\nParameters:\n    dashboard_ids (list, optional): A list of dashboard IDs to migrate.\n        Either `dashboard_ids` or `dashboard_names` must be provided.\n    dashboard_names (list, optional): A list of dashboard names to migrate.\n        Either `dashboard_ids` or `dashboard_names` must be provided.\n    action (str, optional): Determines how to handle existing dashboards in the target environment.\n                            Options:\n                            - 'skip': Skip existing dashboards in the target; new dashboards are processed\n                              normally, including shares and ownership.\n                            - 'overwrite': Overwrite existing dashboards; shares and ownership will not be\n                              migrated. If the dashboard already exists, shares will be retained, but the API\n                              user will be set as the new owner.\n                            - 'duplicate': Create a duplicate of existing dashboards without migrating shares or\n                              ownership.\n                            Default: None. Existing dashboards are skipped, and only new ones are migrated.\n                            **Note:** If an existing dashboard in the target environment has a different owner\n                            than the user's token running the SDK, the dashboard will be migrated with a new ID,\n                            and its shares and ownership will be migrated from the original source dashboard.\n    republish (bool, optional): Whether to republish dashboards after migration. Default: False.\n    migrate_share (bool, optional): Whether to migrate shares for the dashboards. If `True`, shares will be\n        migrated, and ownership migration will be controlled by the `change_ownership` parameter.\n        If `False`, both shares and ownership migration will be skipped. Default: False.\n    change_ownership (bool, optional): Whether to change ownership of the target dashboards.\n        Effective only if `migrate_share` is True. Default: False.\n\nReturns:\n    dict: A summary of the migration results with lists of succeeded, skipped, and failed dashboards.\n\nNotes:\n    - **When `action` is not provided, existing dashboards in the target environment are skipped,\n      and only new dashboards are added.\n    - **Best Use Case**: Suitable when migrating dashboards for the first time to a target environment.\n    - **Overwrite Action:** When using `overwrite`, shares and ownership will not be migrated.\n      If a dashboard already exists, the target dashboard will be overwritten,\n      retaining its existing shares but setting the API user as the new owner.\n      Subsequent adjustments to shares and ownership will not be supported in this mode.\n    - **Duplicate Action**: Creates duplicate dashboards without shares and ownership migration.\n    - **Skip Action**: Skips migration for existing dashboards, but new ones are processed normally.",
    "parameters": {
      "type": "object",
      "properties": {
        "dashboard_ids": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "A list of dashboard IDs to migrate. Either `dashboard_ids` or `dashboard_names` must be provided."
        },
        "dashboard_names": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "A list of dashboard names to migrate. Either `dashboard_ids` or `dashboard_names` must be provided."
        },
        "action": {
          "type": "string",
          "description": "Determines how to handle existing dashboards in the target environment. Options: - 'skip': Skip existing dashboards in the target; new dashboards are processed normally, including shares and ownership. - 'overwrite': Overwrite existing dashboards; shares and ownership will not be migrated. If the dashboard already exists, shares will be retained, but the API user will be set as the new owner. - 'duplicate': Create a duplicate of existing dashboards without migrating shares or ownership. Default: None. Existing dashboards are skipped, and only new ones are migrated. **Note:** If an existing dashboard in the target environment has a different owner than the user's token running the SDK, the dashboard will be migrated with a new ID, and its shares and ownership will be migrated from the original source dashboard.",
          "enum": [
            "skip",
            "overwrite",
            "duplicate"
          ]
        },
        "republish": {
          "type": "boolean",
          "description": "Whether to republish dashboards after migration. Default: False."
        },
        "migrate_share": {
          "type": "boolean",
          "description": "Whether to migrate shares for the dashboards. If `True`, shares will be migrated, and ownership migration will be controlled by the `change_ownership` parameter. If `False`, both shares and ownership migration will be skipped. Default: False."
        },
        "change_ownership": {
          "type": "boolean",
          "description": "Whether to change ownership of the target dashboards. Effective only if `migrate_share` is True. Default: False."
        }
      },
      "required": []
    },
    "mutates": true,
    "tags": [
      "migration",
      "dashboards",
      "write"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "Migrate dashboards by their IDs and overwrite existing ones in the target environment.",
        "arguments": {
          "dashboard_ids": [
            "659583469a933c002adc8574",
            "6261702faae68e0036b1572b",
            "659abcde9a933c002adc1234"
          ],
          "action": "overwrite",
          "republish": false,
          "migrate_share": true,
          "change_ownership": false
        },
        "notes": "This call migrates dashboards by their IDs and overwrites any existing dashboards in the target environment. Shares will be migrated, but ownership will remain unchanged."
      },
      {
        "user_query": "Migrate dashboards by their names, skipping existing ones in the target environment.",
        "arguments": {
          "dashboard_names": [
            "Sales Performance Dashboard",
            "Marketing Analytics"
          ],
          "action": "skip",
          "republish": true,
          "migrate_share": false,
          "change_ownership": false
        },
        "notes": "This call migrates dashboards by their names and skips any existing dashboards in the target environment. New dashboards will be republished after migration, but shares and ownership will not be migrated."
      },
      {
        "user_query": "Duplicate dashboards by their IDs without migrating shares or ownership.",
        "arguments": {
          "dashboard_ids": [
            "659583469a933c002adc8574",
            "6261702faae68e0036b1572b"
          ],
          "action": "duplicate",
          "republish": false,
          "migrate_share": false,
          "change_ownership": false
        },
        "notes": "This call duplicates dashboards by their IDs in the target environment without migrating shares or ownership. Use this when you need a copy of the dashboards without affecting existing ones."
      }
    ]
  },
  {
    "tool_id": "migration.migrate_datamodels",
    "module": "migration",
    "class": "Migration",
    "method": "migrate_datamodels",
    "description": "Migrates specific data models from the source environment to the target environment.",
    "full_doc": "Migrates specific data models from the source environment to the target environment.\n\nParameters:\n    datamodel_ids (list, optional): A list of data model IDs to migrate.\n        Either `datamodel_ids` or `datamodel_names` must be provided.\n    datamodel_names (list, optional): A list of data model names to migrate.\n        Either `datamodel_ids` or `datamodel_names` must be provided.\n    provider_connection_map (dict, optional): A dictionary mapping provider names to connection IDs.\n        This allows specifying different connections per provider.\n        For example:\n        {\n            \"Databricks\": \"Connection ID\",\n            \"GoogleBigQuery\": \"Connection ID\"\n        }\n    dependencies (list, optional): A list of dependencies to include in the migration.\n        If not provided or if 'all' is passed, all dependencies are selected by default.\n                                Possible values for `dependencies` are:\n                                - \"dataSecurity\" (includes both Data Security and Scope Configuration)\n                                - \"formulas\" (for Formulas)\n                                - \"hierarchies\" (for Drill Hierarchies)\n                                - \"perspectives\" (for Perspectives)\n                                If left blank or set to \"all\", all dependencies are included by default.\n    shares (bool, optional): Whether to also migrate the data model's shares. Default is False.\n    action (str, optional): Strategy to handle existing data models in the target environment.\n        - \"overwrite\": Attempts to overwrite existing model using its original ID via the datamodelId parameter.\n          If the model is not found in target environment, it will automatically fall back and create the model.\n        - \"duplicate\": Creates a new model by passing a `new_title` to the `newTitle` parameter of the\n          import API endpoint. If `new_title` is not provided, the original title will be used with\n          \" (Duplicate)\" appended.\n    new_title (str, optional): New name for the duplicated data model. Used only when `action='duplicate'`.\n\nReturns:\n    dict: A summary of the migration results with lists of succeeded, skipped, and failed data models.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodel_ids": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "A list of data model IDs to migrate. Either `datamodel_ids` or `datamodel_names` must be provided."
        },
        "datamodel_names": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "A list of data model names to migrate. Either `datamodel_ids` or `datamodel_names` must be provided."
        },
        "provider_connection_map": {
          "type": "object",
          "description": "A dictionary mapping provider names to connection IDs. This allows specifying different connections per provider. For example: { \"Databricks\": \"Connection ID\", \"GoogleBigQuery\": \"Connection ID\" }"
        },
        "dependencies": {
          "type": "array",
          "items": {
            "type": "string",
            "enum": [
              "dataSecurity",
              "formulas",
              "hierarchies",
              "perspectives"
            ]
          },
          "description": "A list of dependencies to include in the migration. If not provided or if 'all' is passed, all dependencies are selected by default. Possible values for `dependencies` are: - \"dataSecurity\" (includes both Data Security and Scope Configuration) - \"formulas\" (for Formulas) - \"hierarchies\" (for Drill Hierarchies) - \"perspectives\" (for Perspectives) If left blank or set to \"all\", all dependencies are included by default."
        },
        "shares": {
          "type": "boolean",
          "description": "Whether to also migrate the data model's shares. Default is False."
        },
        "action": {
          "type": "string",
          "description": "Strategy to handle existing data models in the target environment. - \"overwrite\": Attempts to overwrite existing model using its original ID via the datamodelId parameter. If the model is not found in target environment, it will automatically fall back and create the model. - \"duplicate\": Creates a new model by passing a `new_title` to the `newTitle` parameter of the import API endpoint. If `new_title` is not provided, the original title will be used with \" (Duplicate)\" appended.",
          "enum": [
            "overwrite",
            "duplicate"
          ]
        },
        "new_title": {
          "type": "string",
          "description": "New name for the duplicated data model. Used only when `action='duplicate'`."
        }
      },
      "required": []
    },
    "mutates": true,
    "tags": [
      "migration",
      "datamodel",
      "write"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "Migrate specific data models by their IDs, including data security and formulas, and also migrate their shares.",
        "arguments": {
          "datamodel_ids": [
            "dddfac9f-86e6-4a9a-af28-f52106dcc55c",
            "97a41a88-3d8e-491d-8fe6-9eb111d86c57"
          ],
          "dependencies": [
            "dataSecurity",
            "formulas"
          ],
          "shares": true
        },
        "notes": "This call migrates two specific data models by their IDs, includes only the selected dependencies (data security and formulas), and migrates their shares."
      },
      {
        "user_query": "Migrate data models by their names, update the provider connections, and include all dependencies.",
        "arguments": {
          "datamodel_names": [
            "sales_data_model",
            "marketing_data_model"
          ],
          "provider_connection_map": {
            "Databricks": "53874a46-1360-45d8-a005-1cca41ef3e1c",
            "GoogleBigQuery": "5550c482-4e52-4b0f-afff-865e964c0df5"
          },
          "dependencies": "all",
          "shares": false
        },
        "notes": "This call migrates two data models by their names, updates the provider connections in the target environment, includes all dependencies, and skips migrating shares."
      },
      {
        "user_query": "Duplicate a data model with a new title, include hierarchies and perspectives, and skip migrating shares.",
        "arguments": {
          "datamodel_names": [
            "financial_analysis_model"
          ],
          "action": "duplicate",
          "new_title": "financial_analysis_model_v2",
          "dependencies": [
            "hierarchies",
            "perspectives"
          ],
          "shares": false
        },
        "notes": "This call duplicates a data model with a new title, includes only hierarchies and perspectives as dependencies, and skips migrating shares."
      }
    ]
  },
  {
    "tool_id": "migration.migrate_groups",
    "module": "migration",
    "class": "Migration",
    "method": "migrate_groups",
    "description": "Migrates specific groups from the source environment to the target environment using the bulk endpoint.",
    "full_doc": "Migrates specific groups from the source environment to the target environment using the bulk endpoint.\n\nParameters:\n    group_name_list (list): A list of group names to migrate.\n\nReturns:\n    list: A list of group migration results, including any errors encountered during the process.",
    "parameters": {
      "type": "object",
      "properties": {
        "group_name_list": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "A list of group names to migrate."
        }
      },
      "required": [
        "group_name_list"
      ]
    },
    "mutates": true,
    "tags": [
      "migration",
      "groups",
      "write"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "How can I migrate the 'Sales_Team' and 'Marketing_Team' groups from the source environment to the target?",
        "arguments": {
          "group_name_list": [
            "Sales_Team",
            "Marketing_Team"
          ]
        },
        "notes": "This call migrates the 'Sales_Team' and 'Marketing_Team' groups from the source environment to the target environment. Use this when you need to move specific groups."
      },
      {
        "user_query": "I need to migrate the 'Admin_Group' and 'Support_Team' groups to the target environment. How can I do that?",
        "arguments": {
          "group_name_list": [
            "Admin_Group",
            "Support_Team"
          ]
        },
        "notes": "This call migrates the 'Admin_Group' and 'Support_Team' groups to the target environment. Use this to selectively migrate specific groups."
      },
      {
        "user_query": "Can I migrate the 'Finance_Dept' group to the target environment?",
        "arguments": {
          "group_name_list": [
            "Finance_Dept"
          ]
        },
        "notes": "This call migrates the 'Finance_Dept' group from the source environment to the target environment. Use this for single-group migrations."
      }
    ]
  },
  {
    "tool_id": "migration.migrate_users",
    "module": "migration",
    "class": "Migration",
    "method": "migrate_users",
    "description": "Migrates specific users from the source environment to the target environment.",
    "full_doc": "Migrates specific users from the source environment to the target environment.\n\nParameters:\n    user_name_list (list): A list of user names to migrate.\n\nReturns:\n    list: A list of user migration results, including any errors encountered during the process.",
    "parameters": {
      "type": "object",
      "properties": {
        "user_name_list": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "A list of user names to migrate."
        }
      },
      "required": [
        "user_name_list"
      ]
    },
    "mutates": true,
    "tags": [
      "migration",
      "users",
      "write"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "Migrate specific users from the source environment to the target environment.",
        "arguments": {
          "user_name_list": [
            "alice.smith@example.com",
            "bob.jones@example.com"
          ]
        },
        "notes": "This call migrates the specified users, Alice Smith and Bob Jones, from the source environment to the target environment. Use this when you need to migrate a specific subset of users."
      },
      {
        "user_query": "Transfer a single user to the target environment for testing purposes.",
        "arguments": {
          "user_name_list": [
            "test.user@example.com"
          ]
        },
        "notes": "This call migrates a single user, Test User, to the target environment. This is useful for testing the migration process with a specific user account."
      },
      {
        "user_query": "Migrate multiple users from the source to the target environment for a department rollout.",
        "arguments": {
          "user_name_list": [
            "john.doe@example.com",
            "jane.doe@example.com",
            "sam.wilson@example.com"
          ]
        },
        "notes": "This call migrates three users\u2014John Doe, Jane Doe, and Sam Wilson\u2014to the target environment. Use this when rolling out migrations for a specific department or team."
      }
    ]
  },
  {
    "tool_id": "wellcheck.check_dashboard_structure",
    "module": "wellcheck",
    "class": "WellCheck",
    "method": "check_dashboard_structure",
    "description": "Analyze the structure of one or more dashboards.",
    "full_doc": "Analyze the structure of one or more dashboards.\n\nThis method:\n  - Counts pivot widgets (pivot / pivot2)\n  - Counts tabber widgets (WidgetsTabber)\n  - Counts accordion widgets via accordionConfig on widgets\n  - Counts jump-to-dashboard (JTD) instances, treated as child dashboards\n\nA \"child dashboard\" is treated as any dashboard referenced as a\njump-to-dashboard target from the parent dashboard, either via\nwidget options (drillTarget) or via dashboard script\n(prism.jumpToDashboard calls).\n\nThis method is intended to back wellcheck tasks and agentic tools that\nrespond to prompts such as:\n  - \"check child dashboards for this dashboard\"\n  - \"check jump-to dashboards on XYZ\"\n  - \"analyze dashboard structure / complexity\"\n\nParameters\n----------\ndashboards : list of str, optional\n    One or more dashboard references to analyze. Each reference can be:\n      - a Sisense dashboard ID, or\n      - a dashboard title (name).\n    At least one dashboard reference is required. At runtime this\n    method is tolerant of a single string being passed instead of a\n    list, and will normalize it to a one-element list.\n\nReturns\n-------\nlist of dict\n    A list with one entry per successfully processed dashboard. Each\n    entry has the keys:\n      - dashboard_id (str): Resolved dashboard ID.\n      - dashboard_title (str): Resolved dashboard title.\n      - pivot_count (int): Number of pivot/pivot2 widgets.\n      - tabber_count (int): Number of WidgetsTabber widgets.\n      - accordion_count (int): Number of accordion widgets detected\n        via accordionConfig on widgets.\n      - jtd_count (int): Number of jump-to-dashboard (JTD) instances\n        (child dashboards) detected in widget options and scripts.\n    If no dashboards are successfully processed, an empty list is\n    returned and details are available in the logs.",
    "parameters": {
      "type": "object",
      "properties": {
        "dashboards": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "One or more dashboard references to analyze. Each reference can be: - a Sisense dashboard ID, or - a dashboard title (name). At least one dashboard reference is required. At runtime this method is tolerant of a single string being passed instead of a list, and will normalize it to a one-element list."
        }
      },
      "required": []
    },
    "mutates": false,
    "tags": [
      "wellcheck",
      "dashboards",
      "read"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "Analyze the structure of the 'Sales Overview' dashboard.",
        "arguments": {
          "dashboards": [
            "Sales Overview"
          ]
        },
        "notes": "This call analyzes the structure of a single dashboard by its title, 'Sales Overview'. It will count pivot widgets, tabber widgets, accordion widgets, and jump-to-dashboard instances."
      },
      {
        "user_query": "Check the structure of dashboards with IDs '12345abc' and '67890xyz'.",
        "arguments": {
          "dashboards": [
            "12345abc",
            "67890xyz"
          ]
        },
        "notes": "This call analyzes the structure of two dashboards identified by their IDs. It is useful for comparing the complexity of multiple dashboards."
      },
      {
        "user_query": "Review the structure of the 'Marketing Performance' dashboard and the dashboard with ID 'abcdef123456'.",
        "arguments": {
          "dashboards": [
            "Marketing Performance",
            "abcdef123456"
          ]
        },
        "notes": "This call analyzes the structure of one dashboard by its title and another by its ID. It is helpful when working with a mix of dashboard references."
      }
    ]
  },
  {
    "tool_id": "wellcheck.check_dashboard_widget_counts",
    "module": "wellcheck",
    "class": "WellCheck",
    "method": "check_dashboard_widget_counts",
    "description": "Compute widget counts for one or more dashboards.",
    "full_doc": "Compute widget counts for one or more dashboards.\n\nThis method retrieves each specified dashboard definition, counts the\nnumber of widgets on that dashboard, and returns a per-dashboard\nsummary.\n\nParameters\n----------\ndashboards : list of str, optional\n    One or more dashboard references to analyze. Each reference can be:\n      - a Sisense dashboard ID, or\n      - a dashboard title (name).\n    At least one dashboard reference is required. At runtime this\n    method is tolerant of a single string being passed instead of a\n    list, and will normalize it to a one-element list.\n\nReturns\n-------\nlist of dict\n    A list with one entry per successfully processed dashboard. Each\n    entry contains:\n      - dashboard_id (str): Resolved dashboard ID.\n      - dashboard_title (str): Resolved dashboard title.\n      - widget_count (int): Number of widgets on the dashboard.\n\n    If no dashboards are successfully processed, an empty list is\n    returned and details are available in the logs.",
    "parameters": {
      "type": "object",
      "properties": {
        "dashboards": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "One or more dashboard references to analyze. Each reference can be: - a Sisense dashboard ID, or - a dashboard title (name). At least one dashboard reference is required. At runtime this method is tolerant of a single string being passed instead of a list, and will normalize it to a one-element list."
        }
      },
      "required": []
    },
    "mutates": false,
    "tags": [
      "wellcheck",
      "dashboards",
      "read"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "How many widgets are on the dashboards 'Sales Overview' and 'Marketing Performance'?",
        "arguments": {
          "dashboards": [
            "Sales Overview",
            "Marketing Performance"
          ]
        },
        "notes": "This call computes the total number of widgets for two dashboards identified by their titles."
      },
      {
        "user_query": "Get widget counts for the dashboard with ID '5f3b9c8e7a3d4b0012345678'.",
        "arguments": {
          "dashboards": [
            "5f3b9c8e7a3d4b0012345678"
          ]
        },
        "notes": "This call retrieves the widget count for a single dashboard using its Sisense dashboard ID."
      },
      {
        "user_query": "How many widgets are on the dashboards 'Customer Insights' and 'Product Analytics'?",
        "arguments": {
          "dashboards": [
            "Customer Insights",
            "Product Analytics"
          ]
        },
        "notes": "This call computes widget counts for two dashboards identified by their titles, useful for understanding dashboard complexity."
      }
    ]
  },
  {
    "tool_id": "wellcheck.check_datamodel_custom_tables",
    "module": "wellcheck",
    "class": "WellCheck",
    "method": "check_datamodel_custom_tables",
    "description": "Inspect custom tables in one or more data models and flag the use of UNION.",
    "full_doc": "Inspect custom tables in one or more data models and flag the use of UNION.\n\nThis method resolves each data model reference (ID or title), retrieves\nits schema from the Sisense API, iterates through all datasets/tables,\nand returns one row per custom table with a flag indicating whether its\nSQL expression contains the word \"union\" (case-insensitive).\n\nParameters\n----------\ndatamodels : list of str, optional\n    One or more data model references to analyze. Each reference can be:\n      - a Sisense data model ID, or\n      - a data model title (name).\n    At least one data model reference is required. At runtime this\n    method is tolerant of a single string being passed instead of a\n    list, and will normalize it to a one-element list.\n\nReturns\n-------\nlist of dict\n    A list with one entry per custom table. Each entry contains:\n      - data_model (str): Data model title.\n      - table (str): Table name.\n      - has_union (str): \"yes\" if the custom table expression contains\n        \"union\" (case-insensitive), otherwise \"no\".\n\n    If no data models are successfully processed, an empty list is\n    returned and details are available in the logs.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodels": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "One or more data model references to analyze. Each reference can be: - a Sisense data model ID, or - a data model title (name). At least one data model reference is required. At runtime this method is tolerant of a single string being passed instead of a list, and will normalize it to a one-element list."
        }
      },
      "required": []
    },
    "mutates": false,
    "tags": [
      "wellcheck",
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "Check if any custom tables in the 'Sales Analytics' data model use UNION in their SQL expressions.",
        "arguments": {
          "datamodels": [
            "Sales Analytics"
          ]
        },
        "notes": "This call inspects all custom tables in the 'Sales Analytics' data model to identify if any of them use UNION in their SQL expressions. This is useful for identifying potentially complex queries that may need optimization."
      },
      {
        "user_query": "Analyze the custom tables in multiple data models to find UNION usage.",
        "arguments": {
          "datamodels": [
            "Marketing Insights",
            "12345abcde67890fghij"
          ]
        },
        "notes": "This call checks the custom tables in two data models: one identified by its title ('Marketing Insights') and another by its ID ('12345abcde67890fghij'). It flags any custom tables that use UNION in their SQL expressions."
      },
      {
        "user_query": "Inspect the custom tables in the data model with ID '60ca5fe3-dc7b-4db7-aaa4-7dff0ac30bcb' for UNION usage.",
        "arguments": {
          "datamodels": [
            "60ca5fe3-dc7b-4db7-aaa4-7dff0ac30bcb"
          ]
        },
        "notes": "This call focuses on a single data model identified by its ID. It checks all custom tables in the model to determine if any of them contain UNION in their SQL expressions."
      }
    ]
  },
  {
    "tool_id": "wellcheck.check_datamodel_import_queries",
    "module": "wellcheck",
    "class": "WellCheck",
    "method": "check_datamodel_import_queries",
    "description": "Inspect tables in one or more data models for import queries.",
    "full_doc": "Inspect tables in one or more data models for import queries.\n\nThis method resolves each data model reference (ID or title), loads its\nschema, and checks every table for a ``configOptions.importQuery``\nconfiguration. For each table, it returns a row indicating whether an\nimport query is configured.\n\nParameters\n----------\ndatamodels : list of str, optional\n    One or more data model references to analyze. Each reference can be:\n      - a Sisense data model ID, or\n      - a data model title (name).\n    At least one data model reference is required. At runtime this\n    method is tolerant of a single string being passed instead of a\n    list, and will normalize it to a one-element list.\n\nReturns\n-------\nlist of dict\n    A list with one entry per table across all successfully processed\n    data models. Each entry contains:\n      - data_model (str): Resolved data model title.\n      - table (str): Table name.\n      - has_import_query (str): \"yes\" if an importQuery is present in\n        the table config options, otherwise \"no\".\n\n    If no data models are successfully processed, an empty list is\n    returned and details are available in the logs.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodels": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "One or more data model references to analyze. Each reference can be: - a Sisense data model ID, or - a data model title (name). At least one data model reference is required. At runtime this method is tolerant of a single string being passed instead of a list, and will normalize it to a one-element list."
        }
      },
      "required": []
    },
    "mutates": false,
    "tags": [
      "wellcheck",
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "Which tables in the 'Sales Data' data model use import queries?",
        "arguments": {
          "datamodels": [
            "Sales Data"
          ]
        },
        "notes": "This checks the 'Sales Data' data model by its title to identify tables that use import queries in their configuration."
      },
      {
        "user_query": "Check for import queries in the data models with IDs '12345' and '67890'.",
        "arguments": {
          "datamodels": [
            "12345",
            "67890"
          ]
        },
        "notes": "This analyzes two data models, identified by their IDs, to find tables that have import queries configured."
      },
      {
        "user_query": "Are there any tables with import queries in the 'Marketing Analytics' data model?",
        "arguments": {
          "datamodels": [
            "Marketing Analytics"
          ]
        },
        "notes": "This inspects the 'Marketing Analytics' data model by its title to determine if any tables are configured with import queries."
      }
    ]
  },
  {
    "tool_id": "wellcheck.check_datamodel_island_tables",
    "module": "wellcheck",
    "class": "WellCheck",
    "method": "check_datamodel_island_tables",
    "description": "Identify island tables (tables with no relationships) in one or more data models.",
    "full_doc": "Identify island tables (tables with no relationships) in one or more data models.\n\nThis method retrieves the schema for each specified data model, inspects\nits relations and tables, and returns information about tables that do\nnot participate in any relationship (often called \"island tables\").\n\nParameters\n----------\ndatamodels : list of str, optional\n    One or more data model references to analyze. Each reference can be:\n      - a Sisense data model ID, or\n      - a data model title (name).\n    At least one data model reference is required. At runtime this\n    method is tolerant of a single string being passed instead of a\n    list, and will normalize it to a one-element list.\n\nReturns\n-------\nlist of dict\n    A list with one entry per island table. Each entry contains:\n      - datamodel (str): Data model title.\n      - datamodel_oid (str): Data model ID.\n      - table (str): Table name.\n      - table_oid (str): Table ID.\n      - type (str): Table type (e.g., 'live', 'custom').\n      - relation (str): Always \"no\" for island tables.\n\n    If no data models are successfully processed, an empty list is\n    returned and details are available in the logs.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodels": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "One or more data model references to analyze. Each reference can be: - a Sisense data model ID, or - a data model title (name). At least one data model reference is required. At runtime this method is tolerant of a single string being passed instead of a list, and will normalize it to a one-element list."
        }
      },
      "required": []
    },
    "mutates": false,
    "tags": [
      "wellcheck",
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "Which tables in the 'Sales Analytics' data model are island tables?",
        "arguments": {
          "datamodels": [
            "Sales Analytics"
          ]
        },
        "notes": "This call checks the 'Sales Analytics' data model by its title to identify tables that do not participate in any relationships."
      },
      {
        "user_query": "Find island tables in the data models with IDs '60ca5fe3-dc7b-4db7-aaa4-7dff0ac30bcb' and '72b8f519ef48f00345bea45'.",
        "arguments": {
          "datamodels": [
            "60ca5fe3-dc7b-4db7-aaa4-7dff0ac30bcb",
            "72b8f519ef48f00345bea45"
          ]
        },
        "notes": "This call analyzes two data models by their IDs to detect tables without relationships."
      },
      {
        "user_query": "Are there any island tables in the 'Marketing Insights' data model?",
        "arguments": {
          "datamodels": [
            "Marketing Insights"
          ]
        },
        "notes": "This call inspects the 'Marketing Insights' data model by its title to find tables that are not connected to other tables."
      }
    ]
  },
  {
    "tool_id": "wellcheck.check_datamodel_m2m_relationships",
    "module": "wellcheck",
    "class": "WellCheck",
    "method": "check_datamodel_m2m_relationships",
    "description": "Check for potential many-to-many (M2M) relationships between tables",
    "full_doc": "Check for potential many-to-many (M2M) relationships between tables\nin one or more data models.\n\nFor each data model, this method inspects the relation graph, builds\ntable/column pairs from the relations, and runs aggregate SQL queries\nagainst the data source to detect whether both sides of the relation\ncontain duplicate keys. Pairs where each side has more than one\noccurrence of its key are flagged as many-to-many.\n\nParameters\n----------\ndatamodels : list of str, optional\n    One or more data model references to analyze. Each reference can be:\n      - a Sisense data model ID, or\n      - a data model title (name).\n    At least one data model reference is required. At runtime this\n    method is tolerant of a single string being passed instead of a\n    list, and will normalize it to a one-element list.\n\nReturns\n-------\nlist of dict\n    A list with one entry per relation field pair checked. Each entry\n    contains:\n      - data_model (str): Data model title.\n      - left_table (str): Name of the left table.\n      - left_column (str): Name of the left column.\n      - right_table (str): Name of the right table.\n      - right_column (str): Name of the right column.\n      - is_m2m (bool): True when both sides have more than one\n        occurrence of their key, False otherwise.\n\n    If no data models are successfully processed, an empty list is\n    returned and details are available in the logs.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodels": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "One or more data model references to analyze. Each reference can be: - a Sisense data model ID, or - a data model title (name). At least one data model reference is required. At runtime this method is tolerant of a single string being passed instead of a list, and will normalize it to a one-element list."
        }
      },
      "required": []
    },
    "mutates": false,
    "tags": [
      "wellcheck",
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "Identify potential many-to-many relationships in the 'Sales Analytics' data model.",
        "arguments": {
          "datamodels": [
            "Sales Analytics"
          ]
        },
        "notes": "This call checks the 'Sales Analytics' data model for any relationships where both sides of the relation have duplicate keys, indicating a potential many-to-many relationship."
      },
      {
        "user_query": "Check for many-to-many relationships in multiple data models, including 'Finance Model' and 'HR Data'.",
        "arguments": {
          "datamodels": [
            "Finance Model",
            "HR Data"
          ]
        },
        "notes": "This call analyzes the 'Finance Model' and 'HR Data' data models to identify any relationships that may be many-to-many, helping to ensure data integrity and optimize model design."
      },
      {
        "user_query": "Analyze the data model with ID '12345abcde' for many-to-many relationships.",
        "arguments": {
          "datamodels": [
            "12345abcde"
          ]
        },
        "notes": "This call inspects the data model identified by the ID '12345abcde' to detect any many-to-many relationships between its tables, which could indicate potential issues in the model's relational structure."
      }
    ]
  },
  {
    "tool_id": "wellcheck.check_datamodel_rls_datatypes",
    "module": "wellcheck",
    "class": "WellCheck",
    "method": "check_datamodel_rls_datatypes",
    "description": "Inspect row-level security (RLS) rules for one or more data models and",
    "full_doc": "Inspect row-level security (RLS) rules for one or more data models and\nreport the datatype of the columns used in those rules.\n\nThis method resolves each data model reference, fetches its RLS (data\nsecurity) rules from the appropriate API endpoint based on the data\nmodel type (extract or live), and returns one row per unique\n(datamodel, table, column, datatype) combination.\n\nParameters\n----------\ndatamodels : list of str, optional\n    One or more data model references to analyze. Each reference can be:\n      - a Sisense data model ID, or\n      - a data model title (name).\n    At least one data model reference is required. At runtime this\n    method is tolerant of a single string being passed instead of a\n    list, and will normalize it to a one-element list.\n\nReturns\n-------\nlist of dict\n    A list with one entry per unique RLS column. Each entry contains:\n      - datamodel (str): Data model title.\n      - table (str): Table name where RLS is applied.\n      - column (str): Column name used in the RLS rule.\n      - datatype (str): Datatype reported by Sisense for that column.\n\n    If no data models are successfully processed, an empty list is\n    returned and details are available in the logs.",
    "parameters": {
      "type": "object",
      "properties": {
        "datamodels": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "One or more data model references to analyze. Each reference can be: - a Sisense data model ID, or - a data model title (name). At least one data model reference is required. At runtime this method is tolerant of a single string being passed instead of a list, and will normalize it to a one-element list."
        }
      },
      "required": []
    },
    "mutates": false,
    "tags": [
      "wellcheck",
      "datamodel",
      "read"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "Can you check the RLS rules for the 'Sales_Data' data model?",
        "arguments": {
          "datamodels": [
            "Sales_Data"
          ]
        },
        "notes": "This call inspects the row-level security rules for the data model titled 'Sales_Data' and returns the columns and their datatypes used in those rules."
      },
      {
        "user_query": "I need to review the RLS column datatypes for multiple data models, including 'Marketing_Analytics' and 'Customer_Insights'.",
        "arguments": {
          "datamodels": [
            "Marketing_Analytics",
            "Customer_Insights"
          ]
        },
        "notes": "This call analyzes the RLS rules for two data models, 'Marketing_Analytics' and 'Customer_Insights', and reports the columns and their datatypes used in the rules."
      },
      {
        "user_query": "What are the RLS column datatypes for the data model with ID '12345abcde'?",
        "arguments": {
          "datamodels": [
            "12345abcde"
          ]
        },
        "notes": "This call inspects the RLS rules for the data model identified by the ID '12345abcde' and returns the columns and their datatypes used in those rules."
      }
    ]
  },
  {
    "tool_id": "wellcheck.check_pivot_widget_fields",
    "module": "wellcheck",
    "class": "WellCheck",
    "method": "check_pivot_widget_fields",
    "description": "Analyze pivot widgets on one or more dashboards and report those with many fields.",
    "full_doc": "Analyze pivot widgets on one or more dashboards and report those with many fields.\n\nThis method retrieves each specified dashboard, scans all pivot widgets\n(types containing ``\"pivot\"`` or ``\"pivot2\"``), counts how many fields\n(items) are attached to those widgets, and returns a per-widget summary\nfor any pivot with more than ``max_fields`` fields.\n\nParameters\n----------\ndashboards : list of str, optional\n    One or more dashboard references to analyze. Each reference can be:\n      - a Sisense dashboard ID, or\n      - a dashboard title (name).\n    At least one dashboard reference is required. At runtime this\n    method is tolerant of a single string being passed instead of a\n    list, and will normalize it to a one-element list.\nmax_fields : int, optional\n    Threshold for the number of fields on a pivot widget. Only pivot\n    widgets with more than this number of fields are included in the\n    returned data. Defaults to 20.\n\nReturns\n-------\nlist of dict\n    A list of dictionaries, each describing a pivot widget that exceeds\n    the configured field threshold. Each entry contains:\n      - dashboard_id (str): Resolved dashboard ID.\n      - dashboard_title (str): Resolved dashboard title.\n      - widget_id (str): Pivot widget ID.\n      - has_more_fields (bool): Always True for returned rows, since\n        only widgets above the threshold are included.\n      - field_count (int): Total number of fields (items) in the widget.\n\n    If no dashboards are successfully processed, or no pivot widgets\n    exceed the threshold, an empty list is returned and details are\n    available in the logs.",
    "parameters": {
      "type": "object",
      "properties": {
        "dashboards": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "One or more dashboard references to analyze. Each reference can be: - a Sisense dashboard ID, or - a dashboard title (name). At least one dashboard reference is required. At runtime this method is tolerant of a single string being passed instead of a list, and will normalize it to a one-element list."
        },
        "max_fields": {
          "type": "integer",
          "description": "Threshold for the number of fields on a pivot widget. Only pivot widgets with more than this number of fields are included in the returned data. Defaults to 20."
        }
      },
      "required": []
    },
    "mutates": false,
    "tags": [
      "wellcheck",
      "read"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "Find pivot widgets with more than 25 fields on the 'Sales Performance' dashboard.",
        "arguments": {
          "dashboards": [
            "Sales Performance"
          ],
          "max_fields": 25
        },
        "notes": "This call analyzes the 'Sales Performance' dashboard to identify any pivot widgets that have more than 25 fields, which could indicate potential performance issues or overly complex widgets."
      },
      {
        "user_query": "Check all pivot widgets on dashboards with IDs '12345abc' and '67890def' for those exceeding 30 fields.",
        "arguments": {
          "dashboards": [
            "12345abc",
            "67890def"
          ],
          "max_fields": 30
        },
        "notes": "This call scans two specific dashboards by their IDs to find pivot widgets with more than 30 fields, helping to optimize widget complexity."
      },
      {
        "user_query": "Analyze pivot widgets on the 'Marketing Overview' dashboard and return those with more than 20 fields.",
        "arguments": {
          "dashboards": [
            "Marketing Overview"
          ],
          "max_fields": 20
        },
        "notes": "This call evaluates the 'Marketing Overview' dashboard to identify pivot widgets that exceed the default threshold of 20 fields, which can help improve dashboard performance."
      }
    ]
  },
  {
    "tool_id": "wellcheck.run_full_wellcheck",
    "module": "wellcheck",
    "class": "WellCheck",
    "method": "run_full_wellcheck",
    "description": "Run a composite \"full\" wellcheck across dashboards and data models.",
    "full_doc": "Run a composite \"full\" wellcheck across dashboards and data models.\n\nThis method is a convenience wrapper that orchestrates multiple\ndashboard-level and data-model-level checks and returns a structured\nreport that groups their results.\n\nParameters\n----------\ndashboards : str or list of str, optional\n    One or more dashboard references to analyze. Each reference can be:\n      - a Sisense dashboard ID, or\n      - a dashboard title (name).\n    At runtime this parameter is tolerant of a single string and will\n    normalize it to a one-element list.\ndatamodels : str or list of str, optional\n    One or more data model references to analyze. Each reference can be:\n      - a data model ID, or\n      - a data model title (name).\n    At runtime this parameter is tolerant of a single string and will\n    normalize it to a one-element list.\nmax_pivot_fields : int, optional\n    Threshold used by the pivot-fields check. Any pivot widget with\n    more than this number of fields is flagged.\n\nReturns\n-------\ndict\n    A dictionary with two top-level sections:\n\n    - \"dashboards\": {\n          \"structure\": [...],\n          \"widget_counts\": [...],\n          \"pivot_widget_fields\": [...],\n      }\n\n    - \"datamodels\": {\n          \"custom_tables\": [...],\n          \"island_tables\": [...],\n          \"rls_datatypes\": [...],\n          \"import_queries\": [...],\n          \"m2m_relationships\": [...],\n          \"unused_columns\": [...],\n      }\n\n    Each subsection contains the list of rows returned by the\n    corresponding check method. If a given set of references is not\n    provided or no assets are successfully processed, that subsection\n    will be an empty list.",
    "parameters": {
      "type": "object",
      "properties": {
        "dashboards": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "One or more dashboard references to analyze. Each reference can be: - a Sisense dashboard ID, or - a dashboard title (name). At runtime this parameter is tolerant of a single string and will normalize it to a one-element list."
        },
        "datamodels": {
          "type": "array",
          "items": {
            "type": "string"
          },
          "description": "One or more data model references to analyze. Each reference can be: - a data model ID, or - a data model title (name). At runtime this parameter is tolerant of a single string and will normalize it to a one-element list."
        },
        "max_pivot_fields": {
          "type": "integer",
          "description": "Threshold used by the pivot-fields check. Any pivot widget with more than this number of fields is flagged."
        }
      },
      "required": []
    },
    "mutates": true,
    "tags": [
      "wellcheck",
      "write"
    ],
    "sdk_version": "0.2.0",
    "updated_at": "2025-12-01T02:31:05Z",
    "examples": [
      {
        "user_query": "Run a full wellcheck on dashboards and data models to identify structural issues and unused columns.",
        "arguments": {
          "dashboards": [
            "6893741265c9f5484dc999d7",
            "Academy AI Content"
          ],
          "datamodels": [
            "60ca5fe3-dc7b-4db7-aaa4-7dff0ac30bcb",
            "SalesDataModel"
          ],
          "max_pivot_fields": 25
        },
        "notes": "This call analyzes the structure of dashboards and data models, flags pivot widgets with more than 25 fields, and checks for unused columns."
      },
      {
        "user_query": "Analyze a single dashboard and data model for performance bottlenecks.",
        "arguments": {
          "dashboards": "Marketing Performance Dashboard",
          "datamodels": "MarketingDataModel",
          "max_pivot_fields": 20
        },
        "notes": "Use this call to run a targeted analysis on one dashboard and one data model, focusing on pivot widgets with more than 20 fields."
      },
      {
        "user_query": "Perform a comprehensive health check on multiple dashboards and data models.",
        "arguments": {
          "dashboards": [
            "663b8f519ef48f00345bea45",
            "Customer Insights Dashboard"
          ],
          "datamodels": [
            "CustomerDataModel",
            "ProductDataModel"
          ],
          "max_pivot_fields": 30
        },
        "notes": "This call is ideal for a broader analysis across multiple dashboards and data models, with a higher threshold for pivot widget fields."
      }
    ]
  }
]